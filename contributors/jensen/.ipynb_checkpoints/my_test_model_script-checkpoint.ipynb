{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366194de-ee7c-45e5-88c7-84169b9c7133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n",
    "\n",
    "Attributes:\n",
    "    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n",
    "\n",
    "Functions:\n",
    "    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n",
    "    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n",
    "    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n",
    "    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n",
    "    train(): Trains the Extra Trees Regressor model.\n",
    "    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model_creation_rf import RandomForestHole\n",
    "from snowcast_utils import work_dir, month_to_season\n",
    "\n",
    "\n",
    "working_dir = work_dir\n",
    "\n",
    "class ETHole(RandomForestHole):\n",
    "  \n",
    "    def custom_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        A custom loss function that penalizes errors for values greater than 10.\n",
    "\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True target values.\n",
    "            y_pred (numpy.ndarray): Predicted target values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Custom loss values.\n",
    "        \"\"\"\n",
    "        errors = np.abs(y_true - y_pred)\n",
    "        \n",
    "        return np.where(y_true > 10, 2 * errors, errors)\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Returns the Extra Trees Regressor model with specified hyperparameters.\n",
    "\n",
    "        Returns:\n",
    "            ExtraTreesRegressor: The Extra Trees Regressor model.\n",
    "        \"\"\"\n",
    "        return ExtraTreesRegressor(n_estimators=200, \n",
    "                                   max_depth=None,\n",
    "                                   random_state=42, \n",
    "                                   min_samples_split=2,\n",
    "                                   min_samples_leaf=1,\n",
    "                                   n_jobs=5\n",
    "                                  )\n",
    "\n",
    "    def create_sample_weights(self, y, scale_factor, columns):\n",
    "        \"\"\"\n",
    "        Creates sample weights based on target values and a scaling factor.\n",
    "\n",
    "        Args:\n",
    "            y (numpy.ndarray): Target values.\n",
    "            scale_factor (float): Scaling factor for sample weights.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Sample weights.\n",
    "        \"\"\"\n",
    "        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n",
    "        # Create a weight vector to assign weights to features - this is not a good idea\n",
    "#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n",
    "#         default_weight = 1.0\n",
    "\n",
    "#         # Create an array of sample weights based on feature_weights\n",
    "#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n",
    "        #return sample_weights\n",
    "\n",
    "      \n",
    "    def preprocessing(self):\n",
    "        \"\"\"\n",
    "        Preprocesses the training data, including data cleaning and feature extraction.\n",
    "        \"\"\"\n",
    "        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n",
    "        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n",
    "        training_data_path = f\"{working_dir}/all_merged_training.csv\"\n",
    "        print(\"preparing training data from csv: \", training_data_path)\n",
    "        data = pd.read_csv(training_data_path)\n",
    "        print(data.head())\n",
    "        \n",
    "        data['date'] = pd.to_datetime(data['date'])\n",
    "        #reference_date = pd.to_datetime('1900-01-01')\n",
    "        #data['date'] = (data['date'] - reference_date).dt.days\n",
    "        # just use julian day\n",
    "        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n",
    "        # just use the season to reduce the bias on month or dates\n",
    "        data['date'] = data['date'].dt.month.apply(month_to_season)\n",
    "        \n",
    "        data.replace('--', pd.NA, inplace=True)\n",
    "        data.fillna(-999, inplace=True)\n",
    "        \n",
    "        data = data[(data['swe_value'] != -999)]\n",
    "        \n",
    "        # careful this is filling all the n/a values with interpolation\n",
    "        #data.fillna(method='ffill', inplace=True)\n",
    "        #print(data.head())\n",
    "        \n",
    "        print(\"get swe statistics\")\n",
    "        print(data[\"swe_value\"].describe())\n",
    "        \n",
    "        #data = data.drop('Unnamed: 0', axis=1)\n",
    "        #data = data.drop('level_0', axis=1)\n",
    "        #data = data.drop(['date'], axis=1)\n",
    "        \n",
    "#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n",
    "# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n",
    "# 'relative_humidity_rmax', 'relative_humidity_rmin',\n",
    "# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n",
    "# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n",
    "        \n",
    "        \n",
    "        X = data.drop('swe_value', axis=1)\n",
    "      \t\n",
    "        print('required features:', X.columns)\n",
    "        y = data['swe_value']\n",
    "        \n",
    "        print(\"input features and order: \", X.columns)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n",
    "\n",
    "        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n",
    "        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n",
    "        self.feature_names = X_train.columns\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the Extra Trees Regressor model.\n",
    "        \"\"\"\n",
    "        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n",
    "\n",
    "    def post_processing(self):\n",
    "        \"\"\"\n",
    "        Performs post-processing, including feature importance analysis and visualization.\n",
    "        \"\"\"\n",
    "        feature_importances = self.classifier.feature_importances_\n",
    "        feature_names = self.feature_names\n",
    "        sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "        sorted_importances = feature_importances[sorted_indices]\n",
    "        sorted_feature_names = feature_names[sorted_indices]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.xlabel('Feature')\n",
    "        plt.ylabel('Feature Importance')\n",
    "        plt.title('Feature Importance Plot (ET model)')\n",
    "        plt.tight_layout()\n",
    "        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n",
    "        plt.savefig(feature_png)\n",
    "        print(f\"Feature image is saved {feature_png}\")\n",
    "\n",
    "# Instantiate ETHole class and perform tasks\n",
    "hole = ETHole()\n",
    "hole.preprocessing()\n",
    "hole.train()\n",
    "hole.test()\n",
    "hole.evaluate()\n",
    "hole.save()\n",
    "hole.post_processing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fe8fe2-6718-40d0-adc9-2425ae659093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Latitude  Longitude\n",
      "649    37.912   -119.564\n",
      "265    38.416   -119.276\n",
      "111    38.632   -118.772\n",
      "301    38.380   -118.988\n",
      "339    38.308   -119.636\n",
      "559    38.056   -118.772\n",
      "742    37.804   -119.240\n",
      "202    38.488   -119.528\n",
      "600    37.984   -119.312\n",
      "680    37.876   -119.456\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/ubuntu/gridmet_test_run/testing_points.csv\")\n",
    "# Choose 10 random rows from the DataFrame\n",
    "random_rows = df.sample(n=10, random_state=1)  # Set a random_state for reproducibility\n",
    "\n",
    "# Print the randomly selected rows\n",
    "print(random_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
