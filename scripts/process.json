[{
  "id" : "78vedq",
  "name" : "data_sentinel2",
  "description" : null,
  "code" : "# Data Preparation for Sentinel 2\n\nprint(\"Not ready yet..Prepare sentinel 2 into .csv\")\n\nprint('test')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "mxpyqt",
  "name" : "model_creation_lstm",
  "description" : "python",
  "code" : "# Create LSTM model\n\nprint(\"Create LSTM\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rauqsh",
  "name" : "model_creation_ghostnet",
  "description" : "python",
  "code" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "u7xh2p",
  "name" : "data_integration",
  "description" : null,
  "code" : "\"\"\"\nThis script reads three CSV files into Dask DataFrames, performs data type conversion,\nand merges them into a final Dask DataFrame. The merged data is then saved to a CSV file.\n\nThe three CSV files include climatology data, training-ready SNOTEL data, and training-ready\nterrain data, each with latitude ('lat'), longitude ('lon'), and date ('date') columns.\n\nAttributes:\n    file_path1 (str): File path of the climatology data CSV file.\n    file_path2 (str): File path of the training-ready SNOTEL data CSV file.\n    file_path3 (str): File path of the training-ready terrain data CSV file.\n\nFunctions:\n    small_function: Reads, processes, and merges the CSV files and saves the result to a CSV file.\n\"\"\"\n\nimport dask.dataframe as dd\n\n# Define the file paths of the three CSV files\nfile_path1 = '/home/chetana/gridmet_test_run/climatology_data.csv'\nfile_path2 = '/home/chetana/gridmet_test_run/training_ready_snotel_data.csv'\nfile_path3 = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\n\ndef small_function():\n    \"\"\"\n    Reads each CSV file into a Dask DataFrame, performs data type conversion for latitude and longitude,\n    merges the DataFrames based on specific columns, and saves the merged Dask DataFrame to a CSV file.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    # Read each CSV file into a Dask DataFrame\n    df1 = dd.read_csv(file_path1)\n    df2 = dd.read_csv(file_path2)\n    df3 = dd.read_csv(file_path3)\n\n    # Perform data type conversion for latitude and longitude columns\n    df1['lat'] = df1['lat'].astype(float)\n    df1['lon'] = df1['lon'].astype(float)\n    df2['lat'] = df2['lat'].astype(float)\n    df2['lon'] = df2['lon'].astype(float)\n    df3['lat'] = df3['lat'].astype(float)\n    df3['lon'] = df3['lon'].astype(float)\n\n    # Merge the first two DataFrames based on 'lat', 'lon', and 'date'\n    merged_df1 = dd.merge(df1, df2, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'Date'])\n\n    # Merge the third DataFrame based on 'lat' and 'lon'\n    merged_df2 = dd.merge(merged_df1, df3, on=['lat', 'lon'])\n\n    # Save the merged Dask DataFrame directly to a CSV file\n    merged_df2.to_csv('/home/chetana/gridmet_test_run/model_training_data.csv', index=False, single_file=True)\n\n# Uncomment the line below to execute the function\n# small_function()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "e8k4wq",
  "name" : "model_train_validate",
  "description" : null,
  "code" : "\"\"\"\nThis script trains and validates machine learning models for hole analysis.\n\nAttributes:\n    RandomForestHole (class): A class for training and using a Random Forest model.\n    XGBoostHole (class): A class for training and using an XGBoost model.\n    ETHole (class): A class for training and using an Extra Trees model.\n\nFunctions:\n    main(): The main function that trains and validates machine learning models for hole analysis.\n\"\"\"\n\nfrom model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\nfrom model_creation_et import ETHole\n\ndef main():\n    print(\"Train Models\")\n\n    # Choose the machine learning models to train (e.g., RandomForestHole, XGBoostHole, ETHole)\n    worm_holes = [ETHole()]\n\n    for hole in worm_holes:\n        # Perform preprocessing for the selected model\n        hole.preprocessing()\n        print(hole.train_x.shape)\n        print(hole.train_y.shape)\n        \n        # Train the machine learning model\n        hole.train()\n        \n        # Test the trained model\n        hole.test()\n        \n        # Evaluate the model's performance\n        hole.evaluate()\n        \n        # Save the trained model\n        hole.save()\n\n    print(\"Finished training and validating all the models.\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "h1qp9v",
  "name" : "model_predict",
  "description" : null,
  "code" : "import joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport numpy as np\nfrom snowcast_utils import homedir, work_dir, month_to_season, test_start_date\nimport os\nimport random\nimport string\nimport shutil\nfrom model_creation_et import selected_columns\n\ndef generate_random_string(length):\n    # Define the characters that can be used in the random string\n    characters = string.ascii_letters + string.digits  # You can customize this to include other characters if needed\n\n    # Generate a random string of the specified length\n    random_string = ''.join(random.choice(characters) for _ in range(length))\n\n    return random_string\n  \n\ndef load_model(model_path):\n    \"\"\"\n    Load a machine learning model from a file.\n\n    Args:\n        model_path (str): Path to the saved model file.\n\n    Returns:\n        model: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file containing the data.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the loaded data.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data, is_model_input: bool = True):\n    \"\"\"\n    Preprocess the input data for model prediction.\n\n    Args:\n        data (pd.DataFrame): Input data in the form of a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: Preprocessed data ready for prediction.\n    \"\"\"\n    \n    #print(\"check date format: \", data.head())\n    #data['date'] = data['date'].dt.strftime('%j').astype(int)\n    #data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    \n    #data = data.apply(pd.to_numeric, errors='coerce')\n\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n#                          'Elevation': 'elevation',\n#                          'Slope': 'Slope',\n#                          'Aspect': 'Aspect',\n#                          'Curvature': 'Curvature',\n#                          'Northness': 'Northness',\n#                          'Eastness': 'Eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n#                          'relative_humidity_rmin': '',\n#                          'cumulative_rmin',\n#                          'mean_vapor_pressure_deficit', \n#                          'cumulative_vpd', \n#                          'wind_speed',\n#                          'cumulative_vs', \n#                          'relative_humidity_rmax', 'cumulative_rmax',\n\n# 'precipitation_amount', 'cumulative_pr', 'air_temperature_tmmx',\n\n# 'cumulative_tmmx', 'potential_evapotranspiration', 'cumulative_etr',\n\n# 'air_temperature_tmmn', 'cumulative_tmmn', 'x', 'y', 'elevation',\n\n# 'slope', 'aspect', 'curvature', 'northness', 'eastness', 'AMSR_SWE',\n\n# 'cumulative_AMSR_SWE', 'AMSR_Flag', 'cumulative_AMSR_Flag',\n                        }, inplace=True)\n\n    print(data.head())\n    print(data.columns)\n    \n    # filter out three days for final visualization to accelerate the process\n    #dates_to_match = ['2018-03-15', '2018-04-15', '2018-05-15']\n    #mask = data['date'].dt.strftime('%Y-%m-%d').isin(dates_to_match)\n    # Filter the DataFrame based on the mask\n    #data = data[mask]\n    if is_model_input:\n        data['date'] = pd.to_datetime(data['date'])\n        selected_columns.remove(\"swe_value\")\n        desired_order = selected_columns + ['lat', 'lon',]\n        \n        data = data[desired_order]\n        data = data.reindex(columns=desired_order)\n        \n        print(\"reorganized columns: \", data.columns)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Predict snow water equivalent (SWE) using a machine learning model.\n\n    Args:\n        model: The machine learning model for prediction.\n        data (pd.DataFrame): Input data for prediction.\n\n    Returns:\n        pd.DataFrame: Dataframe with predicted SWE values.\n    \"\"\"\n    data = data.fillna(-999)\n    input_data = data\n    input_data = data.drop([\"lat\", \"lon\"], axis=1)\n    #input_data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin',], axis=1)\n    #scaler = StandardScaler()\n\n    # Fit the scaler on the training data and transform both training and testing data\n    #input_data_scaled = scaler.fit_transform(input_data)\n    \n    predictions = model.predict(input_data)\n    data['predicted_swe'] = predictions\n    return data\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge predicted SWE data with the original data.\n\n    Args:\n        original_data (pd.DataFrame): Original input data.\n        predicted_data (pd.DataFrame): Dataframe with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged dataframe.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    if \"date\" not in predicted_data:\n    \tpredicted_data[\"date\"] = test_start_date\n    new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    print(\"original_data.columns: \", original_data.columns)\n    print(\"new_data_extracted.columns: \", new_data_extracted.columns)\n    print(\"new prediction statistics: \", new_data_extracted[\"predicted_swe\"].describe())\n    #merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"first merged df: \", merged_df.columns)\n\n    merged_df.loc[merged_df['fsca'] == 237, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['fsca'] == 239, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['fsca'] == 225, 'predicted_swe'] = 0\n    #merged_df.loc[merged_df['cumulative_fsca'] == 0, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['fsca'] == 0, 'predicted_swe'] = 0\n    \n    merged_df.loc[merged_df['air_temperature_tmmx'].isnull(), \n                  'predicted_swe'] = 0\n\n    merged_df.loc[merged_df['lc_prop3'] == 3, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['lc_prop3'] == 255, 'predicted_swe'] = 0\n    merged_df.loc[merged_df['lc_prop3'] == 27, 'predicted_swe'] = 0\n\n    return merged_df\n\ndef predict():\n    \"\"\"\n    Main function for predicting snow water equivalent (SWE).\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'{homedir}/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"Using model: {model_path}\")\n  \n    new_data_path = f'{work_dir}/testing_all_ready_{test_start_date}.csv'\n    #output_path = f'{work_dir}/test_data_predicted_three_days_only.csv'\n    latest_output_path = f'{work_dir}/test_data_predicted_latest_{test_start_date}.csv'\n    output_path = f'{work_dir}/test_data_predicted_{generate_random_string(5)}.csv'\n  \n    if os.path.exists(output_path):\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    print(f\"loading {new_data_path}\")\n    new_data = load_data(new_data_path)\n    new_data = new_data.drop([\"date.1\",], axis=1)\n    print(\"new_data.columns: \", new_data.columns)\n\n    preprocessed_data = preprocess_data(new_data, is_model_input=True)\n    if len(new_data) < len(preprocessed_data):\n      raise ValueError(\"Why the preprocessed data increased?\")\n    #print('Data preprocessing completed.', preprocessed_data.head())\n    #print(f'Model used: {model_path}')\n    predicted_data = predict_swe(model, preprocessed_data)\n    print(\"how many predicted? \", len(predicted_data))\n    \n    if \"date\" not in preprocessed_data:\n    \tpreprocessed_data[\"date\"] = test_start_date\n\n    full_preprocessed_data = preprocess_data(new_data, is_model_input=False)\n    predicted_data = merge_data(full_preprocessed_data, predicted_data)\n    \n    \n    #print('Data prediction completed.')\n  \n    #print(predicted_data['date'])\n    predicted_data.to_csv(output_path, index=False)\n    print(\"Prediction successfully done \", output_path)\n    \n    shutil.copy(output_path, latest_output_path)\n    print(f\"Copied to {latest_output_path}\")\n\n#     if len(predicted_data) == height * width:\n#         print(f\"The image width, height match with the number of rows in the CSV. {len(predicted_data)} rows\")\n#     else:\n#         raise Exception(\"The total number of rows does not match\")\n\nif __name__ == \"__main__\":\n\tpredict()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "urd0nk",
  "name" : "data_terrainFeatures",
  "description" : null,
  "code" : "# Load dependencies\nimport geopandas as gpd\nimport json\nimport geojson\nfrom pystac_client import Client\nimport planetary_computer\nimport xarray\nimport rioxarray\nimport xrspatial\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyproj import Proj, transform\nimport os\nimport sys, traceback\nimport requests\nfrom snowcast_utils import work_dir\n\n\nhome_dir = os.path.expanduser('~')\nsnowcast_github_dir = f\"{home_dir}/Documents/GitHub/SnowCast/\"\n\n\n#exit() # this process no longer need to execute, we need to make Geoweaver to specify which process doesn't need to run\n\n\n# user-defined paths for data-access\ndata_dir = f'{snowcast_github_dir}data/'\ngridcells_file = data_dir+'snowcast_provided/grid_cells_eval.geojson'\n#stations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\nstations_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n#stations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\nall_training_points_with_station_and_non_station_file = f\"{work_dir}/all_training_points_in_westus.csv\"\nall_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\ngridcells_outfile = data_dir+'terrain/gridcells_terrainData_eval.csv'\n#stations_outfile = f\"{work_dir}/training_all_active_snotel_station_list_elevation.csv_terrain_4km_grid_shift.csv\"\nstations_outfile = f\"{work_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv\"\n\n\ndef get_planetary_client():\n  #requests.get('https://planetarycomputer.microsoft.com/api/stac/v1')\n\n  # setup client for handshaking and data-access\n  print(\"setup planetary computer client\")\n  client = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",ignore_conformance=True)\n  \n  return client\n\ndef prepareGridCellTerrain():\n  client = get_planetary_client()\n  # Load metadata\n  gridcellsGPD = gpd.read_file(gridcells_file)\n  gridcells = geojson.load(open(gridcells_file))\n  stations = pd.read_csv(stations_file)\n\n  # instantiate output panda dataframes\n  df_gridcells = df = pd.DataFrame(columns=(\n    \"Longitude [deg]\",\"Latitude [deg]\",\n    \"Elevation [m]\",\"Aspect [deg]\",\n    \"Curvature [ratio]\",\"Slope [deg]\",\n    \"Eastness [unitCirc.]\",\"Northness [unitCirc.]\"))\n  # instantiate output panda dataframes\n  # Calculate gridcell characteristics using Copernicus DEM data\n  print(\"Prepare GridCell Terrain data\")\n  for idx,cell in enumerate(gridcells['features']):\n      print(\"Processing grid \", idx)\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Polygon\", \"coordinates\":cell['geometry']['coordinates']},\n      )\n      items = list(search.get_items())\n      print(\"==> Searched items: \", len(items))\n\n      cropped_data = None\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              #xarray.open_rasterio(signed_asset.href)\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n      except:\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n\n      # calculate lat/long of center of gridcell\n      longitude = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      latitude = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      print(\"reproject data to EPSG:32612\")\n      # reproject the cropped dem data\n      cropped_data = cropped_data.rio.reproject(\"EPSG:32612\")\n\n      # Mean elevation of gridcell\n      mean_elev = cropped_data.mean().values\n      print(\"Elevation: \", mean_elev)\n\n      # Calculate directional components\n      aspect = xrspatial.aspect(cropped_data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(\"Aspect: \", mean_aspect)\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n      print(\"Eastness: \", mean_eastness)\n      print(\"Northness: \", mean_northness)\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(cropped_data)\n      mean_curvature = curvature.mean().values\n      print(\"Curvature: \", mean_curvature)\n\n      # Calculate mean slope\n      slope = xrspatial.slope(cropped_data)\n      mean_slope = slope.mean().values\n      print(\"Slope: \", mean_slope)\n\n      # Fill pandas dataframe\n      df_gridcells.loc[idx] = [longitude,latitude,\n                               mean_elev,mean_aspect,\n                               mean_curvature,mean_slope,\n                               mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n      #     df_gridcells.to_csv(gridcells_outfile)\n\n  # Save output data into csv format\n  df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n  df_gridcells.to_csv(gridcells_outfile)\n\ndef prepareStationTerrain():\n  client = get_planetary_client()\n  \n  df_station = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                     \"Elevation [m]\",\"Elevation_30 [m]\",\"Elevation_1000 [m]\",\n                                     \"Aspect_30 [deg]\",\"Aspect_1000 [deg]\",\n                                     \"Curvature_30 [ratio]\",\"Curvature_1000 [ratio]\",\n                                     \"Slope_30 [deg]\",\"Slope_1000 [deg]\",\n                                     \"Eastness_30 [unitCirc.]\",\"Northness_30 [unitCirc.]\",\n                                     \"Eastness_1000 [unitCirc.]\",\"Northness_1000 [unitCirc.]\"))\n  \n  stations_df = pd.read_csv(stations_file)\n  print(stations_df.head())\n  # Calculate terrain characteristics of stations, and surrounding regions using COP 30\n  for idx,station in stations_df.iterrows():\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\n            \"type\": \"Point\", \n            \"coordinates\": [\n              stations_df['lon'],\n              stations_df['lat']\n            ]\n          },\n      )\n      items = list(search.get_items())\n      print(f\"Returned {len(items)} items\")\n\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-stations_df['lon'])\n          ydiff = np.abs(data.y-stations_df['lat'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n      except:\n          traceback.print_exc(file=sys.stdout)\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-stations_df['lon'])\n          ydiff = np.abs(data.y-stations_df['lat'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n\n      # Reproject the station data to better include only 1000m surrounding area\n      inProj = Proj(init='epsg:4326')\n      outProj = Proj(init='epsg:32612')\n      new_x,new_y = transform(inProj,outProj,\n                              stations_df['lon'],\n                              stations_df['lat'])\n\n      # Calculate elevation of station and surroundings\n      mean_elevation = data.mean().values\n      elevation = data.sel(x=new_x,y=new_y,method='nearest')\n      print(elevation.values)\n\n      # Calcuate directional components\n      aspect = xrspatial.aspect(data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      #print(mean_aspect)\n      aspect = aspect.sel(x=new_x,y=new_y,method='nearest')\n      #print(aspect.values)\n      eastness = np.cos(aspect*(np.pi/180))\n      northness = np.sin(aspect*(np.pi/180))\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(data)\n      mean_curvature = curvature.mean().values\n      curvature = curvature.sel(x=new_x,y=new_y,method='nearest')\n      print(curvature.values)\n\n      # Calculate slope\n      slope = xrspatial.slope(data)\n      mean_slope = slope.mean().values\n      slope = slope.sel(x=new_x,y=new_y,method='nearest')\n      print(slope.values)\n\n      # Fill pandas dataframe\n      df_station.loc[idx] = [stations_df['lon'],\n                             stations_df['lat'],\n                             station['elevation_m'],\n                             elevation.values,mean_elevation,\n                             aspect.values,mean_aspect,\n                             curvature.values,mean_curvature,\n                             slope.values,mean_slope,\n                             eastness.values,northness.values,\n                             mean_eastness,mean_northness]\n\n  # Save output data into CSV format\n  df_station.set_index(stations_df['station_name'][0:idx+1],inplace=True)\n  df_station.to_csv(stations_outfile)\n\n\ndef add_more_points_to_the_gridcells():\n  # check how many points are in the current grid_cell json\n  station_cell_mapping = f\"{work_dir}/station_cell_mapping.csv\"\n  current_grid_df = pd.read_csv(station_cell_mapping)\n  \n  print(current_grid_df.columns)\n  print(current_grid_df.shape)\n  \n  western_us_coords = f'{work_dir}/dem_file.tif.csv'\n  dem_df = pd.read_csv(western_us_coords)\n  print(dem_df.head())\n  print(dem_df.shape)\n  filtered_df = dem_df[dem_df['Elevation'] > 20]  # choose samples from points higher than 20 meters\n\n  # Randomly choose 700 rows from the filtered DataFrame\n  random_rows = filtered_df.sample(n=700)\n  random_rows = random_rows[[\"Latitude\", \"Longitude\"]]\n  random_rows.rename(columns={\n    'Latitude': 'lat', \n    'Longitude': 'lon'\n  }, inplace=True)\n  previous_cells = current_grid_df[[\"lat\", \"lon\"]]\n  result_df = previous_cells.append(random_rows, ignore_index=True)\n  print(result_df.shape)\n  result_df.to_csv(f\"{work_dir}/new_training_points_with_random_dem_locations.csv\")\n  print(f\"New training points are saved to {work_dir}/new_training_points_with_random_dem_locations.csv\")\n  \n  \n  \n  # find the random points that are on land from the dem.json\n  \n  # merge the grid_cell.json with the new dem points into a new grid_cell.json\n  \ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n    #print(\"lat_diff = \", lat_diff)\n    #print(\"lon_diff = \", lon_diff)\n\n    #lat_idx = np.argmin(lat_diff)\n    #lon_idx = np.argmin(lon_diff)\n    # Find the indices corresponding to the minimum differences\n    #lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n    row_idx = np.argmin(lat_diff + lon_diff)\n\n    return row_idx\n  \n  \ndef read_terrain_from_dem_csv():\n  western_us_coords = f'{work_dir}/dem_all.csv'\n  western_df = pd.read_csv(western_us_coords)\n  print(\"western_df.head() = \", western_df.head())\n  \n  stations_file_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n  print(\"stations_file_df.head() = \", stations_file_df.head())\n  \n  def find_closest_dem_row(row, western_df):\n    #print(row)\n    row_idx = find_closest_index(\n      row[\"latitude\"],\n      row[\"longitude\"],\n      western_df[\"Latitude\"], \n      western_df[\"Longitude\"]\n    )\n    #print(\"row_idx = \", row_idx)\n    dem_row = western_df.iloc[row_idx]\n    #print(\"dem_row = \", dem_row)\n    new_row = pd.concat([row, dem_row], axis=0)\n    #print(\"result_series = \", new_row)\n    #exit(1)\n    return new_row\n  \n  stations_file_df = stations_file_df.apply(find_closest_dem_row, args=(western_df,), axis=1)\n  stations_file_df.to_csv(stations_outfile, index=False)\n  print(f\"New elevation csv is aved to {stations_outfile}\")\n  \n\nif __name__ == \"__main__\":\n  try:\n    #prepareGridCellTerrain()\n    #prepareStationTerrain()\n    \n    #add_more_points_to_the_gridcells()\n    read_terrain_from_dem_csv()\n  except:\n    traceback.print_exc(file=sys.stdout)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "525l8q",
  "name" : "data_gee_modis_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport eeauth as e\n\n#exit() # done, uncomment if you want to download new files.\n\ntry:\n    ee.Initialize(e.creds())\nexcept Exception as e:\n    # the following is for the server\n    #service_account = 'eartheginegcloud@earthengine58.iam.gserviceaccount.com'\n#creds = ee.ServiceAccountCredentials(\n    #service_account, '/home/chetana/bhargavi-creds.json')\n    #ee.Initialize(creds)\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\n\n#org_name = 'sentinel1'\n#product_name = 'COPERNICUS/S1_GRD'\n#var_name = 'VV'\n#column_name = 's1_grd_vv'\n\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n    \n    try:\n      \n  \t  print(station_cell_mapper_df['station_id'][ind], station_cell_mapper_df['cell_id'][ind])\n  \t  current_cell_id = station_cell_mapper_df['cell_id'][ind]\n  \t  print(\"collecting \", current_cell_id)\n  \t  single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/modis/{column_name}_{current_cell_id}.csv\"\n\n  \t  if os.path.exists(single_csv_file):\n  \t    print(\"exists skipping..\")\n  \t    continue\n\n  \t  longitude = station_cell_mapper_df['lon'][ind]\n  \t  latitude = station_cell_mapper_df['lat'][ind]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate('2013-01-01','2017-12-31')\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df1 = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  viirs2 = ee.ImageCollection(product_name).filterDate('2018-01-01','2021-12-31')\n  \t  poi_reduced_imgs2 = viirs2.map(poi_mean)\n  \t  nested_list2 = poi_reduced_imgs2.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df2 = pd.DataFrame(nested_list2.getInfo(), columns=['date',column_name])\n      \n\n  \t  df = pd.concat([df1, df2])\n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n  \t  print(e)\n  \t  pass\n    \n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "7temiv",
  "name" : "data_gee_sentinel1_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import work_dir\n\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{work_dir}/testing_points.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'lat', 'lon'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      #current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      #print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{work_dir}/{org_name}_{column_name}_{ind}.csv\"\n\n#       if os.path.exists(single_csv_file):\n#           print(\"exists skipping..\")\n#           continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      #poi = ee.Geometry.Point(longitude, latitude)\n      viirs = ee.ImageCollection(product_name).filterDate('2017-10-01','2018-07-01').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      #df['cell_id'] = current_cell_id\n      df['lat'] = latitude\n      df['lon'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nprint(all_cell_df.head())\nprint(all_cell_df[\"s1_grd_vv\"].describe())\nall_cell_df.to_csv(f\"{work_dir}/Sentinel1_Testing.csv\")\nprint(\"The Sentinel 1 is downloaded successfully. \")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rmxece",
  "name" : "data_associate_station_grid_cell",
  "description" : null,
  "code" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\nfrom snowcast_utils import work_dir, read_json_file\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n\n\nif __name__ == \"__main__\":\n  \n    # pd.set_option('display.max_columns', None)\n\n    # read the grid geometry file\n    homedir = os.path.expanduser('~')\n    print(homedir)\n    github_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n    # read grid cell\n    gridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\n    model_dir = f\"{github_dir}/model/\"\n    training_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\n    testing_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\n    train_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\n    ground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\n    station_list_file = f\"{work_dir}/training_snotel_station_list_elevation.csv\"\n\n    ready_for_training_folder = f\"{github_dir}/data/ready_for_training/\"\n\n    result_mapping_file = f\"{ready_for_training_folder}station_cell_mapping.csv\"\n\n    station_locations = read_json_file(f'{work_dir}/snotelStations.json')\n    # print(station_locations)\n\n    result_df = pd.DataFrame(columns=['station_name', 'elevation', 'lat', 'lon'])\n    for station in station_locations:\n        print(f'station {station[\"name\"]} completed.')\n\n        location_name = station['name']\n        location_triplet = station['triplet']\n        location_elevation = station['elevation']\n        location_station_lat = station['location']['lat']\n        location_station_long = station['location']['lng']\n        new_df = pd.DataFrame([{\n            'station_name': location_name,\n            'elevation': location_elevation,\n            'lat': location_station_lat,\n            'lon': location_station_long\n        }])\n        result_df = pd.concat([result_df, new_df], axis=0, ignore_index=True)\n             \n              \n    print(result_df)\n    # Save the DataFrame to a CSV file\n    result_df.to_csv(station_list_file, index=False)\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "illwc1",
  "name" : "data_gee_modis_real_time",
  "description" : null,
  "code" : "import os\nimport pprint\n\n# import gdal\nimport subprocess\nfrom datetime import datetime, timedelta\n\n# set up your credentials using\n# echo 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\n\nmodis_download_dir = \"/home/chetana/modis_download_folder/\"\nmodis_downloaded_data = modis_download_dir + \"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\ngeo_tiff = modis_download_dir + \"geo-tiff/\"\nvrt_file_dir = modis_download_dir + \"vrt_files/\"\ndir_path = os.path.dirname(os.path.realpath(__file__))\nprint(dir_path)\n\ntile_list = ['h09v04', 'h10v04', 'h11v04', 'h08v04', 'h08v05', 'h09v05', 'h10v05', 'h07v06', 'h08v06', 'h09v06']\n\n\ndef get_files(directory):\n    \"\"\"\n    Get a list of files in a directory and its subdirectories.\n\n    Args:\n        directory (str): The directory to search for files.\n\n    Returns:\n        dict: A dictionary where keys are subdirectory names and values are lists of file paths.\n    \"\"\"\n    file_directory = list()\n    complete_directory_structure = dict()\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            file_directory.append(file_path)\n            complete_directory_structure[str(dirpath).rsplit('/')[-1]] = file_directory\n\n    return complete_directory_structure\n\n\ndef get_latest_date():\n    \"\"\"\n    Retrieve the latest date from the MODIS data website.\n\n    Returns:\n        datetime: The latest date as a datetime object.\n    \"\"\"\n    all_rows = get_web_row_data()\n\n    latest_date = None\n    for row in all_rows:\n        try:\n            new_date = datetime.strptime(row.text[:-1], '%Y.%m.%d')\n            if latest_date is None or latest_date < new_date:\n                latest_date = new_date\n        except:\n            continue\n\n    print(\"Find the latest date: \", latest_date.strftime(\"%Y.%m.%d\"))\n    second_latest_date = latest_date - timedelta(days=8)\n    return second_latest_date\n\n\ndef get_web_row_data():\n    \"\"\"\n    Fetch and parse the MODIS data website content.\n\n    Returns:\n        list: A list of rows from the website's table.\n    \"\"\"\n    try:\n        from BeautifulSoup import BeautifulSoup\n    except ImportError:\n        from bs4 import BeautifulSoup\n    modis_list_url = \"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\n    print(\"Source / Product: \" + modis_list_url)\n    if os.path.exists(\"index.html\"):\n        os.remove(\"index.html\")\n    subprocess.run(\n        f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies '\n        f'--no-check-certificate --auth-no-challenge=on -np -e robots=off {modis_list_url}',\n        shell=True, stderr=subprocess.PIPE)\n    index_file = open('index.html', 'r')\n    webContent = index_file.read()\n    parsed_html = BeautifulSoup(webContent, \"html.parser\")\n    all_rows = parsed_html.body.findAll('td', attrs={'class': 'indexcolname'})\n    return all_rows\n\n\ndef download_recent_modis(date=None):\n    \"\"\"\n    Download recent MODIS data.\n\n    Args:\n        date (datetime, optional): A specific date to download. Defaults to None.\n    \"\"\"\n    if date:\n        latest_date_str = date.strftime(\"%Y.%m.%d\")\n    else:\n        latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n    for tile in tile_list:\n        download_cmd = f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies ' \\\n                       f'--no-check-certificate --auth-no-challenge=on -r --reject \"i' \\\n                       f'ndex.html*\" -P {modis_download_dir} -np -e robots=off ' \\\n                       f'https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/ -A \"*{tile}*.hdf\" --quiet'\n        # print(download_cmd)\n        p = subprocess.run(download_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        print(\"Downloading tile, \", tile, \" with status code \", \"OK\" if p.returncode == 0 else p.returncode)\n\n\n# def merge_wrap_tif_into_western_us_tif():\n#     latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n#     # traverse the folder and find the new download files\n#     for filename in os.listdir(f\"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/\"):\n#         f = os.path.join(directory, filename)\n#         # checking if it is a file\n#         if os.path.isfile(f):\n#             print(f)\n# merge_wrap_tif_into_western_us_tif()\n\ndef hdf_tif_cvt(resource_path, destination_path):\n    \"\"\"\n    Convert HDF files to GeoTIFF format.\n\n    Args:\n        resource_path (str): The path to the source HDF file.\n        destination_path (str): The path to save the converted GeoTIFF file.\n    \"\"\"\n    if not os.path.isfile(resource_path):\n        raise Exception(\"HDF file not found\")\n\n    max_snow_extent_path = destination_path + \"maximum_snow_extent/\"\n    eight_day_snow_cover = destination_path + \"eight_day_snow_cover/\"\n    if not os.path.exists(max_snow_extent_path):\n        os.makedirs(max_snow_extent_path)\n    if not os.path.exists(eight_day_snow_cover):\n        os.makedirs(eight_day_snow_cover)\n\n    tif_file_name_snow_extent = max_snow_extent_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_name_eight_day = eight_day_snow_cover + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name_snow_extent + '_max_snow_extent' + tif_file_extension\n    eight_day_snow_cover_file_name = tif_file_name_eight_day + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Maximum_Snow_Extent\"\n    eight_day_snow_cover = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Eight_Day_Snow_Cover\"\n\n    subprocess.run(f\"gdal_translate {maximum_snow_extent} {maximum_snow_extent_file_name}\", shell=True)\n    subprocess.run(f\"gdal_translate {eight_day_snow_cover} {eight_day_snow_cover_file_name}\", shell=True)\n\n\ndef combine_geotiff_gdal(vrt_array, destination):\n    \"\"\"\n    Combine GeoTIFF files using GDAL.\n\n    Args:\n        vrt_array (list): A list of GeoTIFF file paths to combine.\n        destination (str): The path to save the combined VRT and GeoTIFF files.\n    \"\"\"\n    subprocess.run(f\"gdalbuildvrt {destination} {' '.join(vrt_array)}\", shell=True)\n    tif_name = destination.split('.vrt')[-2] + '.tif'\n    subprocess.run(f\"gdal_translate -of GTiff {destination} {tif_name}\", shell=True)\n\n\ndef hdf_tif_conversion(resource_path, destination_path):\n    \"\"\"\n    Convert HDF files to GeoTIFF format using GDAL.\n\n    Args:\n        resource_path (str): The path to the source HDF file.\n        destination_path (str): The path to save the converted GeoTIFF file.\n    \"\"\"\n    hdf_dataset = gdal.Open(resource_path)\n    if hdf_dataset is None:\n        raise Exception(\"Could not open HDF dataset\")\n\n    maximum_snow_extent = hdf_dataset.GetSubDatasets()[0][0]\n    modis_snow_500m = hdf_dataset.GetSubDatasets()[1][0]\n\n    driver = gdal.GetDriverByName('GTiff')\n\n    tif_file_name = destination_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name + '_max_snow_extent' + tif_file_extension\n    modis_snow_500m_file_name = tif_file_name + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent_dataset = gdal.Open(maximum_snow_extent)\n    modis_snow_500m_dataset = gdal.Open(modis_snow_500m)\n\n    if maximum_snow_extent_dataset is None:\n        raise Exception(\"Could not open maximum_snow_extent dataset\")\n\n    if modis_snow_500m_dataset is None:\n        raise Exception(\"Could not open modis_snow_500m dataset\")\n\n    driver.CreateCopy(maximum_snow_extent_file_name, maximum_snow_extent_dataset, 0)\n    driver.CreateCopy(modis_snow_500m_file_name, modis_snow_500m_dataset, 0)\n\n    print(\"HDF to TIF conversion completed successfully.\")\n\n\ndef download_modis_archive(*, start_date, end_date):\n    \"\"\"\n    Download MODIS data for a specified date range.\n\n    Keyword Args:\n        start_date (datetime): The start date of the date range.\n        end_date (datetime): The end date of the date range.\n    \"\"\"\n    all_archive_dates = list()\n\n    all_rows = get_web_row_data()\n    for r in all_rows:\n        try:\n            all_archive_dates.append(datetime.strptime(r.text.replace('/', ''), '%Y.%m.%d'))\n        except:\n            continue\n\n    for a in all_archive_dates:\n        if start_date <= a <= end_date:\n            download_recent_modis(a)\n\n\ndef step_one_download_modis():\n  \"\"\"\n  Step one of the main workflow: Download recent MODIS data.\n  \"\"\"\n  download_recent_modis()\n                   \ndef step_two_merge_modis_western_us():\n  \"\"\"\n  Step two of the main workflow: Merge MODIS data for the western US.\n  \"\"\"\n  download_modis_archive(start_date=datetime(2022, 1, 1), end_date=datetime(2022, 12, 31))\n\n  files = get_files(modis_downloaded_data)\n  for k, v in get_files(modis_downloaded_data).items():\n\n    conversion_path = modis_download_dir + \"geo-tiff/\" + k + \"/\"\n    if not os.path.exists(conversion_path):\n        os.makedirs(conversion_path)\n    for hdf_file in v:\n        # print(hdf_file.split('/')[-1].split('.hdf')[0], 1)\n        hdf_tif_cvt(hdf_file, conversion_path)\n\n  if not os.path.exists(vrt_file_dir):\n    os.makedirs(vrt_file_dir)\n\n\n  directories = [d for d in os.listdir(geo_tiff) if   os.path.isdir(os.path.join(geo_tiff, d))]\n\n  for d in directories:\n    eight_day_snow_cover = geo_tiff + d + '/eight_day_snow_cover'\n    maximum_snow_extent = geo_tiff + d + '/maximum_snow_extent'\n\n    eight_day_abs_path = list()\n    snow_extent_abs_path = list()\n\n    for file in os.listdir(eight_day_snow_cover):\n        file_path = os.path.abspath(os.path.join(eight_day_snow_cover, file))\n        eight_day_abs_path.append(file_path)\n\n    for file in os.listdir(maximum_snow_extent):\n        file_path = os.path.abspath(os.path.join(maximum_snow_extent, file))\n        snow_extent_abs_path.append(file_path)\n\n    combine_geotiff_gdal(eight_day_abs_path, vrt_file_dir + f\"{d}_eight_day.vrt\")\n    combine_geotiff_gdal(snow_extent_abs_path, vrt_file_dir + f\"{d}_snow_extent.vrt\")\n\n                   \n# main workflow is here:\nstep_one_download_modis()\nstep_two_merge_modis_western_us()\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "sjs5by",
  "name" : "data_gee_sentinel1_real_time",
  "description" : null,
  "code" : "# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernel\n\nfrom all_dependencies import *\nfrom snowcast_utils import *\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # This must be run in the terminal instead of Geoweaver. Geoweaver doesn't support prompts.\n    ee.Initialize()\n\n# Read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# Read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# Read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\n\nprint(\"submission_format_df shape: \", submission_format_df.shape)\n\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_df = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n# Start_date = \"2022-04-20\" # Test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1\", \"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\nif os.path.exists(final_csv_file):\n    #print(\"exists skipping..\")\n    #exit()\n    os.remove(final_csv_file)\n\nall_cell_df = pd.DataFrame(columns=['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor current_cell_id in submission_format_df.index:\n\n    try:\n        #print(\"collecting \", current_cell_id)\n\n        longitude = all_cell_coords_df['lon'][current_cell_id]\n        latitude = all_cell_coords_df['lat'][current_cell_id]\n\n        # Identify a 500-meter buffer around our Point Of Interest (POI)\n        poi = ee.Geometry.Point(longitude, latitude).buffer(10)\n\n        viirs = ee.ImageCollection(product_name) \\\n            .filterDate(start_date, end_date) \\\n            .filterBounds(poi) \\\n            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n            .select('VV')\n\n        def poi_mean(img):\n            reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n            mean = reducer.get(var_name)\n            return img.set('date', img.date().format()).set(column_name, mean)\n\n        poi_reduced_imgs = viirs.map(poi_mean)\n\n        nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date', column_name]).values().get(0)\n\n        # Don't forget we need to call the callback method \"getInfo\" to retrieve the data\n        df = pd.DataFrame(nested_list.getInfo(), columns=['date', column_name])\n\n        df['date'] = pd.to_datetime(df['date'])\n        df = df.set_index('date')\n\n        df['cell_id'] = current_cell_id\n        df['latitude'] = latitude\n        df['longitude'] = longitude\n\n        df_list = [all_cell_df, df]\n        all_cell_df = pd.concat(df_list)  # Merge into a big dataframe\n\n    except Exception as e:\n\n        #print(e)\n        pass\n\nall_cell_df.to_csv(final_csv_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "y7nb46",
  "name" : "base_hole",
  "description" : null,
  "code" : "'''\nThe wrapper for all the snowcast_wormhole predictors.\n'''\n\nimport os\nimport joblib\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport shutil\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nclass BaseHole:\n    '''\n    Base class for snowcast_wormhole predictors.\n\n    Attributes:\n        all_ready_file (str): The path to the CSV file containing the data for training.\n        classifier: The machine learning model used for prediction.\n        holename (str): The name of the wormhole class.\n        train_x (numpy.ndarray): The training input data.\n        train_y (numpy.ndarray): The training target data.\n        test_x (numpy.ndarray): The testing input data.\n        test_y (numpy.ndarray): The testing target data.\n        test_y_results (numpy.ndarray): The predicted results on the test data.\n        save_file (str): The path to save the trained model.\n    '''\n\n    all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready_new.csv\"\n\n    def __init__(self):\n        '''\n        Initializes a new instance of the BaseHole class.\n        '''\n        self.classifier = self.get_model()\n        self.holename = self.__class__.__name__ \n        self.train_x = None\n        self.train_y = None\n        self.test_x = None\n        self.test_y = None\n        self.test_y_results = None\n        self.save_file = None\n    \n    def save(self):\n        '''\n        Save the trained model to a joblib file with a timestamp.\n\n        Returns:\n            None\n        '''\n        now = datetime.now()\n        date_time = now.strftime(\"%Y%d%m%H%M%S\")\n        self.save_file = f\"{github_dir}/model/wormhole_{self.holename}_{date_time}.joblib\"\n        \n        directory = os.path.dirname(self.save_file)\n        if not os.path.exists(directory):\n          os.makedirs(directory)\n        \n        print(f\"Saving model to {self.save_file}\")\n        joblib.dump(self.classifier, self.save_file)\n        # copy a version to the latest file placeholder\n        latest_copy_file = f\"{github_dir}/model/wormhole_{self.holename}_latest.joblib\"\n        shutil.copy(self.save_file, latest_copy_file)\n        print(f\"a copy of the model is saved to {latest_copy_file}\")\n  \n    def preprocessing(self):\n        '''\n        Preprocesses the data for training and testing.\n\n        Returns:\n            None\n        '''\n        all_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n        print(\"all columns: \", all_ready_pd.columns)\n        all_ready_pd = all_ready_pd[all_cols]\n        all_ready_pd = all_ready_pd.dropna()\n        train, test = train_test_split(all_ready_pd, test_size=0.2)\n        self.train_x, self.train_y = train[input_columns].to_numpy().astype('float'), train[['swe_value']].to_numpy().astype('float')\n        self.test_x, self.test_y = test[input_columns].to_numpy().astype('float'), test[['swe_value']].to_numpy().astype('float')\n  \n    def train(self):\n        '''\n        Trains the machine learning model.\n\n        Returns:\n            None\n        '''\n        self.classifier.fit(self.train_x, self.train_y)\n  \n    def test(self):\n        '''\n        Tests the machine learning model on the testing data.\n\n        Returns:\n            numpy.ndarray: The predicted results on the testing data.\n        '''\n        self.test_y_results = self.classifier.predict(self.test_x)\n        return self.test_y_results\n  \n    def predict(self, input_x):\n        '''\n        Makes predictions using the trained model on new input data.\n\n        Args:\n            input_x (numpy.ndarray): The input data for prediction.\n\n        Returns:\n            numpy.ndarray: The predicted results.\n        '''\n        return self.classifier.predict(input_x)\n  \n    def evaluate(self):\n        '''\n        Evaluates the performance of the machine learning model.\n\n        Returns:\n            None\n        '''\n        pass\n  \n    def get_model(self):\n        '''\n        Get the machine learning model.\n\n        Returns:\n            object: The machine learning model.\n        '''\n        pass\n  \n    def post_processing(self):\n        '''\n        Perform post-processing on the model's predictions.\n\n        Returns:\n            None\n        '''\n        pass\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "a8p3n7",
  "name" : "data_gee_gridmet_station_only",
  "description" : null,
  "code" : "import os\nimport glob\nimport urllib.request\nfrom datetime import date, datetime\n\nimport pandas as pd\nimport xarray as xr\nfrom pathlib import Path\nfrom snowcast_utils import work_dir, train_start_date, train_end_date\nimport warnings\nimport dask.dataframe as dd\nfrom dask.delayed import delayed\n\n# Suppress FutureWarnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nstart_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\nend_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n\nyear_list = [start_date.year + i for i in range(end_date.year - start_date.year + 1)]\n\nworking_dir = work_dir\n#stations = pd.read_csv(f'{working_dir}/station_cell_mapping.csv')\nall_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\nstations = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\ngridmet_save_location = f'{working_dir}/gridmet_climatology'\nfinal_merged_csv = f\"{work_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv\"\n\n\ndef get_files_in_directory():\n    f = list()\n    for files in glob.glob(gridmet_save_location + \"/*.nc\"):\n        f.append(files)\n    return f\n\n\ndef download_file(url, save_location):\n    try:\n        print(\"download_file\")\n        with urllib.request.urlopen(url) as response:\n            file_content = response.read()\n        file_name = os.path.basename(url)\n        save_path = os.path.join(save_location, file_name)\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\n\ndef download_gridmet_climatology():\n    folder_name = gridmet_save_location\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            print(\"downloading\", download_link)\n            if not os.path.exists(os.path.join(folder_name, var + '_' + '%s' % y + '.nc')):\n                download_file(download_link, folder_name)\n\n\ndef process_station(ds, lat, lon):\n    subset_data = ds.sel(lat=lat, lon=lon, method='nearest')\n    subset_data['lat'] = lat\n    subset_data['lon'] = lon\n    converted_df = subset_data.to_dataframe()\n    converted_df = converted_df.reset_index(drop=False)\n    converted_df = converted_df.drop('crs', axis=1)\n    return converted_df\n\ndef get_gridmet_variable(file_name):\n    print(f\"Reading values from {file_name}\")\n    ds = xr.open_dataset(file_name)\n    var_name = list(ds.keys())[0]\n\n    csv_file = f'{gridmet_save_location}/{Path(file_name).stem}_snotel_ghcnd.csv'\n    if os.path.exists(csv_file):\n        print(f\"The file '{csv_file}' exists.\")\n        return\n\n    result_data = []\n    for _, row in stations.iterrows():\n        delayed_process_data = delayed(process_station)(ds, row['latitude'], row['longitude'])\n        result_data.append(delayed_process_data)\n\n    print(\"ddf = dd.from_delayed(result_data)\")\n    ddf = dd.from_delayed(result_data)\n    \n    print(\"result_df = ddf.compute()\")\n    result_df = ddf.compute()\n    result_df.to_csv(csv_file, index=False)\n    print(f'Completed extracting data for {file_name}')\n\n\ndef merge_similar_variables_from_different_years():\n    files = os.listdir(gridmet_save_location)\n    file_groups = {}\n\n    for filename in files:\n        base_name, year_ext = os.path.splitext(filename)\n        parts = base_name.split('_')\n        print(parts)\n        print(year_list)\n        if len(parts) == 4 and parts[3] == \"ghcnd\" and year_ext == '.csv' and int(parts[1]) in year_list:\n            file_groups.setdefault(parts[0], []).append(filename)\n\n    for base_name, file_list in file_groups.items():\n        if len(file_list) > 1:\n            dfs = []\n            for filename in file_list:\n                df = pd.read_csv(os.path.join(gridmet_save_location, filename))\n                dfs.append(df)\n            merged_df = pd.concat(dfs, ignore_index=True)\n            merged_filename = f\"{base_name}_merged_snotel_ghcnd.csv\"\n            merged_df.to_csv(os.path.join(gridmet_save_location, merged_filename), index=False)\n            print(f\"Merged {file_list} into {merged_filename}\")\n\n\ndef merge_all_variables_together():\n    merged_df = None\n    file_paths = []\n\n    for filename in os.listdir(gridmet_save_location):\n        if filename.endswith(\"_merged_snotel_ghcnd.csv\"):\n            file_paths.append(os.path.join(gridmet_save_location, filename))\n\t\n    rmin_merged_path = os.path.join(gridmet_save_location, 'rmin_merged_snotel_ghcnd.csv')\n    rmax_merged_path = os.path.join(gridmet_save_location, 'rmax_merged_snotel_ghcnd.csv')\n    tmmn_merged_path = os.path.join(gridmet_save_location, 'tmmn_merged_snotel_ghcnd.csv')\n    tmmx_merged_path = os.path.join(gridmet_save_location, 'tmmx_merged_snotel_ghcnd.csv')\n    \n    df_rmin = pd.read_csv(rmin_merged_path)\n    df_rmax = pd.read_csv(rmax_merged_path)\n    df_tmmn = pd.read_csv(tmmn_merged_path)\n    df_tmmx = pd.read_csv(tmmx_merged_path)\n    \n    df_rmin.rename(columns={'relative_humidity': 'relative_humidity_rmin'}, inplace=True)\n    df_rmax.rename(columns={'relative_humidity': 'relative_humidity_rmax'}, inplace=True)\n    df_tmmn.rename(columns={'air_temperature': 'air_temperature_tmmn'}, inplace=True)\n    df_tmmx.rename(columns={'air_temperature': 'air_temperature_tmmx'}, inplace=True)\n    \n    df_rmin.to_csv(rmin_merged_path)\n    df_rmax.to_csv(rmax_merged_path)\n    df_tmmn.to_csv(tmmn_merged_path)\n    df_tmmx.to_csv(tmmx_merged_path)\n    \n    if file_paths:\n        merged_df = pd.read_csv(file_paths[0])\n        for file_path in file_paths[1:]:\n            df = pd.read_csv(file_path)\n            merged_df = pd.concat([merged_df, df], axis=1)\n        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n        merged_df.to_csv(final_merged_csv, index=False)\n        print(f\"all files are saved to {final_merged_csv}\")\n\n\nif __name__ == \"__main__\":\n    \n    #download_gridmet_climatology()\n    \n    # mock out as this takes too long\n    #nc_files = get_files_in_directory()\n    #for nc in nc_files:\n    #.   # should check if the nc file year number is in the year_list\n    #    get_gridmet_variable(nc)\n    \n    merge_similar_variables_from_different_years()\n    merge_all_variables_together()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "smsdr0",
  "name" : "data_gee_gridmet_real_time",
  "description" : null,
  "code" : "import os\nimport urllib\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Download the NetCDF files from the Idaho HTTP site daily or for the time period matching the MODIS period.\n# Download site: https://www.northwestknowledge.net/metdata/data/\n\ndownload_source = \"https://www.northwestknowledge.net/metdata/data/\"\ngridmet_download_dir = \"/home/chetana/terrian_data/\"\n\ndef download_gridmet():\n    \"\"\"\n    Download GridMET NetCDF files from the specified source to the download directory.\n\n    This function fetches NetCDF files from the provided website and saves them in the specified download directory.\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(gridmet_download_dir):\n        os.makedirs(gridmet_download_dir)\n\n    soup = BeautifulSoup(requests.get(download_source).text, \"html.parser\")\n    tag_links = soup.find_all('a')\n    for t in tag_links:\n        if '.nc' in t.text and not 'eddi' in t.text and not os.path.isfile(gridmet_download_dir + t.get(\"href\")):\n            print(f'Downloading {t.get(\"href\")}')\n            urllib.request.urlretrieve(download_source + t.get('href'), gridmet_download_dir + t.get(\"href\"))\n\ndownload_gridmet()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "4i0sop",
  "name" : "model_creation_xgboost",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the XGBoostHole class, which is used to train and evaluate an Extra Trees Regression model for hole analysis.\n\nAttributes:\n    XGBoostHole (class): A class for training and using an Extra Trees Regression model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n\"\"\"\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass XGBoostHole(RandomForestHole):\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n        return etmodel\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b63prf",
  "name" : "testing_data_integration",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom datetime import datetime\nfrom snowcast_utils import homedir, work_dir, test_start_date\nimport sys\nimport numpy as np\n\ndef get_water_year(date):\n    if date.month >= 10:  # If the month is October or later\n        return date.year + 1  # Water year starts in the following calendar year\n    else:\n        return date.year\n\ndef merge_all_gridmet_amsr_csv_into_one(gridmet_csv_folder, dem_all_csv, testing_all_csv, water_mask_csv):\n    \"\"\"\n    Merge all GridMET and AMSR CSV files into one combined CSV file.\n\n    Args:\n        gridmet_csv_folder (str): The folder containing GridMET CSV files.\n        dem_all_csv (str): Path to the DEM (Digital Elevation Model) CSV file.\n        testing_all_csv (str): Path to save the merged CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # List of file paths for the CSV files\n    csv_files = []\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('_cumulative.csv') and test_start_date in file:\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    all_df = None\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        print(f\"reading {file}\")\n        df = pd.read_csv(file)\n        print(df.head())\n        print(\"df.shape:\", df.shape)\n        df = df.apply(pd.to_numeric, errors='coerce')\n        if all_df is None:\n          all_df = df\n        else:\n          #all_df = all_df.merge(df, on=['Latitude', 'Longitude']).drop_duplicates()\n          df = df.drop(columns=['Latitude', 'Longitude'])\n          all_df = pd.concat([all_df, df], axis=1)\n          \n        print(\"all_df.head() :\", all_df.head())\n        print(\"all_df.columns\", all_df.columns)\n        print(\"all_df.shape: \", all_df.shape)\n\n    unique_loc_pairs = all_df[['Latitude', 'Longitude']].drop_duplicates()\n    print(\"unique_loc_pairs.shape: \", unique_loc_pairs.shape)\n        \n    dem_df = pd.read_csv(f\"{work_dir}/dem_all.csv\", encoding='utf-8', index_col=False)\n    #all_df = pd.merge(all_df, dem_df, on=['Latitude', 'Longitude']).drop_duplicates()\n    dem_df = dem_df.drop(columns=['Latitude', 'Longitude'])\n    print(\"dem_df.shape: \", dem_df.shape)\n    all_df = pd.concat([all_df, dem_df], axis=1)\n\n    date = test_start_date\n    \n    #date = date.replace(\"-\", \".\")\n    amsr_file = f'{work_dir}/testing_ready_amsr_{date}_cumulative.csv'\n    print(f\"reading {amsr_file}\")\n    amsr_df = pd.read_csv(amsr_file, index_col=False)\n    amsr_df.rename(columns={'gridmet_lat': 'Latitude', 'gridmet_lon': 'Longitude'}, inplace=True)\n    print(amsr_df.head())\n    print(\"amsr_df.shape = \", amsr_df.shape)\n    amsr_df = amsr_df.drop(columns=['Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, amsr_df], axis=1)\n    \n    fsca_df = pd.read_csv(f'{homedir}/fsca/final_output/{test_start_date}_output.csv_cumulative.csv')\n    print(fsca_df.head())\n    print(\"fsca_df.shape: \", fsca_df.shape)\n    fsca_df = fsca_df.drop(columns=['Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, fsca_df], axis=1)\n\n    water_mask_df = pd.read_csv(water_mask_csv)\n    water_mask_df = water_mask_df.drop(columns=[\"date\", 'Latitude', 'Longitude'])\n    all_df = pd.concat([all_df, water_mask_df], axis=1)\n    \n    print(\"all columns: \", all_df.columns)\n    # add water year\n    all_df[\"water_year\"] = get_water_year(selected_date)\n    \n    all_df.rename(columns={'date_x': 'date'}, inplace=True)\n    \n    # log10 all the cumulative columns\n    # Get columns with \"cumulative\" in their names\n    for col in all_df.columns:\n        print(\"Checking \", col)\n        if \"cumulative\" in col:\n\t        # Apply log10 transformation to selected columns\n            all_df[col] = np.log10(all_df[col] + 0.1)  # Adding 1 to avoid log(0)\n            print(f\"converted {col} to log10\")\n    \n    # Save the merged dataframe to a new CSV file\n    all_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input CSV files are merged to {testing_all_csv}\")\n    print(all_df.columns)\n    print(\"all_df.shape = \", all_df.shape)\n    print(all_df.describe(include='all'))\n    print(all_df[\"fsca\"].describe())\n    print(all_df[\"cumulative_fsca\"].describe())\n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    gridmet_csv_folder = f\"{work_dir}/gridmet_climatology/\"\n    test_year = int(test_start_date[:4])\n\n    merge_all_gridmet_amsr_csv_into_one(f\"{work_dir}/testing_output/\",\n                                        f\"{work_dir}/dem_all.csv\",\n                                        f\"{work_dir}/testing_all_ready_{test_start_date}.csv\",\n                                        f\"{homedir}/water_mask/final_output/{test_year-1}_output.csv\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "zh38b6",
  "name" : "snowcast_utils",
  "description" : null,
  "code" : "from datetime import date, datetime, timedelta\nimport json\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ntoday = date.today()\n\n# dd/mm/YY\nd1 = today.strftime(\"%Y-%m-%d\")\nprint(\"today date =\", d1)\n\n# -125, 25, -100, 49\nsouthwest_lon = -125.0\nsouthwest_lat = 25.0\nnortheast_lon = -100.0\nnortheast_lat = 49.0\n\n# the training period is three years from 2018 to 2021\ntrain_start_date = \"2018-01-03\"\ntrain_end_date = \"2021-12-31\"\n\ndef get_operation_day():\n  # Get the current date and time\n  current_date = datetime.now()\n\n  # Calculate three days ago\n  three_days_ago = current_date - timedelta(days=3)\n\n  # Format the date as a string\n  three_days_ago_string = three_days_ago.strftime(\"%Y-%m-%d\")\n\n  print(three_days_ago_string)\n  return three_days_ago_string\n\ntest_start_date = get_operation_day()\n#test_start_date = \"2024-03-20\" # use this for debugging and generating SWE map for specific day\ntest_end_date = \"2024-5-19\"\n#test_end_date = d1\nprint(\"test start date: \", test_start_date)\nprint(\"test end date: \", test_end_date)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nwork_dir = f\"{homedir}/gridmet_test_run\"\n\n# Define a function to convert the month to season\ndef month_to_season(month):\n    if 3 <= month <= 5:\n        return 1\n    elif 6 <= month <= 8:\n        return 2\n    elif 9 <= month <= 11:\n        return 3\n    else:\n        return 4\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the distance (Euclidean) between two sets of coordinates (lat1, lon1) and (lat2, lon2).\n    \n    Parameters:\n    - lat1 (float): Latitude of the first point.\n    - lon1 (float): Longitude of the first point.\n    - lat2 (float): Latitude of the second point.\n    - lon2 (float): Longitude of the second point.\n    \n    Returns:\n    - float: The Euclidean distance between the two points.\n    \"\"\"\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n\ndef read_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8-sig') as json_file:\n        data = json.load(json_file)\n        return data\n\ndef create_cell_location_csv():\n    \"\"\"\n    Create a CSV file containing cell locations from a GeoJSON file.\n    \"\"\"\n    # read grid cell\n    gridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n    all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n    if os.path.exists(all_cell_coords_file):\n        os.remove(all_cell_coords_file)\n\n    grid_coords_df = pd.DataFrame(columns=[\"cell_id\", \"lat\", \"lon\"])\n    print(grid_coords_df.head())\n    gridcells = geojson.load(open(gridcells_file))\n    for idx, cell in enumerate(gridcells['features']):\n        current_cell_id = cell['properties']['cell_id']\n        cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n        cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n        grid_coords_df.loc[len(grid_coords_df.index)] = [current_cell_id, cell_lat, cell_lon]\n\n    # grid_coords_np = grid_coords_df.to_numpy()\n    # print(grid_coords_np.shape)\n    grid_coords_df.to_csv(all_cell_coords_file, index=False)\n    # np.savetxt(all_cell_coords_file, grid_coords_np[:, 1:], delimiter=\",\")\n    # print(grid_coords_np.shape)\n\ndef get_latest_date_from_an_array(arr, date_format):\n    \"\"\"\n    Get the latest date from an array of date strings.\n    \n    Parameters:\n    - arr (list): List of date strings.\n    - date_format (str): Date format for parsing the date strings.\n    \n    Returns:\n    - str: The latest date string.\n    \"\"\"\n    return max(arr, key=lambda x: datetime.strptime(x, date_format))\n\ndef findLastStopDate(target_testing_dir, data_format):\n    \"\"\"\n    Find the last stop date from CSV files in a directory.\n    \n    Parameters:\n    - target_testing_dir (str): Directory containing CSV files.\n    - data_format (str): Date format for parsing the date strings.\n    \n    Returns:\n    - str: The latest stop date.\n    \"\"\"\n    date_list = []\n    for filename in os.listdir(target_testing_dir):\n        f = os.path.join(target_testing_dir, filename)\n        # checking if it is a file\n        if os.path.isfile(f) and \".csv\" in f:\n            pdf = pd.read_csv(f, header=0, index_col=0)\n            date_list = np.concatenate((date_list, pdf.index.unique()))\n    latest_date = get_latest_date_from_an_array(date_list, data_format)\n    print(latest_date)\n    date_time_obj = datetime.strptime(latest_date, data_format)\n    return date_time_obj.strftime(\"%Y-%m-%d\")\n\ndef convert_date_from_1900(day_value):\n    \"\"\"\n    Convert a day value since 1900 to a date string in the format \"YYYY-MM-DD\".\n    \n    Parameters:\n    - day_value (int): Number of days since January 1, 1900.\n    \n    Returns:\n    - str: Date string in \"YYYY-MM-DD\" format.\n    \"\"\"\n    reference_date = datetime(1900, 1, 1)\n    result_date = reference_date + timedelta(days=day_value)\n    return result_date.strftime(\"%Y-%m-%d\")\n\ndef convert_date_to_1900(date_string):\n    \"\"\"\n    Convert a date string in the format \"YYYY-MM-DD\" to a day value since 1900.\n    \n    Parameters:\n    - date_string (str): Date string in \"YYYY-MM-DD\" format.\n    \n    Returns:\n    - int: Number of days since January 1, 1900.\n    \"\"\"\n    input_date = datetime.strptime(date_string, \"%Y-%m-%d\")\n    reference_date = datetime(1900, 1, 1)\n    delta = input_date - reference_date\n    day_value = delta.days\n    return day_value\n\ndef date_to_julian(date_str):\n    \"\"\"\n    Convert a date to Julian date.\n    \"\"\"\n    date_object = datetime.strptime(date_str, \"%Y-%m-%d\")\n    tt = date_object.timetuple()\n    \n\n    # Format the result as 'YYYYDDD'\n    julian_format = str('%d%03d' % (tt.tm_year, tt.tm_yday))\n\n    return julian_format\n  \n\nif __name__ == \"__main__\":\n    print(date_to_julian(test_start_date))\n    #day_index = convert_date_to_1900(test_start_date)\n    #create_cell_location_csv()\n    #findLastStopDate(f\"{github_dir}/data/sim_testing/gridmet/\", \"%Y-%m-%d %H:%M:%S\")\n    #findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1/\", \"%Y-%m-%d %H:%M:%S\")\n    #findLastStopDate(f\"{github_dir}/data/sat_testing/modis/\", \"%Y-%m-%d\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wdh394",
  "name" : "model_create_kehan",
  "description" : null,
  "code" : "\nfrom BaseHole import *\n\nclass KehanModel(BaseHole):\n\t\n  def preprocessing():\n    pass  \n  \n  def train():\n    pass\n  \n  def test():\n    pass",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "p87wh1",
  "name" : "data_snotel_real_time",
  "description" : null,
  "code" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# First Python script in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\n\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n# Example of using the SnotelPointData class\n# snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n# df = snotel_point.get_daily_data(\n#     datetime(2020, 1, 2), datetime(2020, 1, 20),\n#     [snotel_point.ALLOWED_VARIABLES.SWE]\n# )\n# print(df)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ilbqzg",
  "name" : "all_dependencies",
  "description" : null,
  "code" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\n#pd.set_option('display.max_columns', None)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "do86ae",
  "name" : "data_WUS_UCLA_SR",
  "description" : "python",
  "code" : "import os\n\nprint(\"get UCLA data and prepare it into csv\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gkhtc0",
  "name" : "data_nsidc_4km_swe",
  "description" : null,
  "code" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nprint(\"station_cell_mapper_file = \", station_cell_mapper_file)\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\nif __name__ == \"__main__\":\n    # call this method to extract the \n    turn_nsidc_nc_to_csv()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "lbd6cp",
  "name" : "model_creation_et",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for SWE prediction.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for SWE prediction.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.utils import compute_sample_weight\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n#         return ExtraTreesRegressor(n_estimators=200, \n#                                    max_depth=None,\n#                                    random_state=42, \n#                                    min_samples_split=2,\n#                                    min_samples_leaf=1,\n#                                    n_jobs=5\n#                                   )\n        return ExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n    def create_sample_weights(self, X, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        #return np.where(X[\"fsca\"] < 100, scale_factor, 1)\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self, chosen_columns=None):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n        #training_data_path = f'{working_dir}/all_merged_training_cum_water_year_winter_month_only.csv' # snotel points\n#         training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n        training_data_path = f\"{working_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n        \n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(\"data.shape = \", data.shape)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        if chosen_columns == None:\n#           data = data.drop('Unnamed: 0', axis=1)\n          non_numeric_columns = data.select_dtypes(exclude=['number']).columns\n          # Drop non-numeric columns\n          data = data.drop(columns=non_numeric_columns)\n          print(\"all non-numeric columns are dropped: \", non_numeric_columns)\n          #data = data.drop('level_0', axis=1)\n          data = data.drop(['date'], axis=1)\n          data = data.drop(['lat'], axis=1)\n          data = data.drop(['lon'], axis=1)\n        else:\n          data = data[chosen_columns]\n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        print(\"describe the statistics of training input: \", X.describe())\n        print(\"describe the statistics of swe_value: \", y.describe())\n        \n        print(\"input features and order: \", X.columns)\n        print(\"training data row number: \", len(X))\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        # Initialize the StandardScaler\n        #scaler = StandardScaler()\n\n        # Fit the scaler on the training data and transform both training and testing data\n        #X_train_scaled = scaler.fit_transform(X_train)\n        #X_test_scaled = scaler.transform(X_test)\n        \n        self.weights = self.create_sample_weights(X_train, y_train, scale_factor=10, columns=X.columns)\n\n        self.train_x, self.train_y = X_train, y_train\n        self.test_x, self.test_y = X_test, y_test\n        #self.train_x, self.train_y = X_train_scaled, y_train\n        #self.test_x, self.test_y = X_test_scaled, y_test\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        # Calculate sample weights based on errors (you may need to customize this)\n#         self.classifier.fit(self.train_x, self.train_y)\n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         print(errors)\n        \n        #self.weights = 1+self.train_y # You can adjust this formula as needed\n#         weights = np.zeros_like(self.train_y, dtype=float)\n\n#         # Set weight to 1 if the target variable is 0\n#         weights[self.train_y == 0] = 10.0\n\n#         # Calculate weights for non-zero target values\n#         non_zero_indices = self.train_y != 0\n#         weights[non_zero_indices] = 0.1 / np.abs(self.train_y[non_zero_indices])\n\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n#         self.classifier.fit(self.train_x, self.train_y)\n        \n#         errors = abs(self.train_y - self.classifier.predict(self.train_x))\n#         self.weights = 1 / (1 + errors)  # You can adjust this formula as needed\n#         self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n        # Fit the classifier\n        self.classifier.fit(self.train_x, self.train_y)\n\n        # Make predictions\n        predictions = self.classifier.predict(self.train_x)\n\n        # Calculate absolute errors\n        errors = np.abs(self.train_y - predictions)\n\n        # Assign weights based on errors (higher errors get higher weights)\n        weights = compute_sample_weight('balanced', errors)\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=weights)\n\n    def post_processing(self, chosen_columns=None):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        if chosen_columns == None:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        else:\n          feature_png = f'{work_dir}/testing_output/et-model-feature-importance-{len(chosen_columns)}.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\n\n# all_used_columns = ['station_elevation', 'elevation', 'aspect', 'curvature', 'slope',\n# 'eastness', 'northness', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx',\n# 'vpd', 'vs', 'lc_code',  'fSCA',  'cumulative_etr',\n# 'cumulative_rmax', 'cumulative_rmin', 'cumulative_tmmn',\n# 'cumulative_tmmx', 'cumulative_vpd', 'cumulative_vs', 'cumulative_pr', 'swe_value']\n\n#all_used_columns = ['cumulative_pr','station_elevation', 'cumulative_tmmn', 'cumulative_tmmx', 'northness', 'cumulative_vs', 'cumulative_rmax', 'cumulative_etr','aspect','cumulative_rmin', 'elevation', 'cumulative_vpd',  'swe_value']\n# selected_columns = [\"lat\",\"lon\",\"elevation\",\"slope\",\"curvature\",\"aspect\",\"eastness\",\"northness\",\"cumulative_SWE\",\"cumulative_Flag\",\"cumulative_air_temperature_tmmn\",\"cumulative_potential_evapotranspiration\",\"cumulative_mean_vapor_pressure_deficit\",\"cumulative_relative_humidity_rmax\",\"cumulative_relative_humidity_rmin\",\"cumulative_precipitation_amount\",\"cumulative_air_temperature_tmmx\",\"cumulative_wind_speed\", \"swe_value\"]\n\n# all current variables without time series\n# selected_columns = ['SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'cumulative_SWE',\n# 'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n# 'cumulative_potential_evapotranspiration',\n# 'cumulative_mean_vapor_pressure_deficit',\n# 'cumulative_relative_humidity_rmax',\n# 'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n# 'cumulative_air_temperature_tmmx', 'cumulative_wind_speed', 'swe_value']\n\n\nselected_columns = [\n  'swe_value',\n  'SWE',\n  'cumulative_SWE',\n#   'cumulative_relative_humidity_rmin',\n#   'cumulative_air_temperature_tmmx', \n#   'cumulative_air_temperature_tmmn',\n#   'cumulative_relative_humidity_rmax',\n#   'cumulative_potential_evapotranspiration',\n#   'cumulative_wind_speed',\n  #'cumulative_fsca',\n  'fsca',\n  'air_temperature_tmmx', \n  'air_temperature_tmmn', \n  'potential_evapotranspiration', \n  'relative_humidity_rmax', \n  'Elevation',\t\n  'Slope',\t\n  'Curvature',\t\n  'Aspect',\t\n  'Eastness',\t\n  'Northness',\n]\n\n# ['cumulative_relative_humidity_rmin', 'cumulative_air_temperature_tmmx', 'cumulative_air_temperature_tmmn', 'cumulative_relative_humidity_rmax', 'cumulative_potential_evapotranspiration', 'cumulative_wind_speed'] \n\nif __name__ == \"__main__\":\n  hole = ETHole()\n  hole.preprocessing(chosen_columns = selected_columns)\n#   hole.preprocessing()\n  hole.train()\n  hole.test()\n  hole.evaluate()\n  hole.save()\n  hole.post_processing(chosen_columns = selected_columns)\n#   hole.post_processing()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "br9etb",
  "name" : "data_snotel_station_only",
  "description" : null,
  "code" : "import math\nimport json\nimport requests\nimport pandas as pd\nimport csv\nimport io\nimport os\nimport dask\nimport dask.dataframe as dd\nfrom snowcast_utils import work_dir, southwest_lat, southwest_lon, northeast_lat, northeast_lon, train_start_date, train_end_date\n\nworking_dir = work_dir\n\n\ndef download_station_json():\n    # https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?activeOnly=true&returnForecastPointMetadata=false&returnReservoirMetadata=false&returnStationElements=false\n    output_json_file = f'{working_dir}/all_snotel_cdec_stations.json'\n    if not os.path.exists(output_json_file):\n        # Fetch data from the URL\n        response = requests.get(\"https://wcc.sc.egov.usda.gov/awdbRestApi/services/v1/stations?activeOnly=true&returnForecastPointMetadata=false&returnReservoirMetadata=false&returnStationElements=false\")\n        \n\n        # Check if the request was successful (status code 200)\n        if response.status_code == 200:\n            # Decode the JSON content\n            json_content = response.json()\n\n            # Save the JSON content to a file\n            with open(output_json_file, 'w') as json_file:\n                json.dump(json_content, json_file, indent=2)\n\n            print(f\"Data downloaded and saved to {output_json_file}\")\n        else:\n            print(f\"Failed to download data. Status code: {response.status_code}\")\n    else:\n        print(f\"The file {output_json_file} already exists.\")\n        \n    \n    # read the json file and convert it to csv\n    csv_file_path = f'{working_dir}/all_snotel_cdec_stations.csv'\n    if not os.path.exists(csv_file_path):\n        # Read the JSON file\n        with open(output_json_file, 'r') as json_file:\n            json_content = json.load(json_file)\n\n        # Check the content (print or analyze as needed)\n        #print(\"JSON Content:\")\n        #print(json.dumps(json_content, indent=2))\n\n        # Convert JSON data to a list of dictionaries (assuming JSON is a list of objects)\n        data_list = json_content if isinstance(json_content, list) else [json_content]\n\n        # Get the header from the keys of the first dictionary (assuming consistent structure)\n        header = data_list[0].keys()\n        # Write to CSV file\n        with open(csv_file_path, 'w', newline='') as csv_file:\n            csv_writer = csv.DictWriter(csv_file, fieldnames=header)\n            csv_writer.writeheader()\n            csv_writer.writerows(data_list)\n\n        print(f\"Data converted and saved to {csv_file_path}\")\n    \n    else:\n        print(f\"The csv all snotel/cdec stations exists.\")\n        \n        \n    active_csv_file_path = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv'\n    if not os.path.exists(active_csv_file_path):\n        all_df = pd.read_csv(csv_file_path)\n        print(all_df.head())\n        all_df['endDate'] = pd.to_datetime(all_df['endDate'])\n        print(all_df.shape)\n        end_date = pd.to_datetime('2050-01-01')\n        filtered_df = all_df[all_df['endDate'] > end_date]\n        \n        # Filter rows within the latitude and longitude ranges\n        filtered_df = filtered_df[\n            (filtered_df['latitude'] >= southwest_lat) & (filtered_df['latitude'] <= northeast_lat) &\n            (filtered_df['longitude'] >= southwest_lon) & (filtered_df['longitude'] <= northeast_lon)\n        ]\n\n        # Print the original and filtered DataFrames\n        print(\"Filtered DataFrame:\")\n        print(filtered_df.shape)\n        filtered_df.to_csv(active_csv_file_path, index=False)\n    else:\n        print(f\"The active csv already exists: {active_csv_file_path}\")\n\t\n\ndef read_json_file(file_path):\n    with open(file_path, 'r', encoding='utf-8-sig') as json_file:\n        data = json.load(json_file)\n        return data\n\n\ndef haversine(lat1, lon1, lat2, lon2):\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n    d_lat = lat2 - lat1\n    d_long = lon2 - lon1\n    a = math.sin(d_lat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_long / 2) ** 2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    distance = 6371 * c  # Earth's radius in kilometers\n    return distance\n\n\ndef find_nearest_location(locations, target_lat, target_lon):\n    n_location = None\n    min_distance = float('inf')\n    for location in locations:\n        lat = location['location']['lat']\n        lon = location['location']['lng']\n        distance = haversine(lat, lon, target_lat, target_lon)\n        if distance < min_distance:\n            min_distance = distance\n            n_location = location\n            return n_location\n\n\ndef csv_to_json(csv_text):\n    lines = csv_text.splitlines()\n    header = lines[0]\n    field_names = header.split(',')\n    json_list = []\n    for line in lines[1:]:\n        values = line.split(',')\n        row_dict = {}\n        for i, field_name in enumerate(field_names):\n            row_dict[field_name] = values[i]\n            json_list.append(row_dict)\n            json_string = json.dumps(json_list)\n            return json_string\n\n\ndef remove_commented_lines(text):\n    lines = text.split(os.linesep)\n    cleaned_lines = []\n    for line in lines:\n        if not line.startswith('#'):\n            cleaned_lines.append(line)\n    cleaned_text = os.linesep.join(cleaned_lines)\n    return cleaned_text\n\n  \ndef get_swe_observations_from_snotel_cdec():\n    new_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n    new_base_df = pd.read_csv(new_base_station_list_file)\n    print(new_base_df.head())\n  \t\n    old_messed_file = f\"{work_dir}/\"\n    csv_file = f'{new_base_station_list_file}_swe_restored_dask_all_vars.csv'\n    start_date = train_start_date\n    end_date = train_end_date\n\t\n    # Create an empty Pandas DataFrame with the desired columns\n    result_df = pd.DataFrame(columns=[\n      'station_name', \n      'date', \n      'lat', \n      'lon', \n      'swe_value', \n      'change_in_swe_inch', \n      'snow_depth', \n      'change_in_swe_inch', \n      'air_temperature_observed_f'\n    ])\n\n    # Function to process each station\n    @dask.delayed\n    def process_station(station):\n        location_name = station['name']\n        location_triplet = station['stationTriplet']\n        location_elevation = station['elevation']\n        location_station_lat = station['latitude']\n        location_station_long = station['longitude']\n\n        url = f\"https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customSingleStationReport/daily/{location_triplet}%7Cid%3D%22%22%7Cname/{start_date},{end_date}%2C0/WTEQ%3A%3Avalue%2CWTEQ%3A%3Adelta%2CSNWD%3A%3Avalue%2CSNWD%3A%3Adelta%2CTOBS%3A%3Avalue\"\n\n        r = requests.get(url)\n        text = remove_commented_lines(r.text)\n        reader = csv.DictReader(io.StringIO(text))\n        json_data = json.loads(json.dumps(list(reader)))\n\n        entries = []\n        \n        for entry in json_data:\n            try:\n              # {'Date': '2021-06-18', 'Snow Water Equivalent (in) Start of Day Values': '', 'Change In Snow Water Equivalent (in)': '', 'Snow Depth (in) Start of Day Values': '', 'Change In Snow Depth (in)': '', 'Air Temperature Observed (degF) Start of Day Values': '70.5'}\n              required_data = {\n                'station_name': location_name,\n                'date': entry['Date'],\n                'lat': location_station_lat, \n                'lon': location_station_long,\n                'swe_value': entry['Snow Water Equivalent (in) Start of Day Values'],\n                'change_in_swe_inch': entry['Change In Snow Water Equivalent (in)'],\n                'snow_depth': entry['Snow Depth (in) Start of Day Values'],\n                'change_in_swe_inch': entry['Change In Snow Depth (in)'],\n                'air_temperature_observed_f': entry['Air Temperature Observed (degF) Start of Day Values']\n              }\n              entries.append(required_data)\n            except Exception as e:\n              print(\"entry = \", entry)\n              raise e\n        return pd.DataFrame(entries)\n\n    # List of delayed computations for each station\n    delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n\n    # Compute the delayed results\n    result_lists = dask.compute(*delayed_results)\n\n    # Concatenate the lists into a Pandas DataFrame\n    result_df = pd.concat(result_lists, ignore_index=True)\n\n    # Print the final Pandas DataFrame\n    print(result_df.head())\n\n    # Save the DataFrame to a CSV file\n    result_df.to_csv(csv_file, index=False)\n#     result_df.to_csv(csv_file, index=False)\n\nif __name__ == \"__main__\":\n    download_station_json()\n    get_swe_observations_from_snotel_cdec()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c2xkhz",
  "name" : "model_creation_rf",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the RandomForestHole class, which is used for training and evaluating a Random Forest Regressor model for hole analysis.\n\nAttributes:\n    RandomForestHole (class): A class for training and using a Random Forest Regressor model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Random Forest Regressor model with specified hyperparameters.\n    evaluate(): Evaluates the performance of the trained model and returns metrics such as MAE, MSE, R2, and RMSE.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = os.path.join(homedir, 'Documents', 'GitHub', 'SnowCast')\n\nclass RandomForestHole(BaseHole):\n  \n    def get_model(self):\n        \"\"\"\n        Returns the Random Forest Regressor model with specified hyperparameters.\n\n        Returns:\n            Pipeline: The Random Forest Regressor model wrapped in a scikit-learn pipeline.\n        \"\"\"\n        rfc_pipeline = Pipeline(steps=[\n            ('data_scaling', StandardScaler()),\n            ('model', RandomForestRegressor(max_depth=15,\n                                           min_samples_leaf=0.004,\n                                           min_samples_split=0.008,\n                                           n_estimators=25))\n        ])\n        return rfc_pipeline\n\n    def evaluate(self):\n        \"\"\"\n        Evaluates the performance of the trained model and returns metrics such as MAE, MSE, R2, and RMSE.\n\n        Returns:\n            dict: A dictionary containing MAE, MSE, R2, and RMSE metrics.\n        \"\"\"\n        mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n        mse = metrics.mean_squared_error(self.test_y, self.test_y_results)\n        r2 = metrics.r2_score(self.test_y, self.test_y_results)\n        rmse = math.sqrt(mse)\n\n        print(\"The random forest model performance for testing set\")\n        print(\"--------------------------------------\")\n        print('MAE is {}'.format(mae))\n        print('MSE is {}'.format(mse))\n        print('R2 score is {}'.format(r2))\n        print('RMSE is {}'.format(rmse))\n        return {\"mae\": mae, \"mse\": mse, \"r2\": r2, \"rmse\": rmse}\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "doinnd",
  "name" : "model_creation_pycaret",
  "description" : null,
  "code" : "import pandas as pd\nimport autokeras as ak\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data from a CSV file\ndf = pd.read_csv('/home/chetana/gridmet_test_run/five_years_data.csv')\n\n# Remove rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize a label encoder\nlabel_encoder = LabelEncoder()\n\n# Drop unnecessary columns from the DataFrame\ndf.drop(df.filter(regex=\"Unname\"), axis=1, inplace=True)\ndf.drop('Date', inplace=True, axis=1)\ndf.drop('mapping_cell_id', inplace=True, axis=1)\ndf.drop('cell_id', inplace=True, axis=1)\ndf.drop('station_id', inplace=True, axis=1)\ndf.drop('mapping_station_id', inplace=True, axis=1)\ndf.drop('station_triplet', inplace=True, axis=1)\ndf.drop('station_name', inplace=True, axis=1)\n\n# Rename columns for better readability\ndf.rename(columns={\n    'Change In Snow Water Equivalent (in)': 'swe_change',\n    'Snow Depth (in) Start of Day Values': 'swe_value',\n    'Change In Snow Depth (in)': 'snow_depth_change',\n    'Air Temperature Observed (degF) Start of Day Values': 'snotel_air_temp',\n    'Elevation [m]': 'elevation',\n    'Aspect [deg]': 'aspect',\n    'Curvature [ratio]': 'curvature',\n    'Slope [deg]': 'slope',\n    'Eastness [unitCirc.]': 'eastness',\n    'Northness [unitCirc.]': 'northness'\n}, inplace=True)\n\n# Split the data into features (X) and target variable (y)\nX = df.drop(columns=['swe_value'])\ny = df['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the AutoKeras regressor\nreg = ak.StructuredDataRegressor(max_trials=10, overwrite=True)\nreg.fit(X_train, y_train, epochs=10)\n\n# Evaluate the AutoKeras regressor on the test set\npredictions = reg.predict(X_test)\n\n# Calculate and print evaluation metrics\nrmse = mean_squared_error(y_test, predictions, squared=False)\nr2 = r2_score(y_test, predictions)\nprint('RMSE:', rmse)\nprint('R2 Score:', r2)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b7a4fu",
  "name" : "model_creation_autokeras",
  "description" : null,
  "code" : "import pandas as pd\nimport autokeras as ak\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom snowcast_utils import work_dir, month_to_season\nfrom datetime import datetime\nimport os\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\nimport pandas as pd\nimport numpy as np\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n\nworking_dir = work_dir\n\nhomedir = os.path.expanduser('~')\nnow = datetime.now()\ndate_time = now.strftime(\"%Y%d%m%H%M%S\")\n\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\ntraining_data_path = f\"{working_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n\nmodel_save_file = f\"{github_dir}/model/wormhole_autokeras_{date_time}.joblib\"\n\n# Read the data from the CSV file\nprint(f\"start to read data {training_data_path}\")\ndf = pd.read_csv(training_data_path)\n#df.dropna(inplace=True)\n\nprint(df.head())\nprint(df.columns())\n\n# Load and prepare your data\n# data = pd.read_csv('your_dataset.csv')  # Replace with your actual data source\nX = df.drop(columns=['swe_value', 'Date'])  # Replace with your actual target column\ny = df['swe_value']\n\n\n# Define the function to create a diverse Keras model\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            if i == 0:\n                model.add(Dense(num_units, activation='relu', input_shape=(input_shape,)))\n            else:\n                model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu',\n                         input_shape=(input_shape, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True),\n                       input_shape=(input_shape, 1)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    # Use a small random subset of the data\n    subset_idx = np.random.choice(len(X_train), size=int(0.1 * len(X_train)), replace=False)\n    X_subset = X_train[subset_idx]\n    y_subset = y_train[subset_idx]\n\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_subset, y_subset, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        loss, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    return mae\n\n# Encode categorical features\nfor col in X.select_dtypes(include=['object']).columns:\n    X[col] = LabelEncoder().fit_transform(X[col])\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Scale the data\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_val = scaler.transform(X_val)\n\n# If using CNN, LSTM, or Transformer, add a new dimension for channels\nX_train = np.expand_dims(X_train, axis=-1)\nX_val = np.expand_dims(X_val, axis=-1)\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Train the best model on the full dataset\nbest_model = create_model(study.best_trial)\nif isinstance(best_model, TabNetRegressor):\n    best_model.fit(\n        X_train, y_train,\n        eval_set=[(X_val, y_val)],\n        eval_metric=['mae'],\n        max_epochs=100,\n        patience=10,\n        batch_size=256,\n        virtual_batch_size=128,\n        verbose=0\n    )\n    best_model.save_model('best_tabnet_model')\n    print(\"Best TabNet model saved as best_tabnet_model.zip\")\nelse:\n    best_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, verbose=0)\n    best_model.save(model_save_file)\n    print(f\"Best model saved as {model_save_file}\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gnpbdq",
  "name" : "model_creation_autopytorch",
  "description" : null,
  "code" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport autopytorch as apt\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\n\ntraining_data_path = f\"{working_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv\"\n# Load the data from a CSV file\ndf = pd.read_csv(training_data_path)\n\n# Remove rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize a label encoder\nlabel_encoder = LabelEncoder()\n\n# Drop unnecessary columns from the DataFrame\ndf.drop(df.filter(regex=\"Unname\"), axis=1, inplace=True)\ndf.drop('Date', inplace=True, axis=1)\ndf.drop('mapping_cell_id', inplace=True, axis=1)\ndf.drop('cell_id', inplace=True, axis=1)\ndf.drop('station_id', inplace=True, axis=1)\ndf.drop('mapping_station_id', inplace=True, axis=1)\ndf.drop('station_triplet', inplace=True, axis=1)\ndf.drop('station_name', inplace=True, axis=1)\n\n# Rename columns for better readability\ndf.rename(columns={\n    'Change In Snow Water Equivalent (in)': 'swe_change',\n    'Snow Depth (in) Start of Day Values': 'swe_value',\n    'Change In Snow Depth (in)': 'snow_depth_change',\n    'Air Temperature Observed (degF) Start of Day Values': 'snotel_air_temp',\n    'Elevation [m]': 'elevation',\n    'Aspect [deg]': 'aspect',\n    'Curvature [ratio]': 'curvature',\n    'Slope [deg]': 'slope',\n    'Eastness [unitCirc.]': 'eastness',\n    'Northness [unitCirc.]': 'northness'\n}, inplace=True)\n\n# Split the dataset into features (X) and target variable (y)\nX = df.drop('swe_value', axis=1)\ny = df['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the Auto-PyTorch configuration\nconfig = apt.AutoNetRegressionConfig()\n\n# Initialize and train the Auto-PyTorch regressor\nreg = apt.AutoNetRegressor(config=config)\nreg.fit(X_train, y_train)\n\n# Evaluate the model\npredictions = reg.predict(X_test)\nrmse = mean_squared_error(y_test, predictions, squared=False)\nr2 = r2_score(y_test, predictions)\n\n# Print the evaluation metrics\nprint(\"RMSE:\", rmse)\nprint(\"R2 Score:\", r2)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "oon4sb",
  "name" : "western_us_dem.py",
  "description" : null,
  "code" : "import numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport warnings\nimport rasterio\nimport csv\nfrom rasterio.transform import Affine\nfrom scipy.ndimage import sobel, gaussian_filter\n\nmile_to_meters = 1609.34\nfeet_to_meters = 0.3048\n\n# Set the warning filter globally to ignore the FutureWarning\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ndef lat_lon_to_pixel(lat, lon, geotransform):\n    \"\"\"\n    Convert latitude and longitude to pixel coordinates using a geotransform.\n\n    Args:\n        lat (float): Latitude.\n        lon (float): Longitude.\n        geotransform (tuple): Geotransform coefficients (e.g., (origin_x, pixel_width, 0, origin_y, 0, pixel_height)).\n\n    Returns:\n        tuple: Pixel coordinates (x, y).\n    \"\"\"\n    x = int((lon - geotransform[0]) / geotransform[1])\n    y = int((lat - geotransform[3]) / geotransform[5])\n    return x, y\n\ndef calculate_slope_aspect_for_single(elevation_data, pixel_size_x, pixel_size_y):\n    \"\"\"\n    Calculate slope and aspect for a single pixel using elevation data.\n\n    Args:\n        elevation_data (array): Elevation data.\n        pixel_size_x (float): Pixel size in the x-direction.\n        pixel_size_y (float): Pixel size in the y-direction.\n\n    Returns:\n        tuple: Slope (in degrees), aspect (in degrees).\n    \"\"\"\n    # Calculate slope using the Sobel operator\n    slope_x = np.gradient(elevation_data, pixel_size_x, axis=1)\n    slope_y = np.gradient(elevation_data, pixel_size_y, axis=0)\n    slope_rad = np.arctan(np.sqrt(slope_x ** 2 + slope_y ** 2))\n    slope_deg = np.degrees(slope_rad)\n\n    # Calculate aspect (direction of the steepest descent)\n    aspect_rad = np.arctan2(slope_y, -slope_x)\n    aspect_deg = (np.degrees(aspect_rad) + 360) % 360\n\n    return slope_deg, aspect_deg\n\ndef save_as_geotiff(data, output_file, src_file):\n    \"\"\"\n    Save data as a GeoTIFF file with metadata from the source file.\n\n    Args:\n        data (array): Data to be saved.\n        output_file (str): Path to the output GeoTIFF file.\n        src_file (str): Path to the source GeoTIFF file to inherit metadata from.\n    \"\"\"\n    with rasterio.open(src_file) as src_dataset:\n        profile = src_dataset.profile\n        transform = src_dataset.transform\n\n        # Update the data type, count, and set the transform for the new dataset\n        profile.update(dtype=rasterio.float32, count=1, transform=transform)\n\n        # Create the new GeoTIFF file\n        with rasterio.open(output_file, 'w', **profile) as dst_dataset:\n            # Write the data to the new GeoTIFF\n            dst_dataset.write(data, 1)\n\ndef print_statistics(data):\n    \"\"\"\n    Print basic statistics of a data array.\n\n    Args:\n        data (array): Data array to calculate statistics for.\n    \"\"\"\n    # Calculate multiple statistics in one line\n    data = data[~np.isnan(data)]\n    mean, median, min_val, max_val, sum_val, std_dev, variance = [np.mean(data), np.median(data), np.min(data), np.max(data), np.sum(data), np.std(data), np.var(data)]\n\n    # Print the calculated statistics\n    print(\"Mean:\", mean)\n    print(\"Median:\", median)\n    print(\"Minimum:\", min_val)\n    print(\"Maximum:\", max_val)\n    print(\"Sum:\", sum_val)\n    print(\"Standard Deviation:\", std_dev)\n    print(\"Variance:\", variance)\n\ndef calculate_slope(dem_file):\n    from osgeo import gdal\n    import numpy as np\n    import rasterio\n    gdal.DEMProcessing(f'{dem_file}_slope.tif', dem_file, 'slope')\n    with rasterio.open(f'{dem_file}_slope.tif') as dataset:\n        slope=dataset.read(1)\n    return slope\n  \ndef calculate_aspect(dem_file):\n    from osgeo import gdal\n    import numpy as np\n    import rasterio\n    gdal.DEMProcessing(f'{dem_file}_aspect.tif', dem_file, 'aspect')\n    with rasterio.open(f'{dem_file}_aspect.tif') as dataset:\n        aspect=dataset.read(1)\n    return aspect\n    \n    \ndef calculate_slope_aspect(dem_file):\n    \"\"\"\n    Calculate slope and aspect from a DEM (Digital Elevation Model) file.\n\n    Args:\n        dem_file (str): Path to the DEM GeoTIFF file.\n\n    Returns:\n        tuple: Slope array (in degrees), aspect array (in degrees).\n    \"\"\"\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n        # Calculate the slope and aspect using numpy\n        dx, dy = np.gradient(dem_data)\n#         print(\"dx : \", dx)\n#         print(\"dy : \", dy)\n        slope = np.arctan(np.sqrt(dx**2 + dy**2))\n        slope = 90 - np.degrees(slope)\n        aspect = np.degrees(np.arctan2(-dy, dx))\n\n        # Adjust aspect values to range from 0 to 360 degrees\n        aspect[aspect < 0] += 360\n\n    return slope, aspect\n\ndef calculate_curvature(elevation_data, pixel_size_x, pixel_size_y):\n    \"\"\"\n    Calculate curvature from elevation data using the Laplacian operator.\n\n    Args:\n        elevation_data (array): Elevation data.\n        pixel_size_x (float): Pixel size in the x-direction.\n        pixel_size_y (float): Pixel size in the y-direction.\n\n    Returns:\n        array: Curvature data.\n    \"\"\"\n    # Calculate curvature using the Laplacian operator\n    curvature_x = np.gradient(np.gradient(elevation_data, pixel_size_x, axis=1), pixel_size_x, axis=1)\n    curvature_y = np.gradient(np.gradient(elevation_data, pixel_size_y, axis=0), pixel_size_y, axis=0)\n    curvature = curvature_x + curvature_y\n\n    return curvature\n  \ndef calculate_curvature(dem_file, sigma=1):\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n        \n        # the dem is in meter unit\n        dem_data = dem_data\n\n        # Calculate the gradient using the Sobel filter\n        dx = sobel(dem_data, axis=1, mode='constant')\n        dy = sobel(dem_data, axis=0, mode='constant')\n\n        # Calculate the second derivatives using the Sobel filter\n        dxx = sobel(dx, axis=1, mode='constant')\n        dyy = sobel(dy, axis=0, mode='constant')\n\n        # Calculate the curvature using the second derivatives\n        curvature = dxx + dyy\n\n        # Smooth the curvature using Gaussian filtering (optional)\n        curvature = gaussian_filter(curvature, sigma)\n\n    return curvature\n\ndef calculate_gradients(dem_file):\n    \"\"\"\n    Calculate Northness and Eastness gradients from a DEM (Digital Elevation Model) file.\n\n    Args:\n        dem_file (str): Path to the DEM GeoTIFF file.\n\n    Returns:\n        tuple: Northness array, Eastness array (both in radians).\n    \"\"\"\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradients along the North and East directions\n        dy, dx = np.gradient(dem_data, dataset.res[0], dataset.res[1])\n\n        # Calculate the Northness and Eastness\n        northness = np.arctan(dy / np.sqrt(dx**2 + dy**2))\n        eastness = np.arctan(dx / np.sqrt(dx**2 + dy**2))\n\n    return northness, eastness\n\ndef geotiff_to_csv(geotiff_file, csv_file, column_name):\n    \"\"\"\n    Convert a GeoTIFF file to a CSV file containing latitude, longitude, and image values.\n\n    Args:\n        geotiff_file (str): Path to the input GeoTIFF file.\n        csv_file (str): Path to the output CSV file.\n        column_name (str): Name for the image value column.\n    \"\"\"\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_file) as dataset:\n        # Get the pixel values as a 2D array\n        data = dataset.read(1)\n\n        if column_name == \"Elevation\":\n            # default unit is meter\n            data = data\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Get the width and height of the GeoTIFF\n        height, width = data.shape\n\n        # Open the CSV file for writing\n        with open(csv_file, 'w', newline='') as csvfile:\n            csvwriter = csv.writer(csvfile)\n\n            # Write the CSV header\n            csvwriter.writerow(['Latitude', 'Longitude', 'x', 'y', column_name])\n\n            # Loop through each pixel and extract latitude, longitude, and image value\n            for y in range(height):\n                for x in range(width):\n                    # Get the pixel value\n                    image_value = data[y, x]\n\n                    # Convert pixel coordinates to geographic coordinates\n                    lon, lat = transform * (x, y)\n\n                    # Write the data to the CSV file\n                    csvwriter.writerow([lat, lon, x, y, image_value])\n\ndef read_elevation_data(file_path, result_dem_csv_path, result_dem_feature_csv_path):\n    \"\"\"\n    Read and process elevation data from a CSV file and save it to another CSV file with additional features.\n\n    Args:\n        file_path (str): Path to the input CSV file containing elevation data.\n        result_dem_csv_path (str): Path to the output CSV file for elevation data.\n        result_dem_feature_csv_path (str): Path to the output CSV file for elevation data with additional features.\n\n    Returns:\n        DataFrame: Merged dataframe with elevation and additional features.\n    \"\"\"\n    neighborhood_size = 4\n    df = pd.read_csv(file_path)\n    \n    dataset = rasterio.open(geotiff_file)\n    data = dataset.read(1)\n\n    # Get the width and height of the GeoTIFF\n    height, width = data.shape\n    \n    # Create an empty DataFrame with column names\n    columns = ['lat', 'lon', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    all_df = pd.DataFrame(columns=columns)\n    \n    all_df.to_csv(result_dem_feature_csv_path)\n    print(f\"DEM and other columns are saved to file {result_dem_feature_csv_path}\")\n    return all_df\n\n\nif __name__ == \"__main__\":\n    # Usage example:\n    result_dem_csv_path = \"/home/chetana/gridmet_test_run/dem_template.csv\"\n    result_dem_feature_csv_path = \"/home/chetana/gridmet_test_run/dem_all.csv\"\n\n    dem_file = \"/home/chetana/gridmet_test_run/dem_file.tif\"\n    slope_file = '/home/chetana/gridmet_test_run/dem_file.tif_slope.tif'\n    aspect_file = '/home/chetana/gridmet_test_run/dem_file.tif_aspect.tif'\n    curvature_file = '/home/chetana/gridmet_test_run/curvature_file.tif'\n    northness_file = '/home/chetana/gridmet_test_run/northness_file.tif'\n    eastness_file = '/home/chetana/gridmet_test_run/eastness_file.tif'\n\n    slope, aspect = calculate_slope_aspect(dem_file)\n    # slope = calculate_slope(dem_file)\n    # aspect = calculate_aspect(dem_file)\n    curvature = calculate_curvature(dem_file)\n    northness, eastness = calculate_gradients(dem_file)\n\n    # Save the slope and aspect as new GeoTIFF files\n    save_as_geotiff(slope, slope_file, dem_file)\n    save_as_geotiff(aspect, aspect_file, dem_file)\n    save_as_geotiff(curvature, curvature_file, dem_file)\n    save_as_geotiff(northness, northness_file, dem_file)\n    save_as_geotiff(eastness, eastness_file, dem_file)\n\n    geotiff_to_csv(dem_file, dem_file+\".csv\", \"Elevation\")\n    geotiff_to_csv(slope_file, slope_file+\".csv\", \"Slope\")\n    geotiff_to_csv(aspect_file, aspect_file+\".csv\", \"Aspect\")\n    geotiff_to_csv(curvature_file, curvature_file+\".csv\", \"Curvature\")\n    geotiff_to_csv(northness_file, northness_file+\".csv\", \"Northness\")\n    geotiff_to_csv(eastness_file, eastness_file+\".csv\", \"Eastness\")\n\n    # List of file paths for the CSV files\n    csv_files = [dem_file+\".csv\", slope_file+\".csv\", aspect_file+\".csv\", \n                 curvature_file+\".csv\", northness_file+\".csv\", eastness_file+\".csv\"]\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        df = pd.read_csv(file, encoding='utf-8')\n        dfs.append(df)\n\n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude', 'x', 'y'])\n\n    # check the statistics of the columns\n    for column in merged_df.columns:\n        merged_df[column] = pd.to_numeric(merged_df[column], errors='coerce')\n        print(merged_df[column].describe())\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(result_dem_feature_csv_path, index=False)\n    print(f\"New dem features are updated in {result_dem_feature_csv_path}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "fa7e4u",
  "name" : "download_srtm_1arcsec (caution!)",
  "description" : null,
  "code" : "#!/bin/bash\n\nGREP_OPTIONS=''\n\ncookiejar=$(mktemp cookies.XXXXXXXXXX)\nnetrc=$(mktemp netrc.XXXXXXXXXX)\nchmod 0600 \"$cookiejar\" \"$netrc\"\nfunction finish {\n  rm -rf \"$cookiejar\" \"$netrc\"\n}\n\ntrap finish EXIT\nWGETRC=\"$wgetrc\"\n\nprompt_credentials() {\n    echo \"Enter your Earthdata Login or other provider supplied credentials\"\n    read -p \"Username (jensengmu): \" username\n    username=${username:-jensengmu}\n    read -s -p \"Password: \" password\n    echo \"machine urs.earthdata.nasa.gov login $username password $password\" >> $netrc\n    echo\n}\n\nexit_with_error() {\n    echo\n    echo \"Unable to Retrieve Data\"\n    echo\n    echo $1\n    echo\n    echo \"https://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip\"\n    echo\n    exit 1\n}\n\nprompt_credentials\n  detect_app_approval() {\n    approved=`curl -s -b \"$cookiejar\" -c \"$cookiejar\" -L --max-redirs 5 --netrc-file \"$netrc\" https://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip -w '\\n%{http_code}' | tail  -1`\n    if [ \"$approved\" -ne \"200\" ] && [ \"$approved\" -ne \"301\" ] && [ \"$approved\" -ne \"302\" ]; then\n        # User didn't approve the app. Direct users to approve the app in URS\n        exit_with_error \"Please ensure that you have authorized the remote application by visiting the link below \"\n    fi\n}\n\nsetup_auth_curl() {\n    # Firstly, check if it require URS authentication\n    status=$(curl -s -z \"$(date)\" -w '\\n%{http_code}' https://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip | tail -1)\n    if [[ \"$status\" -ne \"200\" && \"$status\" -ne \"304\" ]]; then\n        # URS authentication is required. Now further check if the application/remote service is approved.\n        detect_app_approval\n    fi\n}\n\nsetup_auth_wget() {\n    # The safest way to auth via curl is netrc. Note: there's no checking or feedback\n    # if login is unsuccessful\n    touch ~/.netrc\n    chmod 0600 ~/.netrc\n    credentials=$(grep 'machine urs.earthdata.nasa.gov' ~/.netrc)\n    if [ -z \"$credentials\" ]; then\n        cat \"$netrc\" >> ~/.netrc\n    fi\n}\n\nfetch_urls() {\n  if command -v curl >/dev/null 2>&1; then\n      setup_auth_curl\n      while read -r line; do\n        # Get everything after the last '/'\n        filename=\"${line##*/}\"\n\n        # Strip everything after '?'\n        stripped_query_params=\"${filename%%\\?*}\"\n\n        curl -f -b \"$cookiejar\" -c \"$cookiejar\" -L --netrc-file \"$netrc\" -g -o $stripped_query_params -- $line && echo || exit_with_error \"Command failed with error. Please retrieve the data manually.\"\n      done;\n  elif command -v wget >/dev/null 2>&1; then\n      # We can't use wget to poke provider server to get info whether or not URS was integrated without download at least one of the files.\n      echo\n      echo \"WARNING: Can't find curl, use wget instead.\"\n      echo \"WARNING: Script may not correctly identify Earthdata Login integrations.\"\n      echo\n      setup_auth_wget\n      while read -r line; do\n        # Get everything after the last '/'\n        filename=\"${line##*/}\"\n\n        # Strip everything after '?'\n        stripped_query_params=\"${filename%%\\?*}\"\n\n        wget --load-cookies \"$cookiejar\" --save-cookies \"$cookiejar\" --output-document $stripped_query_params --keep-session-cookies -- $line && echo || exit_with_error \"Command failed with error. Please retrieve the data manually.\"\n      done;\n  else\n      exit_with_error \"Error: Could not find a command-line downloader.  Please install curl or wget\"\n  fi\n}\n\nfetch_urls <<'EDSCEOF'\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N35E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E050.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E057.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S33E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E059.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E071.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S24E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S35E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S19E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32W018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E071.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S08E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E042.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N40W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S24E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N04E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S24E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S19E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N04E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N35W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E063.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N35E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N04E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S08E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S08E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N40W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N40E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N00E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39E015.SRTMGL1.hgt.zip\nEDSCEOF\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "drwmbo",
  "name" : "gridmet_testing",
  "description" : null,
  "code" : "\"\"\"\nScript for downloading specific variables of GridMET climatology data.\n\nThis script downloads specific meteorological variables from the GridMET climatology dataset\nfor a specified year. It uses the netCDF4 library for handling NetCDF files, urllib for downloading files,\nand pandas for data manipulation. The script also removes existing files in the target folder before downloading.\n\n\nUsage:\n    Run this script to download specific meteorological variables for a specified year from the GridMET dataset.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, work_dir\nimport matplotlib.pyplot as plt\n\n# Define the folder to store downloaded files\ngridmet_folder_name = f'{work_dir}/gridmet_climatology'\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\n# Define your value ranges for color mapping\n#value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n#value_ranges = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.5, 3]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n  if value_ranges == None:\n    max_value = df_col.max()\n    min_value = df_col.min()\n    if min_value < 0:\n      min_value = 0\n    step_size = (max_value - min_value) / 12\n\n    # Create 10 periods\n    new_value_ranges = [min_value + i * step_size for i in range(12)]\n  # Define your custom function to map data values to colors\n  def map_value_to_color(value):\n    # Iterate through the value ranges to find the appropriate color index\n    for i, range_max in enumerate(new_value_ranges):\n      if value <= range_max:\n        return colors[i]\n\n      # If the value is greater than the largest range, return the last color\n      return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n  color_mapping = [map_value_to_color(value) for value in df_col.values]\n  return color_mapping, new_value_ranges\n\ndef get_current_year():\n    \"\"\"\n    Get the current year.\n\n    Returns:\n        int: The current year.\n    \"\"\"\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\ndef remove_files_in_folder(folder_path, current_year):\n    \"\"\"\n    Remove all files in a specified folder.\n\n    Parameters:\n        folder_path (str): Path to the folder to remove files from.\n    \"\"\"\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path) and str(current_year) in file_path and file_path.endswith(\".nc\"):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    \"\"\"\n    Download a file from a URL and save it to a specified location.\n\n    Parameters:\n        url (str): URL of the file to download.\n        target_file_path (str): Path where the downloaded file should be saved.\n        variable (str): Name of the meteorological variable being downloaded.\n    \"\"\"\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\ndef download_gridmet_of_specific_variables(year_list):\n    \"\"\"\n    Download specific meteorological variables from the GridMET climatology dataset.\n    \"\"\"\n    # Make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(gridmet_folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef create_gridmet_to_dem_mapper(nc_file):\n    western_us_dem_df = pd.read_csv(western_us_coords)\n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n    \n      # Use the apply function to apply the custom function to each row\n      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n      \n    # print(western_us_dem_df.head())\n    \n    # Save the new converted AMSR to CSV file\n    western_us_dem_df.to_csv(target_csv_path, index=False)\n    \n    return western_us_dem_df\n  \n  \ndef get_nc_csv_by_coords_and_variable(nc_file,\n                                      var_name,\n                                      target_date=test_start_date):\n    \n    create_gridmet_to_dem_mapper(nc_file)\n  \t\n    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      var_col = nc_file.variables[long_var_name][:]\n      #print(\"val_col.shape: \", var_col.shape)\n      \n      # Calculate the day of the year\n      day_of_year = selected_date.timetuple().tm_yday\n      day_index = day_of_year - 1\n      #print('day_index:', day_index)\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        lat_index = int(row[\"gridmet_lat_idx\"])\n        lon_index = int(row[\"gridmet_lon_idx\"])\n        var_value = var_col[day_index, lat_index, lon_index]\n        \n        return var_value\n    \n      # Use the apply function to apply the custom function to each row\n      # print(mapper_df.columns)\n      # print(mapper_df.head())\n      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n      \n      # print(\"mapper_df[var_name]: \", mapper_df[var_name].describe())\n      \n      # drop useless columns\n      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n                               \"dem_lon\": \"Longitude\"}, inplace=True)\n\n      \n    # print(mapper_df.head())\n    return mapper_df\n\n\ndef turn_gridmet_nc_to_csv(target_date=test_start_date):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    generated_csvs = []\n    for root, dirs, files in os.walk(gridmet_folder_name):\n        for file_name in files:\n            \n            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n                print(f\"Checking file: {file_name}\")\n                var_name = get_var_from_file_name(file_name)\n                # print(\"Variable name:\", var_name)\n                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n\n                if os.path.exists(res_csv):\n                    #os.remove(res_csv)\n                    print(f\"{res_csv} already exists. Skipping..\")\n                    generated_csvs.append(res_csv)\n                    continue\n\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n                                                       var_name, target_date)\n                df.replace('--', pd.NA, inplace=True)\n                df.to_csv(res_csv, index=False)\n                print(\"gridmet var saved: \", res_csv)\n                generated_csvs.append(res_csv)\n    return generated_csvs   \n\ndef plot_gridmet(target_date=test_start_date):\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  var_name = \"pr\"\n  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\"\n  gridmet_var_df = pd.read_csv(test_csv)\n  gridmet_var_df.replace('--', pd.NA, inplace=True)\n  gridmet_var_df.dropna(inplace=True)\n  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n  #print(gridmet_var_df.head())\n  #print(gridmet_var_df[\"Latitude\"].describe())\n  #print(gridmet_var_df[\"Longitude\"].describe())\n  #print(gridmet_var_df[\"pr\"].describe())\n  \n  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n  \n  # Create a scatter plot\n  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n              gridmet_var_df[\"Latitude\"].values, \n              label='Pressure', \n              color=colormaplist, \n              marker='o')\n\n  # Add labels and a legend\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Scatter Plot Example')\n  plt.legend()\n  \n  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n  plt.savefig(res_png_path)\n  print(f\"test image is saved at {res_png_path}\")\n                \ndef prepare_folder_and_get_year_list(target_date=test_start_date):\n  # Check if the folder exists, if not, create it\n  if not os.path.exists(gridmet_folder_name):\n      os.makedirs(gridmet_folder_name)\n\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n  year_list = [selected_date.year, past_october_1.year]\n\n  # Remove any existing files in the folder\n  if selected_date.year == datetime.now().year:\n    # check if the current year's netcdf contains the selected date\n    # get etr netcdf and read\n    nc_file = f\"{gridmet_folder_name}/tmmx_{selected_date.year}.nc\"\n    ifremove = False\n    if os.path.exists(nc_file):\n      with nc.Dataset(nc_file) as ncd:\n        day = ncd.variables['day'][:]\n        # Calculate the day of the year\n        day_of_year = selected_date.timetuple().tm_yday\n        day_index = day_of_year - 1\n        if len(day) <= day_index:\n          ifremove = True\n    \n    if ifremove:\n      print(\"The current year netcdf has new data. Redownloading..\")\n      remove_files_in_folder(gridmet_folder_name, selected_date.year)  # only redownload when the year is the current year\n    else:\n      print(\"The existing netcdf already covers the selected date. Avoid downloading..\")\n  return year_list\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].sum()\n  return df\n    \n\ndef prepare_cumulative_history_csvs(target_date=test_start_date, force=False):\n  \"\"\"\n    Prepare cumulative history CSVs for a specified target date.\n\n    Parameters:\n    - target_date (str, optional): The target date in the format 'YYYY-MM-DD'. Default is 'test_start_date'.\n    - force (bool, optional): If True, forcefully regenerate cumulative CSVs even if they already exist. Default is False.\n\n    Returns:\n    None\n\n    This function generates cumulative history CSVs for a specified target date. It traverses the date range from the past\n    October 1 to the target date, downloads gridmet data, converts it to CSV, and merges it into a big DataFrame.\n    The cumulative values are calculated and saved in new CSV files.\n\n    Example:\n    ```python\n    prepare_cumulative_history_csvs(target_date='2023-01-01', force=True)\n    ```\n\n    Note: This function assumes the existence of the following helper functions:\n    - download_gridmet_of_specific_variables\n    - prepare_folder_and_get_year_list\n    - turn_gridmet_nc_to_csv\n    - add_cumulative_column\n    - process_group_value_filling\n    ```\n\n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n        past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n        past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Rest of the function logic...\n\n    filled_data = filled_data.loc[:, ['Latitude', 'Longitude', var_name, f'cumulative_{var_name}']]\n    print(\"new_df final shape: \", filled_data.head())\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    print(filled_data.describe())\n    ```\nNote: This docstring includes placeholders such as \"download_gridmet_of_specific_variables\" and \"prepare_folder_and_get_year_list\" for the assumed existence of related helper functions. You should replace these placeholders with actual documentation for those functions.\n  \"\"\"\n  selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  print(selected_date)\n  if selected_date.month < 10:\n    past_october_1 = datetime(selected_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(selected_date.year, 10, 1)\n\n  # Traverse and print every day from past October 1 to the specific date\n  current_date = past_october_1\n  \n  date_keyed_objects = {}\n  \n  download_gridmet_of_specific_variables(\n    prepare_folder_and_get_year_list(target_date=target_date)\n  )\n  \n  while current_date <= selected_date:\n    print(current_date.strftime('%Y-%m-%d'))\n    current_date_str = current_date.strftime('%Y-%m-%d')\n    \n    \n    generated_csvs = turn_gridmet_nc_to_csv(target_date=current_date_str)\n    \n    # read the csv into dataframe and merge to the big dataframe\n    date_keyed_objects[current_date_str] = generated_csvs\n    \n    current_date += timedelta(days=1)\n    \n  print(\"date_keyed_objects: \", date_keyed_objects)\n  target_generated_csvs = date_keyed_objects[target_date]\n  for index, single_csv in enumerate(target_generated_csvs):\n    # traverse the variables of gridmet here\n    # each variable is a loop\n    print(f\"creating cumulative for {single_csv}\")\n    \n    cumulative_target_path = f\"{single_csv}_cumulative.csv\"\n    print(\"cumulative_target_path = \", cumulative_target_path)\n    \n    if os.path.exists(cumulative_target_path) and not force:\n      print(f\"{cumulative_target_path} already exists, skipping..\")\n      continue\n    \n    # Extract the file name without extension\n    file_name = os.path.splitext(os.path.basename(single_csv))[0]\n    gap_filled_csv = f\"{cumulative_target_path}_gap_filled.csv\"\n\n\t# Split the file name using underscores\n    var_name = file_name.split('_')[1]\n    print(f\"Found variable name {var_name}\")\n    current_date = past_october_1\n    new_df = pd.read_csv(single_csv)\n    print(new_df.head())\n    \n    all_df = pd.read_csv(f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.csv\")\n    all_df[\"date\"] = target_date\n    all_df[var_name] = pd.to_numeric(all_df[var_name], errors='coerce')\n    \n#     while current_date <= selected_date:\n#       print(current_date.strftime('%Y-%m-%d'))\n#       current_date_str = current_date.strftime('%Y-%m-%d')\n#       #current_generated_csv = date_keyed_objects[current_date_str][index]\n#       current_generated_csv = f\"{work_dir}/testing_output/{str(current_date.year)}_{var_name}_{current_date_str}.csv\"\n#       print(f\"reading file: {current_generated_csv}\")\n#       previous_df = pd.read_csv(current_generated_csv)\n#       previous_df[\"date\"] = current_date_str\n#       previous_df[var_name] = pd.to_numeric(previous_df[var_name], errors='coerce')\n      \n#       if all_df is None:\n#         all_df = previous_df\n#       else:\n#         all_df = pd.concat([all_df, previous_df], ignore_index=True)\n      \n#       current_date += timedelta(days=1)\n    \n    \n#     print(\"all df head: \", all_df.shape, all_df[var_name].describe())\n    \n    # add all the columns together and save to new csv\n    # Adding all columns except latitude and longitude\n    \n#     def process_group_value_filling(group, var_name, target_date):\n#       # Sort the group by 'date'\n# #       group = group.sort_values(by='date')\n#       # no need to interpolate for gridmet, only add the cumulative columns\n#       cumvalue = group[var_name].sum()\n#       group = group[(group['date'] == target_date)]\n#       group[f'cumulative_{var_name}'] = cumvalue\n#       return group\n\n#     grouped = all_df.groupby(['Latitude', 'Longitude'])\n#     # Apply the function to each group\n#     filled_data = grouped.apply(lambda group: process_group_value_filling(group, var_name, target_date)).reset_index(drop=True)\n    filled_data = all_df\n    filled_data = filled_data[(filled_data['date'] == target_date)]\n    \n    filled_data.fillna(0, inplace=True)\n    \n#     print(\"filled_data.shape = \", filled_data.shape)\n#     print(\"filled_data.head = \", filled_data.head())\n#     print(f\"filled_data[{var_name}].unique() = {filled_data[var_name].describe()}\")\n    # only retain the rows of the target date\n    \n\t\n    print(\"Finished correctly \", filled_data.head())\n    #filled_data.to_csv(gap_filled_csv, index=False)\n    #print(f\"New filled values csv is saved to {gap_filled_csv}_gap_filled.csv\")\n    \n    filled_data = filled_data[['Latitude', 'Longitude', \n                               var_name, \n#                                f'cumulative_{var_name}'\n                              ]]\n    print(filled_data.shape)\n    filled_data.to_csv(cumulative_target_path, index=False)\n    print(f\"new df is saved to {cumulative_target_path}\")\n    print(filled_data.describe())\n\n\nif __name__ == \"__main__\":\n  # Run the download function\n#   download_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\n#   turn_gridmet_nc_to_csv()\n#   plot_gridmet()\n\n  # prepare testing data with cumulative variables\n  prepare_cumulative_history_csvs(force=True)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2n7b06",
  "name" : "create_output_tif_template",
  "description" : null,
  "code" : "import os\nimport rasterio\nfrom rasterio.transform import from_origin\nimport numpy as np\n\ndef create_western_us_geotiff():\n    \"\"\"\n    Create a GeoTIFF template for the western U.S. region with specified spatial extent and resolution.\n    The resulting GeoTIFF file will contain an empty 2D array with a single band.\n    \"\"\"\n    # Define the spatial extent of the western U.S. (minx, miny, maxx, maxy)\n    minx, miny, maxx, maxy = -125, 25, -100, 49\n\n    # Define the resolution in degrees (4km is approximately 0.036 degrees)\n    resolution = 0.036\n\n    # Calculate the image size (width and height in pixels) based on the spatial extent and resolution\n    width = int((maxx - minx) / resolution)\n    height = int((maxy - miny) / resolution)\n\n    # Create an empty 2D NumPy array with a single band to store the image data\n    data = np.zeros((height, width), dtype=np.float32)\n    \n    # Read the user's home directory\n    homedir = os.path.expanduser('~')\n    print(homedir)\n\n    # Define the output filename\n    output_filename = f\"{homedir}/western_us_geotiff_template.tif\"\n\n    # Create the GeoTIFF file and specify the metadata\n    with rasterio.open(\n        output_filename,\n        'w',\n        driver='GTiff',\n        height=height,\n        width=width,\n        count=1,  # Single band\n        dtype=np.float32,\n        crs='EPSG:4326',  # WGS84\n        transform=from_origin(minx, maxy, resolution, resolution),\n    ) as dst:\n        # Write the data to the raster\n        dst.write(data, 1)\n\nif __name__ == \"__main__\":\n    create_western_us_geotiff()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "bwdy3s",
  "name" : "resample_dem",
  "description" : null,
  "code" : "#!/bin/bash\n# this script will reproject and resample the western US dem, clip it, to match the exact spatial extent and resolution as the template tif\n\ncd /home/chetana/gridmet_test_run\n\nmkdir template_shp/\n\ncp /home/chetana/western_us_geotiff_template.tif template_shp/\n\n# generate the template shape\ngdaltindex template.shp template_shp/*.tif\n\ngdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    \ngdalinfo output_4km_clipped.tif\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2wkl6e",
  "name" : "convert_results_to_images",
  "description" : null,
  "code" : "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import timedelta, datetime\nimport numpy as np\nimport uuid\nimport matplotlib.colors as mcolors\nimport geopandas as gpd\nimport rasterio\nfrom rasterio.transform import from_origin\nfrom rasterio.enums import Resampling\nfrom rasterio import warp\nfrom shapely.geometry import Point\nfrom rasterio.crs import CRS\nimport rasterio.features\nfrom rasterio.features import rasterize\nimport os\nimport math\n\n# Import utility functions and variables from 'snowcast_utils'\nfrom snowcast_utils import homedir, work_dir, test_start_date\n\n# Define a custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\ncmap_name = 'custom_snow_colormap'\ncustom_cmap = mcolors.ListedColormap(colors)\n\nlon_min, lon_max = -125, -100\nlat_min, lat_max = 25, 49.5\n\n# Define value ranges for color mapping\nfixed_value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n\n# Define the lat_lon_to_map_coordinates function\ndef lat_lon_to_map_coordinates(lon, lat, m):\n    \"\"\"\n    Convert latitude and longitude coordinates to map coordinates.\n\n    Args:\n        lon (float or array-like): Longitude coordinate(s).\n        lat (float or array-like): Latitude coordinate(s).\n        m (Basemap): Basemap object representing the map projection.\n\n    Returns:\n        tuple: Tuple containing the converted map coordinates (x, y).\n    \"\"\"\n    x, y = m(lon, lat)\n    return x, y\n\n\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n    \"\"\"\n    Create a colormap for value ranges and map data values to colors.\n\n    Args:\n        df_col (pd.Series): A Pandas Series containing data values.\n        value_ranges (list, optional): A list of value ranges for color mapping.\n            If not provided, the ranges will be determined automatically.\n\n    Returns:\n        tuple: Tuple containing the color mapping and the updated value ranges.\n    \"\"\"\n    new_value_ranges = value_ranges\n    if value_ranges is None:\n        max_value = df_col.max()\n        min_value = df_col.min()\n        if min_value < 0:\n            min_value = 0\n        step_size = (max_value - min_value) / 12\n\n        # Create 10 periods\n        new_value_ranges = [min_value + i * step_size for i in range(12)]\n    \n    #print(\"new_value_ranges: \", new_value_ranges)\n  \n    # Define a custom function to map data values to colors\n    def map_value_to_color(value):\n        # Iterate through the value ranges to find the appropriate color index\n        for i, range_max in enumerate(new_value_ranges):\n            if value <= range_max:\n                return colors[i]\n\n        # If the value is greater than the largest range, return the last color\n        return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n    color_mapping = [map_value_to_color(value) for value in df_col.values]\n    return color_mapping, new_value_ranges\n\ndef convert_csvs_to_images():\n    \"\"\"\n    Convert CSV data to images with color-coded SWE predictions.\n\n    Returns:\n        None\n    \"\"\"\n    global fixed_value_ranges\n    data = pd.read_csv(f\"{homedir}/gridmet_test_run/test_data_predicted_n97KJ.csv\")\n    print(\"statistic of predicted_swe: \", data['predicted_swe'].describe())\n    data['predicted_swe'].fillna(0, inplace=True)\n    \n    for column in data.columns:\n        column_data = data[column]\n        print(column_data.describe())\n    \n    # Create a figure with a white background\n    fig = plt.figure(facecolor='white')\n\n    \n\n    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n                projection='merc', resolution='i')\n\n    x, y = m(data['lon'].values, data['lat'].values)\n    print(data.columns)\n\n    color_mapping, value_ranges = create_color_maps_with_value_range(data[\"predicted_swe\"], fixed_value_ranges)\n    \n    # Plot the data using the custom colormap\n    plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\n    \n    # Draw coastlines and other map features\n    m.drawcoastlines()\n    m.drawcountries()\n    m.drawstates()\n\n    reference_date = datetime(1900, 1, 1)\n    day_value = day_index\n    \n    result_date = reference_date + timedelta(days=day_value)\n    today = result_date.strftime(\"%Y-%m-%d\")\n    timestamp_string = result_date.strftime(\"%Y-%m-%d\")\n    \n    # Add a title\n    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n\n    # Add labels for latitude and longitude on x and y axes with smaller font size\n    plt.xlabel('Longitude', fontsize=6)\n    plt.ylabel('Latitude', fontsize=6)\n\n    # Add longitude values to the x-axis and adjust font size\n    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n    x_tick_labels_str = [f\"{lon:.1f}°W\" if lon < 0 else f\"{lon:.1f}°E\" for lon in x_ticks_labels]\n    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n    plt.gca().set_xticklabels(x_tick_labels_str)\n\n    # Add latitude values to the y-axis and adjust font size\n    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n    y_tick_labels_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_ticks_labels]\n    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n    plt.gca().set_yticklabels(y_tick_labels_str)\n\n    # Convert map coordinates to latitude and longitude for y-axis labels\n    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n    y_tick_positions_lat_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_tick_positions_lat]\n    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n\n    # Create custom legend elements using the same colormap\n    legend_elements = [Patch(color=colors[i], label=f\"{value_ranges[i]} - {value_ranges[i+1]-1}\" if i < len(value_ranges) - 1 else f\"> {value_ranges[-1]}\") for i in range(len(value_ranges))]\n\n    # Create the legend outside the map\n    legend = plt.legend(handles=legend_elements, loc='upper left', title='Legend', fontsize=8)\n    legend.set_bbox_to_anchor((1.01, 1)) \n\n    # Remove the color bar\n    #plt.colorbar().remove()\n\n    plt.text(0.98, 0.02, 'Copyright © SWE Wormhole Team',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=plt.gcf().transFigure, fontsize=6, color='black')\n\n    # Set the aspect ratio to 'equal' to keep the plot at the center\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    # Adjust the bottom and top margins to create more white space between the title and the plot\n    plt.subplots_adjust(bottom=0.15, right=0.80)  # Adjust right margin to accommodate the legend\n    # Show the plot or save it to a file\n    new_plot_path = f'{homedir}/gridmet_test_run/predicted_swe-{test_start_date}.png'\n    print(f\"The new plot is saved to {new_plot_path}\")\n    plt.savefig(new_plot_path)\n    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n\ndef plot_all_variables_in_one_csv(csv_path, res_png_path, target_date = test_start_date):\n    result_var_df = pd.read_csv(csv_path)\n    # Convert the 'date' column to datetime\n    result_var_df['date'] = pd.to_datetime(result_var_df['date'])\n    result_var_df.rename(\n      columns={\n        'Latitude': 'lat', \n        'Longitude': 'lon',\n        'gridmet_lat': 'lat',\n        'gridmet_lon': 'lon',\n      }, \n      inplace=True)\n    \n  \t# Create subplots with a number of rows based on the number of columns in the DataFrame\n    us_boundary = gpd.read_file('/home/chetana/gridmet_test_run/tl_2023_us_state.shp')\n    us_boundary_clipped = us_boundary.cx[lon_min:lon_max, lat_min:lat_max]\n\t\n    lat_col = result_var_df[[\"lat\"]]\n    lon_col = result_var_df[[\"lon\"]]\n    print(\"lat_col.values = \", lat_col[\"lat\"].values)\n#     if \"lat\" == column_name or \"lon\" == column_name or \"date\" == column_name:\n    columns_to_remove = [ \"date\", \"Latitude\", \"Longitude\", \"gridmet_lat\", \"gridmet_lon\", \"lat\", \"lon\"]\n\n    # Check if each column exists before removing it\n    for col in columns_to_remove:\n        if col in result_var_df.columns:\n            result_var_df = result_var_df.drop(columns=col)\n        else:\n            print(f\"Column '{col}' not found in DataFrame.\")\n    \n    print(\"result_var_df.shape: \", result_var_df.shape)\n    print(\"result_var_df.head: \", result_var_df.head())\n    \n    \n    num_columns = len(result_var_df.columns)  # don't plot lat and lon\n    fig_width = 7 * num_columns  # You can adjust this multiplier based on your preference\n    num_variables = len(result_var_df.columns)\n    num_cols = int(math.sqrt(num_variables))\n    num_rows = math.ceil(num_variables / num_cols)\n    fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(num_cols*7, num_rows*6))\n    \n    \n    # Flatten the axes array to simplify indexing\n    axes = axes.flatten()\n    \n  \t# Plot each variable in a separate subplot\n    for i, column_name in enumerate(result_var_df.columns):\n  \t    print(f\"Plot {column_name}\")\n  \t    if column_name in [\"lat\", \"lon\"]:\n  \t        continue\n        \n        # Filter the DataFrame based on the target date\n  \t    result_var_df[column_name] = pd.to_numeric(result_var_df[column_name], errors='coerce')\n  \t    \n  \t    colormaplist, value_ranges = create_color_maps_with_value_range(result_var_df[column_name], fixed_value_ranges)\n  \t    scatter_plot = axes[i].scatter(\n            lon_col[\"lon\"].values, \n  \t        lat_col[\"lat\"].values, \n            label=column_name, \n            c=result_var_df[column_name], \n            cmap='viridis', \n              #s=200, \n            s=10, \n            marker='s',\n            edgecolor='none',\n        )\n        \n        # Add a colorbar\n  \t    cbar = plt.colorbar(scatter_plot, ax=axes[i])\n  \t    cbar.set_label(column_name)  # Label for the colorbar\n        \n        # Add boundary over the figure\n  \t    us_boundary_clipped.plot(ax=axes[i], color='none', edgecolor='black', linewidth=1)\n\n        # Add labels and a legend\n  \t    axes[i].set_xlabel('Longitude')\n  \t    axes[i].set_ylabel('Latitude')\n  \t    axes[i].set_title(column_name+\" - \"+target_date)  # You can include target_date if needed\n  \t    axes[i].legend(loc='lower left')\n    \n    # Remove any empty subplots\n    for i in range(num_variables, len(axes)):\n        fig.delaxes(axes[i])\n    \n    plt.tight_layout()\n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n    \n    \ndef plot_all_variables_in_one_figure_for_date(target_date=test_start_date):\n  \tselected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n  \ttest_csv = f\"{homedir}/gridmet_test_run/test_data_predicted_latest.csv\"\n  \tres_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_all_variables_{target_date}.png\"\n  \tplot_all_variables_in_one_csv(test_csv, res_png_path, target_date)\n    \ndef convert_csvs_to_images_simple(target_date=test_start_date, column_name = \"predicted_swe\"):\n    \"\"\"\n    Convert CSV data to simple scatter plot images for predicted SWE.\n\n    Returns:\n        None\n    \"\"\"\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    var_name = column_name\n    test_csv = f\"{homedir}/gridmet_test_run/test_data_predicted_latest.csv\"\n    result_var_df = pd.read_csv(test_csv)\n    # Convert the 'date' column to datetime\n    result_var_df['date'] = pd.to_datetime(result_var_df['date'])\n\n    # Filter the DataFrame based on the target date\n    result_var_df[var_name] = pd.to_numeric(result_var_df[var_name], errors='coerce')\n    \n    colormaplist, value_ranges = create_color_maps_with_value_range(result_var_df[var_name], fixed_value_ranges)\n\n    # Create a scatter plot\n    plt.scatter(result_var_df[\"lon\"].values, \n                result_var_df[\"lat\"].values, \n                label=column_name, \n                c=result_var_df[column_name], \n                cmap='viridis', \n                #s=200, \n                s=10, \n                marker='s',\n                edgecolor='none',\n               )\n\n    \n    \n    # Add a colorbar\n    cbar = plt.colorbar()\n    cbar.set_label(column_name)  # Label for the colorbar\n    \n    # Add labels and a legend\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title(f'{column_name} - {target_date}')\n    plt.legend(loc='lower left')\n    \n    us_boundary = gpd.read_file('/home/chetana/gridmet_test_run/tl_2023_us_state.shp')\n    us_boundary_clipped = us_boundary.cx[lon_min:lon_max, lat_min:lat_max]\n\n    us_boundary_clipped.plot(ax=plt.gca(), color='none', edgecolor='black', linewidth=1)\n\n    res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{target_date}.png\"\n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n\n\ndef convert_csv_to_geotiff(target_date = test_start_date):\n    # Load your CSV file\n    test_csv = f\"{homedir}/gridmet_test_run/test_data_predicted_latest_{target_date}.csv\"\n    \n    result_var_df = pd.read_csv(test_csv)\n    result_var_df.rename(\n      columns={\n        'Latitude': 'lat', \n        'Longitude': 'lon',\n        'gridmet_lat': 'lat',\n        'gridmet_lon': 'lon',\n      }, \n      inplace=True)\n\n    # Specify the output GeoTIFF file\n    target_geotiff_file = f\"{homedir}/gridmet_test_run/testing_output/swe_predicted_{target_date}.tif\"\n\n    df = result_var_df[[\"lat\", \"lon\", \"predicted_swe\"]]\n    \n    # Extract latitude, longitude, and snow columns\n    latitude = df['lat'].values\n    longitude = df['lon'].values\n    \n    swe = df['predicted_swe'].values\n\n    # Define the resolution and bounding box of the GeoTIFF\n#     resolution = 0.036  # adjust as needed\n#     min_longitude, max_latitude = min(longitude), max(latitude)\n#     max_longitude, min_latitude = max(longitude), min(latitude)\n\n#     # Calculate the width and height of the GeoTIFF\n#     width = int((max_longitude - min_longitude) / resolution)\n#     height = int((max_latitude - min_latitude) / resolution)\n#     print(\"width: \", width, \" - height: \", height)\n    \n#     print(f\"Environment variables: {os.environ}\")\n#     os.environ[\"PROJ_LIB\"] = f\"{homedir}/anaconda3/lib/python3.9/site-packages/rasterio/proj_data\"\n#     print(f\"Updated Environment variables: {os.environ}\")\n    \n    # Create a transformation for the GeoTIFF\n#     transform = from_origin(min_longitude, max_latitude, resolution, resolution)\n#     projection = CRS.from_string(\"EPSG:4326\")\n#     print(projection)\n    \n    \n    \n    dem_file = f\"{homedir}/gridmet_test_run/dem_file.tif\"\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n        # Print the shape of the raster data\n        print(\"Shape of the raster data:\", dem_data.shape)\n\n        # Print information about the raster dataset\n        print(\"Raster Dataset Information:\")\n        print(\"Driver:\", dataset.driver)\n        print(\"CRS (Coordinate Reference System):\", dataset.crs)\n        print(\"Transform (Affine Matrix):\", dataset.transform)\n        print(\"Number of Bands:\", dataset.count)\n        print(\"Data Type:\", dataset.dtypes[0])  # Assuming a single band, use [0]\n        print(\"Nodata Value:\", dataset.nodatavals[0])  # Assuming a single band, use [0]\n        # Read metadata from the original file\n        meta = dataset.meta\n\n        # Update metadata for the new data\n        meta.update(dtype='float32', count=1)\n        new_data = swe.reshape((666, 694))\n\n        # Create a new GeoTIFF file for writing\n        with rasterio.open(target_geotiff_file, 'w', **meta) as dst:\n            # Write the new 2D array to the new GeoTIFF file\n            dst.write(new_data, 1)\n        \n    print(\"df['predicted_swe'].shape: \", df.shape)\n    print(f\"GeoTIFF file '{target_geotiff_file}' created successfully.\")\n    \nif __name__ == \"__main__\":\n    # Uncomment the function call you want to use:\n    #convert_csvs_to_images()\n    \n    #test_start_date = \"2022-10-09\"\n\n    # plot the predicted SWE first\n    convert_csvs_to_images_simple(test_start_date)\n\n    # skip the long figure plot for now. this is too slow as someone is competiting resources with me.\n    # plot_all_variables_in_one_figure_for_date(test_start_date)\n    \n    convert_csv_to_geotiff(test_start_date)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "i2fynz",
  "name" : "deploy_images_to_website",
  "description" : null,
  "code" : "import distutils.dir_util\nfrom snowcast_utils import work_dir\nimport os\nimport shutil\nimport re\nimport pandas as pd\nfrom datetime import datetime\nimport time\n\n\nprint(\"move the plots and the results into the http folder\")\n\ndef copy_if_modified(source_file, destination_file):\n    if os.path.exists(destination_file):\n        source_modified_time = os.path.getmtime(source_file)\n        dest_modified_time = os.path.getmtime(destination_file)\n        \n        # If the source file is modified after the destination file\n        if source_modified_time > dest_modified_time:\n            shutil.copy(source_file, destination_file)\n            print(f'Copied: {source_file}')\n    else:\n        shutil.copy(source_file, destination_file)\n        print(f'Copied: {source_file}')\n\ndef create_mapserver_map_config(target_geotiff_file_path, force=False):\n  geotiff_file_name = os.path.basename(target_geotiff_file_path)\n  geotiff_mapserver_file_path = f\"/var/www/html/swe_forecasting/map/{geotiff_file_name}.map\"\n  \n  if os.path.exists(geotiff_mapserver_file_path) and not force:\n    print(f\"{geotiff_mapserver_file_path} already exists\")\n    return geotiff_mapserver_file_path\n  \n  # Define a regular expression pattern to match the date in the filename\n  pattern = r\"\\d{4}-\\d{2}-\\d{2}\"\n\n  # Use re.search to find the match\n  match = re.search(pattern, geotiff_file_name)\n\n  # Check if a match is found\n  if match:\n      date_string = match.group()\n      print(\"Date:\", date_string)\n  else:\n      print(\"No date found in the filename.\")\n      return f\"The file's name {target_geotiff_file} is wrong\"\n  \n  mapserver_config_content = f\"\"\"\nMAP\n  NAME \"swemap\"\n  STATUS ON\n  EXTENT -125 25 -100 49\n  SIZE 800 400\n  UNITS DD\n  SHAPEPATH \"/var/www/html/swe_forecasting/output/\"\n\n  PROJECTION\n    \"init=epsg:4326\"\n  END\n\n  WEB\n    IMAGEPATH \"/temp/\"\n    IMAGEURL \"/temp/\"\n    METADATA\n      \"wms_title\" \"SWE MapServer WMS\"\n      \"wms_onlineresource\" \"http://geobrain.csiss.gmu.edu/cgi-bin/mapserv?map=/var/www/html/swe_forecasting/output/swe.map&\"\n      WMS_ENABLE_REQUEST      \"*\"\n      WCS_ENABLE_REQUEST      \"*\"\n      \"wms_srs\" \"epsg:5070 epsg:4326 epsg:3857\"\n    END\n  END\n\n\n  LAYER\n    NAME \"predicted_swe_{date_string}\"\n    TYPE RASTER\n    STATUS DEFAULT\n    DATA \"{target_geotiff_file_path}\"\n\n    PROJECTION\n      \"init=epsg:4326\"\n    END\n\n    METADATA\n      \"wms_include_items\" \"all\"\n    END\n    PROCESSING \"NODATA=0\"\n    STATUS ON\n    DUMP TRUE\n    TYPE RASTER\n    OFFSITE 0 0 0\n    CLASSITEM \"[pixel]\"\n    TEMPLATE \"template.html\"\n    INCLUDE \"legend_swe.map\"\n  END\nEND\n\"\"\"\n  \n  with open(geotiff_mapserver_file_path, \"w\") as file:\n    file.write(mapserver_config_content)\n    \n  print(f\"Mapserver config is created at {geotiff_mapserver_file_path}\")\n  return geotiff_mapserver_file_path\n\ndef refresh_available_date_list():\n  \n  # Define columns for the DataFrame\n  columns = [\"date\", \"predicted_swe_url_prefix\"]\n\n  # Create an empty DataFrame with columns\n  df = pd.DataFrame(columns=columns)\n  \n  for filename in os.listdir(geotiff_destination_folder):\n    target_geotiff_file = os.path.join(geotiff_destination_folder, filename)\n    \n    date_str = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", filename).group()\n    date = datetime.strptime(date_str, \"%Y-%m-%d\")\n    \n    # Append a new row to the DataFrame\n    df = df.append({\n      \"date\": date, \n      \"predicted_swe_url_prefix\": f\"../swe_forecasting/output/{filename}\"\n    }, ignore_index=True)\n  \n  # Save DataFrame to a CSV file\n  df.to_csv(\"/var/www/html/swe_forecasting/date_list.csv\", index=False)\n  print(\"directly write into the server file which might be used at the time might not be a good idea. \")\n\n  # Display the final DataFrame\n  print(df)\n  \n\n               \ndef copy_files_to_right_folder():\n  \n  # copy the variable comparison folder\n  source_folder = f\"{work_dir}/var_comparison/\"\n  figure_destination_folder = f\"/var/www/html/swe_forecasting/plots/\"\n  geotiff_destination_folder = f\"/var/www/html/swe_forecasting/output/\"\n\n  # Copy the folder with overwriting existing files/folders\n  distutils.dir_util.copy_tree(source_folder, figure_destination_folder, update=1)\n\n  print(f\"Folder '{source_folder}' copied to '{figure_destination_folder}' with overwriting.\")\n\n\n  # copy the png from testing_output to plots\n  source_folder = f\"{work_dir}/testing_output/\"\n\n  # Ensure the destination folder exists, create it if necessary\n  if not os.path.exists(figure_destination_folder):\n    os.makedirs(figure_destination_folder)\n    \n  if not os.path.exists(geotiff_destination_folder):\n    os.makedirs(geotiff_destination_folder)\n\n  # Loop through the files in the source folder\n  for filename in os.listdir(source_folder):\n    # Check if the file is a PNG file\n    if filename.endswith('.png') or filename.endswith('.tif'):\n      # Build the source and destination file paths\n      source_file = os.path.join(source_folder, filename)\n      destination_file = os.path.join(figure_destination_folder, filename)\n\n      # Copy the file from the source to the destination\n      copy_if_modified(source_file, destination_file)\n      \n      # Copy the file to the output folder if it is geotif\n      if filename.endswith('.tif'):\n        output_dest_file = os.path.join(geotiff_destination_folder, filename)\n        copy_if_modified(source_file, output_dest_file)\n        \n  \n\nif __name__ == \"__main__\":\n  \n  geotiff_destination_folder = f\"/var/www/html/swe_forecasting/output/\"\n  copy_files_to_right_folder()\n  \n  # create mapserver config for all geotiff files in output folder\n  for filename in os.listdir(geotiff_destination_folder):\n    destination_file = os.path.join(geotiff_destination_folder, filename)\n    create_mapserver_map_config(destination_file, force=True)\n  print(\"Finished creation of all mapserver files.\")\n    \n  # refresh the output file list for the website to refresh its calendar\n  refresh_available_date_list()\n  print(\"All done\")\n  time.sleep(10)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2o6cp8",
  "name" : "training_feature_selection",
  "description" : null,
  "code" : "import dask.dataframe as dd\n\n# Replace 'data.csv' with the path to your 50GB CSV file\ninput_csv = '/home/chetana/gridmet_test_run/model_training_data.csv'\n\n# List of columns you want to extract\nselected_columns = ['date', 'lat', 'lon', 'etr', 'pr', 'rmax',\n                    'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', \n                    'elevation',\n                    'slope', 'curvature', 'aspect', 'eastness',\n                    'northness', 'Snow Water Equivalent (in) Start of Day Values']\n\n# Read the CSV file into a Dask DataFrame\ndf = dd.read_csv(input_csv, usecols=selected_columns)\n\n# Rename the column as you intended\ndf = df.rename(columns={\"Snow Water Equivalent (in) Start of Day Values\": \"swe_value\"})\n\n# Replace 'output.csv' with the desired output file name\noutput_csv = '/home/chetana/gridmet_test_run/model_training_cleaned.csv'\n\n# Write the selected columns to a new CSV file\ndf.to_csv(output_csv, index=False, single_file=True)  # single_file=True \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "0n26v2",
  "name" : "amsr_testing_realtime",
  "description" : null,
  "code" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\nfrom scipy.spatial import KDTree\nimport time\nfrom datetime import datetime, timedelta, date\nimport warnings\nimport sys\nfrom convert_results_to_images import plot_all_variables_in_one_csv\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef is_binary(file_path):\n    try:\n        with open(file_path, 'rb') as file:\n            # Read a chunk of bytes from the file\n            chunk = file.read(1024)\n\n            # Check for null bytes, a common indicator of binary data\n            if b'\\x00' in chunk:\n                return True\n\n            # Check for a high percentage of non-printable ASCII characters\n            text_characters = \"\".join(chr(byte) for byte in chunk if 32 <= byte <= 126)\n            if not text_characters:\n                return True\n\n            # If none of the binary indicators are found, assume it's a text file\n            return False\n\n    except FileNotFoundError:\n        print(f\"File '{file_path}' not found.\")\n        return False\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return False\n  \ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    parent_directory = os.path.dirname(target_amsr_hdf_path)\n    if not os.path.exists(parent_directory):\n        os.makedirs(parent_directory)\n        print(f\"Parent directory '{parent_directory}' created successfully.\")\n    \n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid(target_date = test_start_date):\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    \n    \n    \n    # the mapper\n    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    #print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = target_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return target_csv_path\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path) and is_binary(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n        # Check the exit code\n        if result.returncode != 0:\n            print(f\"Command failed with exit code {result.returncode}.\")\n            if os.path.exists(target_amsr_hdf_path):\n              os.remove(target_amsr_hdf_path)\n              print(f\"Wrong {target_amsr_hdf_path} removed successfully.\")\n            raise Exception(f\"Failed to download {target_amsr_hdf_path} - {result.stderr}\")\n    \n    # Read the HDF\n    print(f\"Reading {target_amsr_hdf_path}\")\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n    return target_csv_path\n\ndef add_cumulative_column(df, column_name):\n    df[f'cumulative_{column_name}'] = df[column_name].sum()\n    return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_all_key = row.index\n  \n  x_subset_key = x_all_key[x_all_key.str.startswith(column_name)]\n  #print(\"x_subset_key = \", x_subset_key)\n#   x = np.arange(len(x_subset_key))\n\n#   # Extract Y series (values from the first row)\n#   y = row[x_subset_key]\n# #   print(\"start row: \", y)\n  \n#   # Create a mask for missing values\n#   if column_name == \"AMSR_SWE\":\n#     mask = (y > 240) | y.isnull()\n#   elif column_name == \"fsca\":\n#     mask = (y > 100) | y.isnull() \n#   else:\n#     mask = y.isnull()\n\n#   # Check if all elements in the mask array are True\n#   all_true = np.all(mask)\n\n#   if all_true or len(np.where(~mask)[0]) == 1:\n#     row[x_subset_key] = 0\n# #     print(\"Final all columns: \", row)\n#   else:\n#     # Perform interpolation\n#     #new_y = np.interp(x, x[~mask], y[~mask])\n#     # Replace missing values with interpolated values\n#     #df[column_name] = new_y\n    \n#     try:\n#       # Coefficients of the polynomial fit\n#       #coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n#       # Perform polynomial interpolation\n#       #interpolated_values = np.polyval(coefficients, x)\n\n#       # Merge using np.where\n#       #merged_array = np.where(mask, interpolated_values, y)\n\n#       #row.loc[x_subset_key] = merged_array\n# #       print(\"after assign: \", row)\n#       #print(\"don't interpolate and check the original data\")\n#       pass\n#     except Exception as e:\n#       # Print the error message and traceback\n#       import traceback\n#       traceback.print_exc()\n#       print(\"x:\", x)\n#       print(\"y:\", y)\n#       print(\"mask:\", mask)\n#       print(f\"Error: {e}\")\n#       raise e\n      \n#     if column_name == \"AMSR_SWE\":\n#       row[x_subset_key] = row[x_subset_key].clip(upper=240, lower=0)\n#     elif column_name == \"fsca\":\n#       row[x_subset_key] = row[x_subset_key].clip(upper=100, lower=0)\n#     else:\n#       row[x_subset_key] = row[x_subset_key].clip(upper=240, lower=0)\n      \n#     print(\"after clip: \", row)\n      \n#     if row[x_subset_key].isnull().any():\n#       print(\"x:\", x)\n#       print(\"y:\", y)\n#       print(\"mask:\", mask)\n#       print(\"why row still has values > 100\", row)\n#       raise ValueError(\"Single group: shouldn't have null values here\")\n\n  are_all_values_between_0_and_240 = row[x_subset_key].between(1, 239).all()\n  if are_all_values_between_0_and_240:\n    print(\"row[x_subset_key] = \", row[x_subset_key])\n    print(\"row[x_subset_key].sum() = \", row[x_subset_key].sum())\n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row[x_subset_key].sum()\n  return row\n    \n    \ndef get_cumulative_amsr_data(target_date = test_start_date, force=False):\n    \n    selected_date = datetime.strptime(target_date, \"%Y-%m-%d\")\n    print(selected_date)\n    if selected_date.month < 10:\n      past_october_1 = datetime(selected_date.year - 1, 10, 1)\n    else:\n      past_october_1 = datetime(selected_date.year, 10, 1)\n\n    # Traverse and print every day from past October 1 to the specific date\n    current_date = past_october_1\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{target_date}_cumulative.csv'\n\n    columns_to_be_cumulated = [\"AMSR_SWE\"]\n    \n    gap_filled_csv = f\"{target_csv_path}_gap_filled.csv\"\n    if os.path.exists(gap_filled_csv) and not force:\n      print(f\"{gap_filled_csv} already exists, skipping..\")\n      df = pd.read_csv(gap_filled_csv)\n      print(df[\"AMSR_SWE\"].describe())\n    else:\n      date_keyed_objects = {}\n      data_dict = {}\n      new_df = None\n      while current_date <= selected_date:\n        print(current_date.strftime('%Y-%m-%d'))\n        current_date_str = current_date.strftime('%Y-%m-%d')\n\n        data_dict[current_date_str] = download_amsr_and_convert_grid(current_date_str)\n        current_df = pd.read_csv(data_dict[current_date_str])\n        current_df.drop(columns=[\"date\"], inplace=True)\n\n        if current_date != selected_date:\n          current_df.rename(columns={\n            \"AMSR_SWE\": f\"AMSR_SWE_{current_date_str}\",\n            \"AMSR_Flag\": f\"AMSR_Flag_{current_date_str}\",\n          }, inplace=True)\n        #print(current_df.head())\n\n        if new_df is None:\n          new_df = current_df\n        else:\n          new_df = pd.merge(new_df, current_df, on=['gridmet_lat', 'gridmet_lon'])\n          #new_df = new_df.append(current_df, ignore_index=True)\n\n        current_date += timedelta(days=1)\n\n      print(\"new_df.columns = \", new_df.columns)\n      print(\"new_df.head = \", new_df.head())\n      df = new_df\n\n      #df.sort_values(by=['gridmet_lat', 'gridmet_lon', 'date'], inplace=True)\n      print(\"All current head: \", df.head())\n      print(\"the new_df.shape: \", df.shape)\n\n      print(\"Start to fill in the missing values\")\n      #grouped = df.groupby(['gridmet_lat', 'gridmet_lon'])\n      filled_data = pd.DataFrame()\n\n      # Apply the function to each group\n      for column_name in columns_to_be_cumulated:\n        start_time = time.time()\n        #filled_data = df.apply(lambda row: interpolate_missing_and_add_cumulative_inplace(row, column_name), axis=1)\n        #alike_columns = filled_data.filter(like=column_name)\n        #filled_data[f'cumulative_{column_name}'] = alike_columns.sum(axis=1)\n        print(\"filled_data.columns = \", filled_data.columns)\n        filtered_columns = df.filter(like=column_name)\n        print(filtered_columns.columns)\n        filtered_columns = filtered_columns.mask(filtered_columns > 240)\n        filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n        filtered_columns.fillna(0, inplace=True)\n        \n        sum_column = filtered_columns.sum(axis=1)\n        # Define a specific name for the new column\n        df[f'cumulative_{column_name}'] = sum_column\n        df[filtered_columns.columns] = filtered_columns\n        \n        if filtered_columns.isnull().any().any():\n          print(\"filtered_columns :\", filtered_columns)\n          raise ValueError(\"Single group: shouldn't have null values here\")\n        \n        \n        \n\n        # Concatenate the original DataFrame with the Series containing the sum\n        #df = pd.concat([df, sum_column.rename(new_column_name)], axis=1)\n#         cumulative_column = filled_data.filter(like=column_name).sum(axis=1)\n#         filled_data[f'cumulative_{column_name}'] = cumulative_column\n        #filled_data = pd.concat([filled_data, cumulative_column], axis=1)\n        print(\"filled_data.columns: \", filled_data.columns)\n        end_time = time.time()\n        # Calculate the elapsed time\n        elapsed_time = end_time - start_time\n        print(f\"calculate column {column_name} elapsed time: {elapsed_time} seconds\")\n\n#       if any(filled_data['AMSR_SWE'] > 240):\n#         raise ValueError(\"Error: shouldn't have AMSR_SWE > 240 at this point\")\n      filled_data = df\n      filled_data[\"date\"] = target_date\n      print(\"Finished correctly \", filled_data.head())\n      filled_data.to_csv(gap_filled_csv, index=False)\n      print(f\"New filled values csv is saved to {gap_filled_csv}\")\n      df = filled_data\n    \n    result = df\n    print(\"result.head = \", result.head())\n    # fill in the rest NA as 0\n    if result.isnull().any().any():\n      print(\"result :\", result)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    # only retain the rows of the target date\n    print(result['date'].unique())\n    print(result.shape)\n    print(result[[\"AMSR_SWE\", \"AMSR_Flag\"]].describe())\n    result.to_csv(target_csv_path, index=False)\n    print(f\"New data is saved to {target_csv_path}\")\n    \n      \n    \nif __name__ == \"__main__\":\n    # Run the download and conversion function\n    #prepare_amsr_grid_mapper()\n    prepare_amsr_grid_mapper()\n#     download_amsr_and_convert_grid()\n    \n    get_cumulative_amsr_data(force=False)\n    input_time_series_file = f'{work_dir}/testing_ready_amsr_{test_start_date}_cumulative.csv_gap_filled.csv'\n\n    #plot_all_variables_in_one_csv(input_time_series_file, f\"{input_time_series_file}.png\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rvqv35",
  "name" : "perform_download.sh",
  "description" : null,
  "code" : "#!/bin/bash\n\n# Specify the file containing the download links\ninput_file=\"/home/chetana/gridmet_test_run/amsr/download_links.txt\"\n\n# Specify the base wget command with common options\nbase_wget_command=\"wget --http-user=<your_username> --http-password=<your_password> --load-cookies /home/chetana/gridmet_test_run/amsr/mycookies.txt --save-cookies mycookies.txt --keep-session-cookies --no-check-certificate -$\n\n# Specify the output directory for downloaded files\noutput_directory=\"/home/chetana/gridmet_test_run/amsr\"\n\n# Ensure the output directory exists\nmkdir -p \"$output_directory\"\n\n# Loop through each line (URL) in the input file and download it using wget\nwhile IFS= read -r url; do\n    echo \"Downloading: $url\"\n    $base_wget_command -P \"$output_directory\" \"$url\"\ndone < \"$input_file\"",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "vo8bc9",
  "name" : "merge_custom_traning_range",
  "description" : null,
  "code" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir, homedir\nimport pandas as pd\nimport time\n\nworking_dir = work_dir\nfinal_output_name = \"final_merged_data_4yrs_snotel_and_ghcnd_stations.csv\"\nchunk_size = '10MB'  # You can adjust this chunk size based on your hardware and data size\namsr_file = f'{working_dir}/all_training_points_in_westus.csv_amsr_dask_all_training_ponits.csv'\nsnotel_file = f'{working_dir}/all_snotel_cdec_stations_active_in_westus.csv_swe_restored_dask_all_vars.csv'\nghcnd_file = f'{working_dir}/active_station_only_list.csv_all_vars_masked_non_snow.csv'\nall_station_obs_file = f'{working_dir}/snotel_ghcnd_all_obs.csv'\ngridmet_file = f'{working_dir}/training_all_point_gridmet_with_snotel_ghcnd.csv'\nterrain_file = f'{working_dir}/all_training_points_with_ghcnd_in_westus.csv_terrain_4km_grid_shift.csv'\nfsca_file = f'{homedir}/fsca/fsca_final_training_all.csv'\nfinal_final_output_file = f'{work_dir}/{final_output_name}'\n\n\ndef merge_snotel_ghcnd_together():\n    snotel_df = pd.read_csv(snotel_file)\n    ghcnd_df = pd.read_csv(ghcnd_file)\n    print(snotel_df.columns)\n    print(ghcnd_df.columns)\n    ghcnd_df = ghcnd_df.rename(columns={'STATION': 'station_name',\n                                       'DATE': 'date',\n                                       'LATITUDE': 'lat',\n                                       'LONGITUDE': 'lon',\n                                       'SNWD': 'snow_depth',})\n    df_combined = pd.concat([snotel_df, ghcnd_df], axis=0, ignore_index=True)\n    df_combined.to_csv(all_station_obs_file, index=False)\n    print(f\"All snotel ang ghcnd are saved to {all_station_obs_file}\")\n    \n\ndef merge_all_data_together():\n    \n    if os.path.exists(final_final_output_file):\n      print(f\"The file '{final_final_output_file}' exists. Skipping\")\n      return final_final_output_file\n    \n    # merge the snotel and ghcnd together first\n    \n      \n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(amsr_file, blocksize=chunk_size)\n    print(\"amsr.columns = \", amsr.columns)\n    ground_truth = dd.read_csv(all_station_obs_file, blocksize=chunk_size)\n    print(\"ground_truth.columns = \", ground_truth.columns)\n#     gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(gridmet_file, blocksize=chunk_size)\n    gridmet = gridmet.drop(columns=[\"Unnamed: 0\"])\n    print(\"gridmet.columns = \", gridmet.columns)\n    terrain = dd.read_csv(terrain_file, blocksize=chunk_size)\n    terrain = terrain.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    terrain = terrain[[\"lat\", \"lon\", 'Elevation', 'Slope', 'Aspect', 'Curvature', 'Northness', 'Eastness']]\n    print(\"terrain.columns = \", terrain.columns)\n    snowcover = dd.read_csv(fsca_file, blocksize=chunk_size)\n    snowcover = snowcover.rename(columns={\n      \"latitude\": \"lat\", \n      \"longitude\": \"lon\"\n    })\n    print(\"snowcover.columns = \", snowcover.columns)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    ground_truth = ground_truth.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n    snow_cover = snowcover.repartition(partition_size=chunk_size)\n    print(\"all the dataframes are partitioned\")\n\n    # Merge DataFrames based on specified columns\n    print(\"start to merge amsr and ground_truth\")\n    merged_df = dd.merge(amsr, ground_truth, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_ground_truth.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge gridmet\")\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_gridmet.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge terrain\")\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_terrain.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    print(\"start to merge snowcover\")\n    merged_df = dd.merge(merged_df, snow_cover, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = merged_df.drop_duplicates(keep='first')\n    output_file = os.path.join(working_dir, f\"{final_output_name}_snow_cover.csv\")\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f\"intermediate file saved to {output_file}\")\n    \n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, final_output_name)\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print(f'Merge completed. {output_file}')\n\ndef cleanup_dataframe():\n    \"\"\"\n    Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    \"\"\"\n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    df = dd.read_csv(final_final_output_file, dtype=dtype)\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(final_final_output_file, single_file=True, index=False)\n    print('Data cleaning completed.')\n    return final_final_output_file\n\n  \ndef sort_training_data(input_training_csv, sorted_training_csv):\n    # Read Dask DataFrame from CSV with increased blocksize and assuming missing data\n    dtype = {'station_name': 'object'}  # 'object' dtype represents strings\n    ddf = dd.read_csv(input_training_csv, assume_missing=True, blocksize='10MB', dtype=dtype)\n\n    # Persist the Dask DataFrame in memory\n    ddf = ddf.persist()\n\n    # Sort Dask DataFrame by three columns: date, lat, and Lon\n    sorted_ddf = ddf.sort_values(by=['date', 'lat', 'lon'])\n\n    # Save the sorted Dask DataFrame to a new CSV file\n    sorted_ddf.to_csv(sorted_training_csv, index=False, single_file=True)\n    print(f\"sorted training data is saved to {sorted_training_csv}\")\n  \nif __name__ == \"__main__\":\n  \n#     merge_snotel_ghcnd_together()\n  \n#     merge_all_data_together()\n#     cleanup_dataframe()\n#     final_final_output_file = f'{work_dir}/{final_output_name}'\n#     sort_training_data(final_final_output_file, f'{work_dir}/{final_output_name}_sorted.csv')\n    \n    start_time = time.time()\n    \n    merge_all_data_together()\n    print(f\"Time taken for merge_all_data_together: {time.time() - start_time} seconds\")\n    \n    start_time = time.time()\n    cleanup_dataframe()\n    print(f\"Time taken for cleanup_dataframe: {time.time() - start_time} seconds\")\n    \n    final_final_output_file = f'{work_dir}/{final_output_name}'\n    sorted_output_file = f'{work_dir}/{final_output_name}_sorted.csv'\n    \n    start_time = time.time()\n    sort_training_data(final_final_output_file, sorted_output_file)\n    print(f\"Time taken for sort_training_data: {time.time() - start_time} seconds\")\n    ",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6evkh4",
  "name" : "training_data_range",
  "description" : null,
  "code" : "\"\"\"\nThis script loads and processes several CSV files into Dask DataFrames, applies filters, renames columns,\nand saves the resulting DataFrames to new CSV files based on a specified time range.\n\nAttributes:\n    gridmet_20_years_file (str): File path of the GridMET climatology data CSV file.\n    snotel_20_years_file (str): File path of the SNOTEL data CSV file.\n    terrain_file (str): File path of the terrain data CSV file.\n    amsr_3_years_file (str): File path of the AMSR data CSV file.\n    output_file (str): File path where the merged and processed data will be saved.\n\nFunctions:\n    clip_csv_using_time_range: Main function that loads, processes, and saves CSV data based on a specified time range.\n\"\"\"\n\nfrom snowcast_utils import work_dir\nimport dask.dataframe as dd\n\ndef clip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file):\n    \"\"\"\n    Loads, processes, and saves CSV data into Dask DataFrames, applies filters, renames columns, and saves the resulting\n    DataFrames to new CSV files based on a specified time range.\n\n    Args:\n        gridmet_20_years_file (str): File path of the GridMET climatology data CSV file.\n        snotel_20_years_file (str): File path of the SNOTEL data CSV file.\n        terrain_file (str): File path of the terrain data CSV file.\n        amsr_3_years_file (str): File path of the AMSR data CSV file.\n        output_file (str): File path where the merged and processed data will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # Load CSV files into Dask DataFrames\n    gridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\n    snotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\n    terrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\n    amsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n    # Filter and rename columns for each DataFrame in a single step\n    # (Code to filter and rename columns...)\n\n    # Save the processed Dask DataFrames to new CSV files\n    gridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\n    amsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\n    snotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n\n# Define file paths and execute the function\ngridmet_20_years_file = f\"{work_dir}/gridmet_climatology/testing_ready_gridmet.csv\"\nsnotel_20_years_file = f\"{work_dir}/training_ready_snotel_data.csv\"\nterrain_file = f'{work_dir}/training_ready_terrain.csv'\namsr_3_years_file = f\"{work_dir}/training_amsr_data.csv\"\noutput_file = f\"{work_dir}/training_ready_merged_data_dd.csv\"\n\nclip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "76ewp5",
  "name" : "amsr_features",
  "description" : null,
  "code" : "import os\nimport csv\nimport h5py\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport dask\nimport dask.dataframe as dd\nimport dask.delayed as delayed\nimport dask.bag as db\nimport xarray as xr\nfrom snowcast_utils import work_dir, train_start_date, train_end_date\nimport warnings\n\n# Suppress specific warning\nwarnings.filterwarnings(\"ignore\", message=\"overflow encountered in add\")\n\n\ndef copy_he5_files(source_dir, destination_dir):\n    '''\n    Copy .he5 files from the source directory to the destination directory.\n\n    Args:\n        source_dir (str): The source directory containing .he5 files to copy.\n        destination_dir (str): The destination directory where .he5 files will be copied.\n\n    Returns:\n        None\n    '''\n    # Get a list of all subdirectories and files in the source directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if file.endswith('.he5'):\n                # Get the absolute path of the source file\n                source_file_path = os.path.join(root, file)\n                # Copy the file to the destination directory\n                shutil.copy(source_file_path, destination_dir)\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    '''\n    Find the index of the grid cell with the closest coordinates to the target latitude and longitude.\n\n    Args:\n        target_latitude (float): The target latitude.\n        target_longitude (float): The target longitude.\n        lat_grid (numpy.ndarray): An array of latitude values.\n        lon_grid (numpy.ndarray): An array of longitude values.\n\n    Returns:\n        Tuple[int, int, float, float]: A tuple containing the row index, column index, closest latitude, and closest longitude.\n    '''\n    # Compute the absolute differences between target and grid coordinates\n    lat_diff = np.abs(lat_grid - target_latitude)\n    lon_diff = np.abs(lon_grid - target_longitude)\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n\ndef create_snotel_ghcnd_station_to_amsr_mapper(\n  new_base_station_list_file, \n  target_csv_path\n):\n    station_data = pd.read_csv(new_base_station_list_file)\n    \n    \n    date = \"2022-10-01\"\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    \n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        df = pd.read_csv(target_csv_path)\n        return df\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'station_lat', 'station_lon'])\n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    for idx, row in station_data.iterrows():\n        target_lat = row['latitude']\n        target_lon = row['longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat,\n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n    return df\n  \n  \ndef extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, new_base_station_list_file, start_date, end_date):\n    if os.path.exists(output_csv_file):\n        os.remove(output_csv_file)\n    \n    target_csv_path = f'{work_dir}/training_snotel_ghcnd_station_to_amsr_mapper_all_training_points.csv'\n    mapper_df = create_snotel_ghcnd_station_to_amsr_mapper(new_base_station_list_file, \n                                         target_csv_path)\n        \n    # station_data = pd.read_csv(new_base_station_list_file)\n\n    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n\n    # Create a Dask DataFrame\n    dask_station_data = dd.from_pandas(mapper_df, npartitions=1)\n\n    # Function to process each file\n    def process_file(filename):\n        file_path = os.path.join(amsr_data_dir, filename)\n        print(file_path)\n        \n        file = h5py.File(file_path, 'r')\n        hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n\n        date_str = filename.split('_')[-1].split('.')[0]\n        date = datetime.strptime(date_str, '%Y%m%d')\n\n        if not (start_date <= date <= end_date):\n            print(f\"{date} is not in the training period, skipping..\")\n            return None\n\n        new_date_str = date.strftime(\"%Y-%m-%d\")\n        swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n        flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n        # Create an empty Pandas DataFrame with the desired columns\n        result_df = pd.DataFrame(columns=['date', 'lat', 'lon', 'AMSR_SWE'])\n\n        # Sample loop to add rows to the Pandas DataFrame using dask.delayed\n        @delayed\n        def process_row(row, swe, new_date_str):\n          closest_lat_idx = int(row['amsr_lat_idx'])\n          closest_lon_idx = int(row['amsr_lon_idx'])\n          closest_swe = swe[closest_lat_idx, closest_lon_idx]\n          \n          return pd.DataFrame([[\n            new_date_str, \n            row['station_lat'],\n            row['station_lon'],\n            closest_swe]], \n            columns=result_df.columns\n          )\n\n\n        # List of delayed computations\n        delayed_results = [process_row(row, swe, new_date_str) for _, row in mapper_df.iterrows()]\n\n        # Compute the delayed results and concatenate them into a Pandas DataFrame\n        result_df = dask.compute(*delayed_results)\n        result_df = pd.concat(result_df, ignore_index=True)\n\n        # Print the final Pandas DataFrame\n        #print(result_df)\n          \n        return result_df\n\n    # Get the list of files\n    files = [f for f in os.listdir(amsr_data_dir) if f.endswith('.he5')]\n\n    # Create a Dask Bag from the files\n    dask_bag = db.from_sequence(files, npartitions=2)\n\n    # Process files in parallel\n    processed_data = dask_bag.map(process_file).filter(lambda x: x is not None).compute()\n\n    # Concatenate the processed data\n    combined_df = pd.concat(processed_data, ignore_index=True)\n\n    # Save the combined DataFrame to a CSV file\n    combined_df.to_csv(output_csv_file, index=False)\n\n    print(f\"Merged data saved to {output_csv_file}\")\n\n                    \nif __name__ == \"__main__\":\n    amsr_data_dir = '/home/chetana/gridmet_test_run/amsr'\n    # new_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\n    all_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\n    new_base_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n    print(new_base_df.head())\n    output_csv_file = f\"{all_training_points_with_snotel_ghcnd_file}_amsr_dask_all_training_ponits_with_ghcnd.csv\"\n    \n    start_date = train_start_date\n    end_date = train_end_date\n\n    extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file, all_training_points_with_snotel_ghcnd_file, start_date, end_date)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "5wzgx5",
  "name" : "amsr_swe_data_download",
  "description" : null,
  "code" : "from datetime import datetime, timedelta\nimport os\nimport subprocess\n\ndef generate_links(start_year, end_year):\n    '''\n    Generate a list of download links for AMSR daily snow data files.\n\n    Args:\n        start_year (int): The starting year.\n        end_year (int): The ending year (inclusive).\n\n    Returns:\n        list: A list of download links for AMSR daily snow data files.\n    '''\n    base_url = \"https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/\"\n    date_format = \"%Y.%m.%d\"\n    delta = timedelta(days=1)\n\n    start_date = datetime(start_year, 1, 1)\n    end_date = datetime(end_year + 1, 1, 1)\n\n    links = []\n    current_date = start_date\n\n    while current_date < end_date:\n        date_str = current_date.strftime(date_format)\n        link = base_url + date_str + \"/AMSR_U2_L3_DailySnow_B02_\" + date_str + \".he5\"\n        links.append(link)\n        current_date += delta\n\n    return links\n\nif __name__ == \"__main__\":\n    start_year = 2019\n    end_year = 2022\n\n    links = generate_links(start_year, end_year)\n    save_location = \"/home/chetana/gridmet_test_run/amsr\"\n    with open(\"/home/chetana/gridmet_test_run/amsr/download_links.txt\", \"w\") as txt_file:\n      for l in links:\n        txt_file.write(\" \".join(l) + \"\\n\")\n\n    #if not os.path.exists(save_location):\n    #    os.makedirs(save_location)\n\n    #for link in links:\n    #    filename = link.split(\"/\")[-1]\n    #    save_path = os.path.join(save_location, filename)\n    #    curl_cmd = f\"curl -b ~/.urs_cookies -c ~/.urs_cookies -L -n -o {save_path} {link}\"\n    #    subprocess.run(curl_cmd, shell=True, check=True)\n        # print(f\"Downloaded: {filename}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "d4zcq6",
  "name" : "install_dependencies",
  "description" : null,
  "code" : "#!/bin/bash\n\n# change this line if you are creating a new env\nsource /home/ubuntu/anaconda3/bin/activate\nconda activate new_env\nwhich python\npython --version\n\n# install GDAL\nsudo apt update\nsudo apt install gdal-bin libgdal-dev -y\n\n# Use cat to create the requirements.txt file\ncat <<EOL > requirements.txt\nabsl-py==1.4.0\naffine==2.4.0\naiobotocore==2.7.0\naioitertools==0.11.0\nanaconda-navigator==2.4.2\nansi2html==1.8.0\nanyio==3.6.2\nargon2-cffi==21.3.0\nargon2-cffi-bindings==21.2.0\nasciitree==0.3.3\nastunparse==1.6.3\nasync-generator==1.10\nattrs==22.2.0\nBabel==2.11.0\nbackcall==0.2.0\nbackports-datetime-fromisoformat==2.0.0\nbackports.weakref==1.0.post1\nbackports.zoneinfo==0.2.1\nbasemap-data==1.3.2\nbasemap-data-hires==1.3.2\nbleach==4.1.0\nbotocore==1.31.64\nbounded-pool-executor==0.0.3\nbrotlipy==0.7.0\ncatboost==1.2\ncategory-encoders==2.6.1\ncertifi==2023.5.7\ncffi==1.15.1\ncftime==1.6.2\ncharset-normalizer==2.0.12\nclick==8.0.4\nclick-plugins==1.1.1\ncligj==0.7.2\ncloudpickle==2.2.1\nclyent==1.2.2\ncolorcet==3.0.1\nconda-repo-cli==1.0.75\nconda-verify==3.4.2\nConfigSpace==0.7.1\ncontextvars==2.4\ncontourpy==1.1.1\ncycler==0.11.0\ndash==2.11.1\ndash-core-components==2.0.0\ndash-html-components==2.0.0\ndash-table==5.0.0\ndask==2023.9.2\ndataclasses==0.8\ndatashader==0.15.2\ndatashape==0.5.2\ndecorator==5.1.1\ndefusedxml==0.7.1\nDeprecated==1.2.14\ndeprecation==2.1.0\ndistributed==2023.9.2\nearthaccess==0.7.1\nemcee==3.1.4\nentrypoints==0.4\nfasteners==0.18\nfilelock==3.4.1\nFiona==1.9.4.post1\nflaky==3.7.0\nFlask==2.2.5\nflatbuffers==23.5.26\nfonttools==4.42.1\nfsspec==2023.10.0\ngast==0.4.0\nGDAL\ngeojson==3.0.1\ngeopandas==0.14.0\ngoogle-auth-oauthlib==1.0.0\ngoogle-pasta==0.2.0\ngraphviz==0.20.1\ngstatsim==1.0.1\nh5py==3.9.0\nhtmldate==1.4.2\nhuggingface-hub==0.4.0\nidna==3.4\nimageio==2.31.3\nimbalanced-learn==0.11.0\nimgaug==0.4.0\nimmutables==0.19\nimportlib-resources==5.4.0\nipykernel==5.5.6\nipython==7.16.3\nipython-genutils==0.2.0\nisodate==0.6.1\nitsdangerous==2.1.2\njedi==0.17.2\nJinja2==3.0.3\njmespath==1.0.1\njoblib==1.3.2\njson5==0.9.11\njsonpointer==2.1\njsonschema==3.2.0\njupyter-client==7.1.2\njupyter-core==4.9.2\njupyter-dash==0.4.2\njupyter-server==1.13.1\njupyterlab==3.2.9\njupyterlab-iframe==0.4.0\njupyterlab-pygments==0.1.2\njupyterlab-server==2.10.3\nkaleido==0.2.1\nkeras==2.13.1\nkeras-core==0.1.0\nkeras-nlp==0.6.0\nkeras-tuner==1.3.5\nkiwisolver==1.4.5\nkt-legacy==1.0.5\nlazy_loader==0.1\nlibclang==16.0.0\nlightgbm==3.3.5\nllvmlite==0.40.1\nlocket==1.0.0\nlockfile==0.12.2\nMarkdown==3.4.4\nmarkdown-it-py==3.0.0\nmatplotlib==3.8.0\nmdurl==0.1.2\nmistune==0.8.4\nmkl-fft==1.3.1\nmkl-service==2.4.0\nmsgpack==1.0.5\nmultimethod==1.10\nmultipledispatch==1.0.0\nnamex==0.0.7\nnavigator-updater==0.3.0\nnbclassic==0.3.5\nnbclient==0.5.9\nnbconvert==6.0.7\nnbformat==5.1.3\nnest-asyncio==1.5.6\nnetCDF4==1.5.8\nnotebook==6.4.10\nnumba==0.57.1\nnumcodecs==0.11.0\nnvidia-cublas-cu11==11.10.3.66\nnvidia-cuda-nvrtc-cu11==11.7.99\nnvidia-cuda-runtime-cu11==11.7.99\nnvidia-cudnn-cu11==8.5.0.96\noauthlib==3.2.2\nopencv-python==4.7.0.68\nopt-einsum==3.3.0\norjson==3.9.2\npackage-skimage==0.0.1\npackaging==21.3\npandas==1.5.3\npandocfilters==1.5.0\nparam==1.13.0\nparso==0.7.1\npartd==1.4.0\npexpect==4.8.0\npickleshare==0.7.5\nPillow==9.4.0\nPint==0.18\nplanetary-computer==0.4.9\nplotly==5.17.0\nplotly-resampler==0.8.3.2\nply==3.11\npmdarima==2.0.3\nPolygon3==3.0.9.1\npqdm==0.2.0\nprometheus-client==0.16.0\nprompt-toolkit==3.0.36\nprotobuf==3.20.3\nptyprocess==0.7.0\npyasn1-modules==0.2.8\npycparser==2.21\npyct==0.5.0\npydantic==1.10.5\npyEddyTracker==3.6.1\npygeoweaver==0.9.14\nPygments==2.14.0\npynisher==0.6.4\npyod==1.1.0\npyparsing==3.0.9\npyproj==3.6.1\nPyQt5-sip==12.11.0\npyrfr==0.8.3\npyrsistent==0.18.0\npyshp==2.3.1\npystac==1.6.1\npystac-client==0.6.0\npython-cmr==0.9.0\npython-dateutil==2.8.2\npython-dotenv==1.0.0\npytz==2022.7.1\nPyYAML==6.0\npyzmq==25.0.2\nrasterio==1.3.9\nregex==2023.6.3\nrequests==2.27.1\nrequests-oauthlib==1.3.1\nrequests-toolbelt==0.10.1\nretrying==1.3.4\nrich==13.4.2\nrioxarray==0.13.4\ns3fs==2023.10.0\nsacremoses==0.0.53\nschemdraw==0.15\nscikit-base==0.5.0\nscikit-image==0.20.0\nscikit-learn==1.3.0\nscikit-plot==0.3.7\nscipy==1.11.2\nseaborn==0.12.2\nSend2Trash==1.8.0\nshap==0.42.1\nshapely==2.0.1\nsix==1.16.0\nsktime==0.20.0\nslicer==0.0.7\nsmac==1.4.0\nsniffio==1.2.0\nsnuggs==1.4.7\nsortedcontainers==2.4.0\ntbats==1.1.3\ntblib==2.0.0\ntenacity==8.2.3\ntensorboard==2.13.0\ntensorboard-data-server==0.7.0\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.13.0\ntensorflow-estimator==2.13.0\ntensorflow-hub==0.13.0\ntensorflow-io-gcs-filesystem==0.32.0\ntensorflow-text==2.13.0\ntermcolor==2.3.0\nterminado==0.12.1\ntestpath==0.6.0\nthreadpoolctl==3.2.0\ntinynetrc==1.3.1\ntokenizers==0.12.1\ntorch==1.13.1\ntorchaudio==0.13.1\ntorchmetrics==0.11.1\ntorchvision==0.14.1\ntornado==6.1\ntornado-proxy-handlers==0.0.5\ntqdm==4.64.1\ntrace-updater==0.0.9.1\ntraitlets==4.3.3\ntransformers==4.18.0\ntyping_extensions==4.5.0\nuritemplate==3.0.1\nurllib3==1.26.15\nwcwidth==0.2.6\nwebencodings==0.5.1\nwebsocket-client==1.3.1\nWerkzeug==2.2.3\nwrapt==1.15.0\nxarray==2023.8.0\nxarray-spatial==0.3.5\nxxhash==3.2.0\nyellowbrick==1.5\nzarr==2.14.1\nzeep==4.2.1\nzict==3.0.0\nzipp==3.6.0\nEOL\n\npython -m pip install -r requirements.txt\n# clean up\nrm requirements.txt\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6x6myw",
  "name" : "model_evaluation",
  "description" : null,
  "code" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit(0)  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "r4knm9",
  "name" : "interpret_model_results",
  "description" : null,
  "code" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\nfeature_names = None\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness',\n                         'cumulative_AMSR_SWE': 'cumulative_SWE',\n                         'cumulative_AMSR_Flag': 'cumulative_Flag',\n                         'cumulative_tmmn':'cumulative_air_temperature_tmmn',\n                         'cumulative_etr': 'cumulative_potential_evapotranspiration',\n                         'cumulative_vpd': 'cumulative_mean_vapor_pressure_deficit',\n                         'cumulative_rmax': 'cumulative_relative_humidity_rmax', \n                         'cumulative_rmin': 'cumulative_relative_humidity_rmin',\n                         'cumulative_pr': 'cumulative_precipitation_amount',\n                         'cumulative_tmmx': 'cumulative_air_temperature_tmmx',\n                         'cumulative_vs': 'cumulative_wind_speed',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                        }, inplace=True)\n\n    desired_order = ['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness', 'cumulative_SWE',\n'cumulative_Flag', 'cumulative_air_temperature_tmmn',\n'cumulative_potential_evapotranspiration',\n'cumulative_mean_vapor_pressure_deficit',\n'cumulative_relative_humidity_rmax',\n'cumulative_relative_humidity_rmin', 'cumulative_precipitation_amount',\n'cumulative_air_temperature_tmmx', 'cumulative_wind_speed']\n    \n    feature_names = desired_order\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    data = data.drop(['lat', 'lon',], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \ninterpret_prediction()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "9c573m",
  "name" : "train_test_pattern_compare",
  "description" : null,
  "code" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    #tr_df = tr_df.drop('date', axis=1)\n    #te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n    \n    te_df = te_df.apply(pd.to_numeric, errors='coerce')\n    print(\"te_df describe: \", te_df.describe())\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n        else:\n          print(f\"Error: {col} is not in testing csv\")\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ee5ur4",
  "name" : "correct_slope",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom snowcast_utils import work_dir\nfrom scipy.spatial import KDTree\nimport dask.dataframe as dd\nfrom dask.distributed import Client\n\nready_csv_path = f'{work_dir}/final_merged_data_4yrs_snotel_and_ghcnd_stations.csv_sorted.csv'\ndem_slope_csv_path = f\"{work_dir}/slope_file.tif.csv\"\nprint(f\"ready_csv_path = {ready_csv_path}\")\nnew_result_csv_path = f'{work_dir}/final_merged_data_4yrs_snotel_ghcnd.csv_sorted_slope_corrected.csv'\n\n\ndef replace_slope(row, tree, dem_df):\n    '''\n    Replace the 'slope' column in the input DataFrame row with the closest slope value from the DEM data.\n\n    Args:\n        row (pandas.Series): A row of data containing 'lat' and 'lon' columns.\n        tree (scipy.spatial.KDTree): KDTree built from DEM data.\n        dem_df (pandas.DataFrame): DataFrame containing DEM data.\n\n    Returns:\n        float: The closest slope value from the DEM data for the given latitude and longitude.\n    '''\n    # print(\"row = \", row)\n    target_lat = row[\"lat\"]\n    target_lon = row[\"lon\"]\n    _, idx = tree.query([(target_lat, target_lon)])\n    closest_row = dem_df.iloc[idx[0]]\n    return closest_row[\"Slope\"]\n\ndef parallelize_slope_correction():\n    # Start Dask client\n#     client = Client()\n\n    # Scatter DEM data\n#     dem_future = client.scatter(dem_slope_df)\n    # Read the cleaned ready CSV and DEM slope CSV\n    train_ready_df = pd.read_csv(ready_csv_path)\n    dem_slope_df = pd.read_csv(dem_slope_csv_path)\n\n    print(train_ready_df.head())\n    print(dem_slope_df.head())\n\n    print(\"all train.csv columns: \", train_ready_df.columns)\n    print(\"all dem slope columns: \", dem_slope_df.columns)\n    \n    # Build KDTree for DEM data\n    tree = KDTree(dem_slope_df[['Latitude', 'Longitude']].values)\n    \n    print(\"deduplicate the training point lat/lon\")\n    print(\"train_ready_df.shape = \", train_ready_df.shape)\n    lat_lon_df = train_ready_df[['lat', 'lon']].drop_duplicates()\n\n    print(\"lat_lon_df.shape\", lat_lon_df.shape)\n    # Apply the 'replace_slope' function to calculate and replace slope values in the DataFrame\n    print(\"start to correct slope\")\n    #train_ready_df['corrected_slope'] = train_ready_df.apply(replace_slope, args=(tree, dem_slope_df), axis=1)\n    \n    # Apply the function with scattered data and log progress\n#     train_ready_ddf['corrected_slope'] = train_ready_ddf.map_partitions(\n#         lambda df: df.apply(replace_slope, args=(tree, dem_future)), \n#         meta=('slope', 'float64')\n#     )\n#     train_ready_ddf['corrected_slope'].compute(progress_callback=progress)\n\n    lat_lon_df['corrected_slope'] = lat_lon_df.apply(replace_slope, args=(tree, dem_slope_df), axis=1)\n  \n    train_ready_df = train_ready_df.merge(lat_lon_df, on=['lat', 'lon'], how='left')\n    \n    print(\"The new train_ready_df.shape with corrected slope is \", train_ready_df.shape)\n\n    print(\"finished correcting slope\")\n    print(train_ready_df.head())\n    print(train_ready_df.columns)\n\n    \n    print(f\"saving the correct data into {new_result_csv_path}\")\n    # Save the modified DataFrame to a new CSV file\n    train_ready_df.to_csv(new_result_csv_path, index=False)\n    print(f\"The new slope corrected dataframe is saved to {new_result_csv_path}\")\n    \n  \n  \nif __name__ == \"__main__\":\n    parallelize_slope_correction()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "f03i7p",
  "name" : "convert_to_time_series",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom snowcast_utils import work_dir\nimport shutil\nimport numpy as np\nimport dask.dataframe as dd\n\n# Set Pandas options to display all columns\npd.set_option('display.max_columns', None)\n\n# Define file paths for various CSV files\n# current_ready_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\ncurrent_ready_csv_path = f'{work_dir}/final_merged_data_4yrs_snotel_ghcnd.csv_sorted_slope_corrected.csv'\nnon_station_counted_csv_path = f'{work_dir}/snotel_ghcnd_stations_4yrs_fill_empty_snotel.csv'\ncleaned_csv_path = f\"{work_dir}/snotel_ghcnd_stations_4yrs_cleaned.csv\"\ntarget_time_series_csv_path = f'{work_dir}/snotel_ghcnd_stations_4yrs_time_series.csv'\nbackup_time_series_csv_path = f'{work_dir}/snotel_ghcnd_stations_4yrs_time_series_backup.csv'\n# target_time_series_cumulative_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_cumulative_v1.csv'\ntarget_time_series_cumulative_csv_path = f'{work_dir}/snotel_ghcnd_stations_4yrs_cumulative.csv'\nslope_renamed_path = f'{work_dir}/snotel_ghcnd_stations_4yrs_slope_renamed.csv'\nlogged_csv_path = f'{work_dir}/snotel_ghcnd_stations_4yrs_all_cols_log10.csv'\ndeduplicated_csv_path = f'{work_dir}/deduplicated_training_points_final.csv'\n\n\ndef array_describe(arr):\n    \"\"\"\n    Calculate descriptive statistics for a given NumPy array.\n\n    Args:\n        arr (numpy.ndarray): The input array for which statistics are calculated.\n\n    Returns:\n        dict: A dictionary containing descriptive statistics such as Mean, Median, Standard Deviation, Variance, Minimum, Maximum, and Sum.\n    \"\"\"\n    stats = {\n        'Mean': np.mean(arr),\n        'Median': np.median(arr),\n        'Standard Deviation': np.std(arr),\n        'Variance': np.var(arr),\n        'Minimum': np.min(arr),\n        'Maximum': np.max(arr),\n        'Sum': np.sum(arr),\n    }\n    return stats\n\ndef interpolate_missing_inplace(df, column_name, degree=3):\n    x = df.index\n    y = df[column_name]\n\n    # Create a mask for missing values\n    if column_name == \"SWE\":\n      mask = (y > 240) | y.isnull()\n    elif column_name == \"fsca\":\n      y = y.replace([225, 237, 239], 0)\n      y[y < 0] = 0\n      mask = (y > 100) | y.isnull()\n    else:\n      mask = y.isnull()\n\n    # Check if all elements in the mask array are True\n    all_true = np.all(mask)\n\n    if all_true:\n      df[column_name] = 0\n    else:\n      # Perform interpolation\n      new_y = np.interp(x, x[~mask], y[~mask])\n      # Replace missing values with interpolated values\n      df[column_name] = new_y\n\n    if np.any(df[column_name].isnull()):\n      raise ValueError(\"Single group: shouldn't have null values here\")\n        \n    return df\n\ndef convert_to_time_series(input_csv, output_csv, force=False):\n    \"\"\"\n    Convert the data from the ready CSV file into a time series format.\n\n    This function reads the cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation, and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n    \"\"\"\n    \n    columns_to_be_time_series = [\"SWE\", \n                                 'air_temperature_tmmn',\n                                 'potential_evapotranspiration', \n                                 'mean_vapor_pressure_deficit',\n                                 'relative_humidity_rmax', \n                                 'relative_humidity_rmin',\n                                 'precipitation_amount', \n                                 'air_temperature_tmmx', \n                                 'wind_speed',\n                                 'fsca']\n\n    # Read the cleaned ready CSV\n    df = pd.read_csv(input_csv, dtype={'station_name': 'object'})\n    df.sort_values(by=['lat', 'lon', 'date'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    # rename all columns to unified names\n    #     ['date', 'lat', 'lon', 'SWE', 'Flag', 'swe_value', 'cell_id',\n# 'station_id', 'etr', 'pr', 'rmax', 'rmin', 'tmmn', 'tmmx', 'vpd', 'vs',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness',\n# 'fsca']\n    df.rename(columns={'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                        }, inplace=True)\n    \n    filled_csv = f\"{output_csv}_gap_filled.csv\"\n    if os.path.exists(filled_csv) and not force:\n        print(f\"{filled_csv} already exists, skipping\")\n        filled_data = pd.read_csv(filled_csv)\n    else:\n        # Function to perform polynomial interpolation and fill in missing values\n        def process_group_filling_value(group):\n          # Sort the group by 'date'\n          group = group.sort_values(by='date')\n      \n          for column_name in columns_to_be_time_series:\n            group = interpolate_missing_inplace(group, column_name)\n          # Return the processed group\n          return group\n        # Group the data by 'lat' and 'lon' and apply interpolation for each column\n        print(\"Start to fill in the missing values\")\n        grouped = df.groupby(['lat', 'lon'])\n        filled_data = grouped.apply(process_group_filling_value).reset_index(drop=True)\n    \n\n        if any(filled_data['SWE'] > 240):\n          raise ValueError(\"Error: shouldn't have SWE>240 at this point\")\n\n        filled_data.to_csv(filled_csv, index=False)\n        \n        print(f\"New filled values csv is saved to {filled_csv}\")\n    \n    if os.path.exists(output_csv) and not force:\n        print(f\"{output_csv} already exists, skipping\")\n    else:\n        df = filled_data\n        # Create a new DataFrame to store the time series data for each location\n        print(\"Start to create the training csv with previous 7 days columns\")\n        result = pd.DataFrame()\n\n        # Define the number of days to consider (7 days in this case)\n        num_days = 7\n\n        grouped = df.groupby(['lat', 'lon'])\n        \n        def process_group_time_series(group, num_days):\n          group = group.sort_values(by='date')\n          for day in range(1, num_days + 1):\n            for target_col in columns_to_be_time_series:\n              new_column_name = f'{target_col}_{day}'\n              group[new_column_name] = group[target_col].shift(day)\n              \n          return group\n        \n        result = grouped.apply(lambda group: process_group_time_series(group, num_days)).reset_index(drop=True)\n        result.fillna(0, inplace=True)\n        \n        result.to_csv(output_csv, index=False)\n        print(f\"New data is saved to {output_csv}\")\n        shutil.copy(output_csv, backup_time_series_csv_path)\n        print(f\"File is backed up to {backup_time_series_csv_path}\")\n\ndef add_cumulative_columns(input_csv, output_csv, force=False):\n    \"\"\"\n    Add cumulative columns to the time series dataset.\n\n    This function reads the time series CSV file created by `convert_to_time_series`, calculates cumulative values for specific columns, and saves the data to a new CSV file.\n    \"\"\"\n    \n    columns_to_be_cumulated = [\n      \"SWE\",\n      'air_temperature_tmmn',\n      'potential_evapotranspiration', \n      'mean_vapor_pressure_deficit',\n      'relative_humidity_rmax', \n      'relative_humidity_rmin',\n      'precipitation_amount', \n      'air_temperature_tmmx', \n      'wind_speed',\n      'fsca'\n    ]\n\n    # Read the time series CSV (ensure it was created using `convert_to_time_series` function)\n    # directly read from original file\n    df = pd.read_csv(input_csv, dtype={'station_name': 'object'})\n    print(\"the column statistics from time series before cumulative: \", df.describe())\n    \n    df['date'] = pd.to_datetime(df['date'])\n    \n    unique_years = df['date'].dt.year.unique()\n    print(\"This is our unique years\", unique_years)\n    #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n    \n    # only start from the water year 10-01\n    # Filter rows based on the date range (2019 to 2022)\n    start_date = pd.to_datetime('2018-10-01')\n    end_date = pd.to_datetime('2021-09-30')\n    df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n    print(\"how many rows are left in the three water years?\", df.describe())\n    df.to_csv(f\"{current_ready_csv_path}.test_check.csv\")\n\n    # Define a function to calculate the water year\n    def calculate_water_year(date):\n        year = date.year\n        if date.month >= 10:  # Water year starts in October\n            return year + 1\n        else:\n            return year\n    \n    # every water year starts at Oct 1, and ends at Sep 30. \n    df['water_year'] = df['date'].apply(calculate_water_year)\n    \n    # Group the DataFrame by 'lat' and 'lon'\n    grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n    print(\"how many groups? \", grouped)\n    \n    grouped = df.groupby(['lat', 'lon', 'water_year'], group_keys=False)\n    for column in columns_to_be_cumulated:\n        df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n    print(\"This is the dataframe after cumulative columns are added\")\n    print(df[\"cumulative_fsca\"].describe())\n    \n    df.to_csv(output_csv, index=False)\n    \n    print(f\"All the cumulative variables are added successfully! {target_time_series_cumulative_csv_path}\")\n    print(\"double check the swe_value statistics:\", df[\"swe_value\"].describe())\n\n    \ndef assign_zero_swe_value_to_all_fsca_zero_rows(na_filled_csv, non_station_zero_csv, force=False):\n    \n    # Define the conditions\n    condition_column = 'fsca'\n    target_column = 'swe_value'\n    values_to_check = [0, 225, 237, 239]\n    \n    \n    df = pd.read_csv(na_filled_csv, dtype={'station_name': 'object'})\n    empty_count = df[target_column].isnull().values.ravel().sum()\n    \n    print(f\"The empty number of rows are {empty_count} before filling in\")\n    print(\"double check the swe_value statistics before filling in:\", df[\"swe_value\"].describe())\n    \n    rows_less_than_zero = (df[target_column] < 0).sum()\n    print(\"Number of rows where '{}' is less than 0: {}\".format(target_column, rows_less_than_zero))\n    \n\n    # Mask the target column where the condition is met\n    df[target_column] = df[target_column].mask(\n        (df[target_column].isna()) & df[condition_column].isin(values_to_check),\n        0\n    )\n    \n    empty_count = df[target_column].isnull().values.ravel().sum()\n    \n    print(f\"The empty number of rows are {empty_count} after filling in\")\n    \n    print(\"total dataframe row number : \", len(df))\n    \n    df.to_csv(non_station_zero_csv, index=False)\n    \n    print(f\"The rows without snotel but fsca is zero or land or water or ocean are set to 0! {non_station_zero_csv}\")\n    print(\"double check the swe_value statistics after filling in:\", df[\"swe_value\"].describe())\n    \n    \ndef clean_non_swe_rows(current_ready_csv_path, cleaned_csv_path, force=False):\n    # Read Dask DataFrame from CSV\n    dask_df = dd.read_csv(current_ready_csv_path, dtype={'station_name': 'object'})\n\n    # Remove rows where 'swe_value' is empty\n    dask_df_filtered = dask_df.dropna(subset=['swe_value'])\n\n    # Save the result to a new CSV file\n    dask_df_filtered.to_csv(cleaned_csv_path, index=False, single_file=True)\n    print(\"dask_df_filtered.shape = \", dask_df_filtered.shape)\n    print(f\"The filtered csv with no swe values is saved to {cleaned_csv_path}\")\n\ndef rename_corrected_slope(corrected_slope_path, renamed_slope_path, force=False):\n    df = pd.read_csv(corrected_slope_path, dtype={'station_name': 'object'})\n    df.drop(columns=['Slope'], inplace=True)\n\t# Rename 'column_to_rename' to 'old_column'\n    df.rename(columns={'corrected_slope': 'Slope'}, inplace=True)\n    df.to_csv(renamed_slope_path, index=False)\n    print(\"dask_df.shape = \", df.shape)\n    print(f\"The log10 file is saved to {renamed_slope_path}\")\n    \ndef log10_all_fields(cleaned_csv_path, logged_csv_path, force=False):\n    print(\"convert all cumulative columns into log10\")\n    # Read Dask DataFrame from CSV\n    df = pd.read_csv(cleaned_csv_path, dtype={'station_name': 'object'})\n    \n    # Get columns with \"cumulative\" in their names\n    for col in df.columns:\n        print(\"Checking \", col)\n        if \"cumulative\" in col:\n\t        # Apply log10 transformation to selected columns\n            df[col] = np.log10(df[col] + 0.1)  # Adding 1 to avoid log(0)\n            print(f\"converted {col} to log10\")\n\n    df.to_csv(logged_csv_path, index=False)\n    print(\"dask_df.shape = \", df.shape)\n    print(f\"The log10 file is saved to {logged_csv_path}\")\n\n\nif __name__ == \"__main__\":\n    \n    # filling the non station rows with fsca indicating no snow\n    assign_zero_swe_value_to_all_fsca_zero_rows(current_ready_csv_path, non_station_counted_csv_path, force=True)\n    \n    # remove the empty swe_value rows first\n    clean_non_swe_rows(non_station_counted_csv_path, cleaned_csv_path, force=True)\n  \n    # Uncomment this line to execute the 'convert_to_time_series' function\n    convert_to_time_series(cleaned_csv_path, target_time_series_csv_path, force=True)\n\n    # Uncomment this line to execute the 'add_cumulative_columns' function\n    add_cumulative_columns(target_time_series_csv_path, target_time_series_cumulative_csv_path, force=True)\n    \n    # Rename the corrected slope to slope\n    rename_corrected_slope(target_time_series_cumulative_csv_path, slope_renamed_path, force=True)\n    \n    # convert all cumulative columns to log10\n    log10_all_fields(slope_renamed_path, logged_csv_path, force=True)\n    \n    df = pd.read_csv(logged_csv_path, dtype={'station_name': 'object'})\n    print(\"the number of the total rows: \", len(df))\n    \n    deduplicated_df = df.drop_duplicates(subset=['lat', 'lon'])\n    # Export the deduplicated DataFrame to a CSV file\n    deduplicated_df.to_csv(deduplicated_csv_path, index=False)\n    print(f\"deduplicated_df.to_csv('{deduplicated_csv_path}', index=False)\")\n    \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "83d2yv",
  "name" : "data_merge_hackweek",
  "description" : null,
  "code" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the training files are here\noriginal_training_points = f\"{work_dir}/training_points.csv\"\npmw_training = f\"{work_dir}/PMW_training.csv\"\npmw_training_new = f\"{work_dir}/PMW_training_new.csv\"\nlandcover_training = f\"{work_dir}/lc_data_train.csv\"\nua_model_data = f\"{work_dir}/ua_single_csv_file.csv\" # this is not used due to no overlap with study area\nfsca_training = f\"{work_dir}/fSCA_trainingCells/fSCA_trainingCells_2019.csv\"\nsnowclassification_training_data = f\"{work_dir}/hackweek_testing2.csv\"\namsr_gridmet_terrain_training_data = f\"{work_dir}/training_data_20_years_cleaned.csv_hackweek_subset_all_years.csv\"\n# amsr_testing_data = f\"{work_dir}/amsr_2017_2018_testing.csv\"\n# amsr_training_data = f\"{work_dir}/amsr_2017_2018_training_points.csv\"\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\n  \ndef filter_2years_from_20_years():\n  pd.set_option('display.max_columns', None)\n\n  all_ready_csv_path = f'{work_dir}/training_data_20_years_cleaned.csv'\n\n  #current_ready_csv_path = f'{work_dir}/testing_all_ready.csv'\n\n  # Step 1: Read the CSV file into a pandas DataFrame\n  file_path = all_ready_csv_path\n  df = pd.read_csv(file_path)\n  print(df.columns)\n\n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = df[(df['lat'] >= min_lat) & \n                   (df['lat'] <= max_lat) & \n                   (df['lon'] >= min_lon) & \n                   (df['lon'] <= max_lon)]\n\n  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  start_date = pd.to_datetime('2017-01-01')\n  end_date = pd.to_datetime('2018-12-31')\n  filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{all_ready_csv_path}_hackweek_subset_all_years_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\n\n\ndef merge_all_training_data_together():\n  df5 = pd.read_csv(amsr_gridmet_terrain_training_data)\n  df1 = pd.read_csv(landcover_training)\n  df2 = pd.read_csv(pmw_training_new)\n  df3 = pd.read_csv(fsca_training)\n  df4 = pd.read_csv(snowclassification_training_data)\n  df4 = df4.rename(columns={'Date': 'date', \n                           'long': 'lon'})\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  print(df4.head())\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_training.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")\n  \n\n  \ndef filter_water_year_winter_months_only():\n  \n  df = pd.read_csv(f\"{work_dir}/all_merged_training.csv\")\n  df['date'] = pd.to_datetime(df['date'])\n\n  # Define the time range window\n  start_date = pd.to_datetime('2017-10-01')\n  end_date = pd.to_datetime('2018-07-01')\n\n  # Create a boolean mask to filter rows within the time range\n  mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n\n  # Apply the mask to the DataFrame to keep only the rows within the time range\n  df = df[mask]\n  print(df.head())\n  df.to_csv(f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\")\n\n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\n\ndef add_no_snow_data_points():\n  current_training_file_path = f\"{work_dir}/all_merged_training_cum_water_year_winter_month_only.csv\"\n  final_testing_file_path = f\"{work_dir}/all_merged_testing_with_station_elevation.csv\"\n  final_training_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only_with_no_snow.csv\"\n  old_train_df = pd.read_csv(current_training_file_path)\n  snotel_covered_len = len(old_train_df)\n  print(f\"the snotel covered area: {snotel_covered_len}\")\n  old_train_df.drop([\"pmv\", \"SnowClass\", \"cumulative_fSCA\"], axis=1, inplace=True)\n  final_test_df = pd.read_csv(final_testing_file_path)\n  zero_fsca_rows = final_test_df[final_test_df['fSCA'] == 0]\n  # Use the sample method to randomly select rows\n  chosen_no_snow_row_df = zero_fsca_rows.sample(n=snotel_covered_len, random_state=42)  # You can set a random_state for reproducibility\n  chosen_no_snow_row_df[\"swe_value\"] = 0\n  # Use the list to select the subset of columns\n  chosen_no_snow_row_df = chosen_no_snow_row_df[old_train_df.columns]\n  print(f\"len of no snow dataframe: {len(chosen_no_snow_row_df)}\")\n  concatenated_df = pd.concat([old_train_df, chosen_no_snow_row_df], ignore_index=True)\n  concatenated_df.to_csv(final_training_file_path)\n  print(f\"final training data is saved to {final_training_file_path}\")\n\n\n# this is section for preparing training data\n#convert_pmv_to_right_format()\n# filter_2years_from_20_years()\n# merge_all_training_data_together()\n#filter_water_year_winter_months_only()\n#create_accumulative_columns()\nadd_no_snow_data_points()\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "j8swco",
  "name" : "data_gee_smap_station_only",
  "description" : null,
  "code" : "# for Bonan to work on pulling the SMAP data for training and testing points\n\n\n\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nfrom snowcast_utils import work_dir\n\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{work_dir}/testing_points.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_names = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'lat', 'lon'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      #current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      #print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{work_dir}/{org_name}_{column_name}_{ind}.csv\"\n\n#       if os.path.exists(single_csv_file):\n#           print(\"exists skipping..\")\n#           continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      #poi = ee.Geometry.Point(longitude, latitude)\n      viirs = ee.ImageCollection(product_name).filterDate('2017-10-01','2018-07-01').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      #df['cell_id'] = current_cell_id\n      df['lat'] = latitude\n      df['lon'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nprint(all_cell_df.head())\nprint(all_cell_df[\"s1_grd_vv\"].describe())\nall_cell_df.to_csv(f\"{work_dir}/Sentinel1_Testing.csv\")\nprint(\"The Sentinel 1 is downloaded successfully. \")\n\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "pnr64x",
  "name" : "data_merge_hackweek_testing",
  "description" : null,
  "code" : "# This process is to merge the land cover data into the training.csv\nimport glob\nimport pandas as pd\nfrom snowcast_utils import work_dir\nimport re\nfrom datetime import datetime\nimport os\nimport shutil\nimport numpy as np\nfrom gridmet_testing import download_gridmet_of_specific_variables, turn_gridmet_nc_to_csv, gridmet_var_mapping\nfrom datetime import datetime, timedelta\n\n# list all the csvs to merge here\ndem_data = f\"{work_dir}/dem_all.csv\"\n\n# all the testing files are here\noriginal_testing_points = f\"{work_dir}/testing_points.csv\"\nlandcover_testing = f\"{work_dir}/lc_data_test.csv\"\npmw_testing = f\"{work_dir}/PMW_testing.csv\"\npmw_testing_new = f\"{work_dir}/pmw_testing_new.csv\"\nsnowclassification_testing_data = f\"{work_dir}/snowclassification_hackweek_testing.csv\"\nterrain_testing_data = f\"{work_dir}/dem_all.csv_hackweek_subset_testing.csv\"\nfsca_testing_data = f\"{work_dir}/fsca_testing_all_years.csv\"\ngridmet_testing_data = f\"{work_dir}/gridmet_testing_hackweek_subset.csv\"\n\n\ndef convert_pmv_to_right_format():\n  # convert pmv data into the same format\n  column_names = [\"date\", \"lat\", \"lon\", \"pmv\"]\n  \n  #pmv_new_df = pd.DataFrame(columns=column_names)\n  \n  def adjust_column_to_rows(row):\n\t\n    for property_name, value in row.items():\n      #print(\"property_name: \", property_name)\n      if property_name == \"Time\":\n          continue\n\n      column_data = row[property_name]  # Access the column data\n      #match = re.search(r'\\((.*?),\\s(.*?)\\)', property_name) # for training\n      match = re.search(r'([-+]?\\d+\\.\\d+), ([-+]?\\d+\\.\\d+)', property_name) # for testing data\n\n      # Convert the input time string to a datetime object\n      datetime_obj = datetime.strptime(row[\"Time\"], \"%m/%d/%Y %H:%M\")\n\n      # Convert the datetime object to the desired format\n      formatted_time_string = datetime_obj.strftime(\"%Y-%m-%d\")\n\n      if match:\n          lat = match.group(1)\n          lon = match.group(2)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n          \n          #print(\"Latitude:\", lat, \"Longitude:\", lon, \"pmw:\", new_row_data)\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n\n      else:\n          match = re.search(r'\\((.*?),\\s(.*?)', property_name)\n          lat = match.group(1)\n          lon = match.group(2)\n          #print(\"Latitude:\", lat)\n          #print(\"Longitude:\", lon)\n          new_row_data = [formatted_time_string, lat, lon, column_data]\n\n          # Create a new DataFrame with the new row\n          new_row_df = pd.DataFrame([new_row_data], columns=column_names)\n\n          # Concatenate the new row DataFrame with the original DataFrame\n          #pmv_new_df = pd.concat([pmv_new_df, new_row_df], ignore_index=True)\n          return pd.Series(new_row_data)\n  \n  pmv_old_df = pd.read_csv(pmw_testing)\n  \n  pmv_new_df = pmv_old_df.apply(lambda row: adjust_column_to_rows(row), axis=1)\n  \n  pmv_new_df.columns = column_names\n\n  print(pmv_new_df.head())\n  pmv_new_df.to_csv(pmw_testing_new, index=False)\n  print(f\"New PMV file is saved!!! {pmw_training_new}\")\n\ndef collect_gridmet_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  # download gridmet\n  download_gridmet_of_specific_variables([2017, 2018])\n  \n  # convert all the dates in the year to csvs\n  start_date = datetime(2017, 10, 1)\n  end_date = datetime(2018, 7, 1)\n\n  # Define the step (1 day)\n  step = timedelta(days=1)\n\n  # Initialize the current date with the start date\n  current_date = start_date\n  \n  # Traverse the days using a loop\n  all_days_df = None\n  while current_date <= end_date:\n      current_date_str = current_date.strftime('%Y-%m-%d')\n      print(f\"processing {current_date_str}\")\n      #turn_gridmet_nc_to_csv(current_date.strftime('%Y-%m-%d'))\n      # example file: 2017_etr_2017-10-01_hackweek_subset.csv\n      # step 1: combine all the variables\n      single_day_df = None\n\n      year = current_date.year\n      for key in gridmet_var_mapping.keys():\n        print(key)\n        single_year_var_df = pd.read_csv(f\"{work_dir}/testing_output/{year}_{key}_{current_date_str}_hackweek_subset.csv\")\n        single_year_var_df[\"date\"] = current_date_str\n        if single_day_df is None or single_day_df.empty:\n          single_day_df = single_year_var_df\n        else:\n          single_day_df = pd.merge(single_day_df, single_year_var_df, on=[\"Latitude\", \"Longitude\", \"date\"])\n\n      \n      if all_days_df is None or all_days_df.empty:\n        all_days_df = single_day_df\n      else:\n        all_days_df = pd.concat([all_days_df, single_day_df], axis=0) \n        \n      \n      current_date += step\n  print(all_days_df.head())\n  all_days_df.to_csv(gridmet_testing_data, index=False)\n  print(f\"Ok, all gridmet data for testing days are saved to {gridmet_testing_data}\")\n  \ndef create_gridmet_dem_mapper_subset():\n  all_mapper = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n  filtered_df = all_mapper[(all_mapper['dem_lat'] >= min_lat) & \n                   (all_mapper['dem_lat'] <= max_lat) & \n                   (all_mapper['dem_lon'] >= min_lon) & \n                   (all_mapper['dem_lon'] <= max_lon)]\n  subset_csv_path = f'{work_dir}/gridmet_to_dem_mapper_hackweek_subset.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_terrain_for_testing():\n  testing_points_df = pd.read_csv(original_testing_points)\n  print(testing_points_df.head())\n  \n  dem_all_df = pd.read_csv(dem_data)\n  print(dem_all_df.head())\n  \n  # Step 2: Define the geometry bounding box\n  # Replace these values with the actual bounding box coordinates\n  min_lat, max_lat = 37.75, 38.75\n  min_lon, max_lon = -119.75, -118.75\n\n  dem_all_df.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n  \n  # Step 3: Filter the DataFrame to keep rows within the geometry\n  filtered_df = dem_all_df[(dem_all_df['lat'] >= min_lat) & \n                   (dem_all_df['lat'] <= max_lat) & \n                   (dem_all_df['lon'] >= min_lon) & \n                   (dem_all_df['lon'] <= max_lon)]\n\n  #filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n\n  # Filter rows based on the date range (2017 to 2018)\n  #start_date = pd.to_datetime('2017-01-01')\n  #end_date = pd.to_datetime('2018-12-31')\n  #filtered_df = filtered_df[(filtered_df['date'] >= start_date) & (filtered_df['date'] <= end_date)]\n  \n  #lat_lon_df = filtered_df[['lat', 'lon']]\n\n  # Step 4: Display or process the filtered data\n  print(\"Filtered Data:\")\n  print(filtered_df.head())\n\n  # Step 5: (Optional) Write the filtered data to a new CSV file\n  subset_csv_path = f'{dem_data}_hackweek_subset_testing.csv'\n  filtered_df.to_csv(subset_csv_path, index=False)\n  print(f\"The subset of the rows is saved to {subset_csv_path}\")\n\n  # Step 6: copy it to the website folder\n  destination_folder = \"/var/www/html/swe_forecasting/\"\n  shutil.copy(subset_csv_path, destination_folder)\n  \n\n  # Step 7: Done!\n  print(\"Done\")\n\ndef collect_amsr_for_testing():\n  # it seems we might not need AMSR\n  pass\n  \n\ndef merge_fsca_testing():\n  \n  #fsca_testing_single_year_folder = f\"{work_dir}/fSCA_testingCells/\"\n  # Define a list to store the paths to the CSV files\n  csv_files = glob.glob(f'{work_dir}/fSCA_testingCells/*.csv')  # Replace 'path/to/files/' with the actual path to your CSV files\n  \n\n  # Initialize an empty list to store DataFrames\n  dataframes = []\n\n  # Iterate through the CSV files and read them into DataFrames\n  for csv_file in csv_files:\n      df = pd.read_csv(csv_file)\n      dataframes.append(df)\n\n  # Concatenate the DataFrames into one\n  merged_df = pd.concat(dataframes, ignore_index=True)\n  data_sanity_checks(merged_df)\n  \n  # Now, 'merged_df' contains the merged data from all CSV files\n  print(merged_df)\n  final_fsca_testing_file = f'{work_dir}/fsca_testing_all_years.csv'\n  merged_df.to_csv(final_fsca_testing_file, index=False)\n  print(f\"All years of data are saved to {final_fsca_testing_file}\")\n  \n  \n\ndef create_accumulative_columns(csv_file_path=None):\n  if csv_file_path is None:\n  \tcsv_file_path = f\"{work_dir}/all_merged_training_water_year_winter_month_only.csv\"\n  current_df = pd.read_csv(csv_file_path)\n  print(current_df.head())\n  current_df['date'] = pd.to_datetime(current_df['date'])\n  #current_df['fSCA'] = current_df['fSCA'].fillna(0)\n\n  # Group the DataFrame by 'lat' and 'lon'\n  grouped = current_df.groupby(['lat', 'lon'], group_keys=False)\n  \n  # Sort each group by date and calculate cumulative precipitation\n  \n  cum_columns = [\"etr\", \"rmax\", \"rmin\", \"tmmn\", \"tmmx\", \"vpd\", \"vs\", \"pr\"]\n  for column in cum_columns:\n  \tcurrent_df[f'cumulative_{column}'] = grouped.apply(lambda group: group.sort_values('date')[column].cumsum())\n\n  print(current_df.head())\n  hackweek_cum_csv = f\"{csv_file_path}_cum.csv\"\n  current_df.to_csv(hackweek_cum_csv, index=False)\n  print(f\"All the cumulative variables are added successfully! {hackweek_cum_csv}\")\n\ndef add_elevation_in_feet():\n  all_df = pd.read_csv(f\"{work_dir}/all_merged_testing_cum_water_year_winter_month_only.csv\")\n  all_df['station_elevation'] = all_df['elevation'] * 3.28084\n  all_df.to_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  print(f\"all data is saved to {work_dir}/all_merged_testing_with_station_elevation.csv\")\n  \n\ndef data_sanity_checks(df):\n  #all_csv_df = pd.read_csv(f\"{work_dir}/all_merged_testing_with_station_elevation.csv\")\n  #points_df = pd.read_csv(data_path)\n  # Get unique locations\n  unique_locations = df[['lat', 'lon']].drop_duplicates()\n\n  # Display the unique locations DataFrame\n  #print(unique_locations)\n  print(len(unique_locations))\n  if len(unique_locations) < 700:\n    raise ValueError(\"Number of unique locations is less than 700\")\n\ndef merge_all_testing_data_together():\n  df5 = pd.read_csv(gridmet_testing_data)\n  df5 = df5.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df5)\n  \n  df1 = pd.read_csv(landcover_testing)\n  data_sanity_checks(df1)\n  #df1['date'] = df1['date'].dt.strftime('%Y-%m-%d')\n  #df2 = pd.read_csv(pmw_testing_new)\n  #data_sanity_checks(df2)\n  #df2['date'] = df2['date'].dt.strftime('%Y-%m-%d')\n  #print(\"d2 types: \", df2.dtypes)\n  df3 = pd.read_csv(fsca_testing_data)\n  data_sanity_checks(df3)\n  #df3['date'] = df3['date'].dt.strftime('%Y-%m-%d')\n  print(df3.dtypes)\n  #df4 = pd.read_csv(snowclassification_testing_data)\n  \n#   df4 = df4.rename(columns={'Date': 'date', \n#                            'long': 'lon'})\n#   data_sanity_checks(df4)\n  #df4['date'] = df4['date'].dt.strftime('%Y-%m-%d')\n  #print(df4.head())\n  df6 = pd.read_csv(terrain_testing_data)\n  df6 = df6.rename(columns={'Latitude': 'lat', \n                           'Longitude': 'lon'})\n  data_sanity_checks(df6)\n  #df6['date'] = df6['date'].dt.strftime('%Y-%m-%d')\n\n  merged_df = pd.merge(df5, df1, on=['date', 'lat', 'lon'], how='left')\n  #merged_df = pd.merge(merged_df, df2, on=['date', 'lat', 'lon'], how='left')\n  #merged_df[\"pmv\"] = 0\n  merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n  print(\"check head: \", merged_df.head())\n  #merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  merged_df = pd.merge(merged_df, df6, on=['lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df3, on=['date', 'lat', 'lon'], how='left')\n#   merged_df = pd.merge(merged_df, df4, on=['date', 'lat', 'lon'], how='left')\n  #merged_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n  print(merged_df.columns)\n  \n  # Set a custom index using a combination of date, lat, and lon\n  merged_df.set_index(['date', 'lat', 'lon'], inplace=True)\n\n  # Remove duplicated rows based on the custom index\n  merged_df = merged_df[~merged_df.index.duplicated(keep='first')]\n\n  # Reset the index to restore the original structure\n  merged_df.reset_index(inplace=True)\n\n  # # Print the merged DataFrame\n  print(merged_df)\n  training_hackweek_csv = f\"{work_dir}/all_merged_testing.csv\"\n  merged_df.to_csv(training_hackweek_csv, index=False)\n  print(f\"Training data is saved to {training_hackweek_csv}\")  \n\n\n# this is section for preparing testing data\n#create_gridmet_dem_mapper_subset()\n#collect_gridmet_for_testing()\n#collect_terrain_for_testing()\n#collect_amsr_for_testing()\n#merge_fsca_testing()\n#merge_all_testing_data_together()\n#create_accumulative_columns(f\"{work_dir}/all_merged_testing.csv\")\n#add_elevation_in_feet()\n\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "qg80lj",
  "name" : "training_sanity_check",
  "description" : null,
  "code" : "# Do sanity checks on the training.csv to make sure all the columns' vales are extracted correctly\nfrom snowcast_utils import work_dir\nimport pandas as pd\nfrom gridmet_testing import download_gridmet_of_specific_variables\nfrom datetime import datetime\nimport xarray as xr\nfrom datetime import date\nimport numpy as np\n\n\n# pick the final training csv\ncurrent_training_csv_path = f'{work_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv_time_series_cumulative_v1.csv'\ndf = pd.read_csv(current_training_csv_path)\n\nprint(\"all the current columns: \", df.columns)\n\n# choose several random sample rows for sanity checks\nsample_size = 10\nrandom_sample = df.sample(n=sample_size)\n\nprint(random_sample)\n\n# all the current columns: Index(['date', 'level_0', 'index', 'lat', 'lon', 'SWE', 'Flag', 'swe_value',\n# 'Unnamed: 0', 'air_temperature_tmmn', 'potential_evapotranspiration',\n# 'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n# 'relative_humidity_rmin', 'precipitation_amount',\n# 'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n# 'aspect', 'eastness', 'northness', 'SWE_1', 'Flag_1',\n# 'air_temperature_tmmn_1', 'potential_evapotranspiration_1',\n# 'mean_vapor_pressure_deficit_1', 'relative_humidity_rmax_1',\n# 'relative_humidity_rmin_1', 'precipitation_amount_1',\n# 'air_temperature_tmmx_1', 'wind_speed_1', 'SWE_2', 'Flag_2',\n# 'air_temperature_tmmn_2', 'potential_evapotranspiration_2',\n# 'mean_vapor_pressure_deficit_2', 'relative_humidity_rmax_2',\n# 'relative_humidity_rmin_2', 'precipitation_amount_2',\n# 'air_temperature_tmmx_2', 'wind_speed_2', 'SWE_3', 'Flag_3',\n# 'air_temperature_tmmn_3', 'potential_evapotranspiration_3',\n# 'mean_vapor_pressure_deficit_3', 'relative_humidity_rmax_3',\n# 'relative_humidity_rmin_3', 'precipitation_amount_3',\n# 'air_temperature_tmmx_3', 'wind_speed_3', 'SWE_4', 'Flag_4',\n# 'air_temperature_tmmn_4', 'potential_evapotranspiration_4',\n# 'mean_vapor_pressure_deficit_4', 'relative_humidity_rmax_4',\n# 'relative_humidity_rmin_4', 'precipitation_amount_4',\n# 'air_temperature_tmmx_4', 'wind_speed_4', 'SWE_5', 'Flag_5',\n# 'air_temperature_tmmn_5', 'potential_evapotranspiration_5',\n# 'mean_vapor_pressure_deficit_5', 'relative_humidity_rmax_5',\n# 'relative_humidity_rmin_5', 'precipitation_amount_5',\n# 'air_temperature_tmmx_5', 'wind_speed_5', 'SWE_6', 'Flag_6',\n# 'air_temperature_tmmn_6', 'potential_evapotranspiration_6',\n# 'mean_vapor_pressure_deficit_6', 'relative_humidity_rmax_6',\n# 'relative_humidity_rmin_6', 'precipitation_amount_6',\n# 'air_temperature_tmmx_6', 'wind_speed_6', 'SWE_7', 'Flag_7',\n# 'air_temperature_tmmn_7', 'potential_evapotranspiration_7',\n# 'mean_vapor_pressure_deficit_7', 'relative_humidity_rmax_7',\n# 'relative_humidity_rmin_7', 'precipitation_amount_7',\n# 'air_temperature_tmmx_7', 'wind_speed_7'],\n\n\ndef check_gridmet(row):\n  # check air_temperature_tmmn, precipitation_amount\n  date_value = row[\"date\"]\n  lat = row[\"lat\"]\n  lon = row[\"lon\"]\n  # Specify the format of the date string\n  date_format = '%Y-%m-%d'\n\n  # Convert the date string to a date object\n  date_object = datetime.strptime(date_value, date_format).date()\n  yearlist = [date_object.year]\n  download_gridmet_of_specific_variables(yearlist)\n  \n  nc_file = f\"{work_dir}/gridmet_climatology/tmmn_{date_object.year}.nc\"\n  \n  dataset = xr.open_dataset(nc_file)\n  reference_date = date(1900, 1, 1)\n\n  # Calculate the difference in days\n  days_difference = (date_object - reference_date).days\n  \n  # Calculate the Euclidean distance between the target coordinate and all grid points\n  lat_diff = dataset['lat'].values - lat\n  lon_diff = dataset['lon'].values - lon\n  #distance = np.sqrt(lat_diff**2 + lon_diff**2)\n\n  # Find the indices (i, j) of the grid cell with the minimum distance\n  i = np.argmin(np.abs(lat_diff))\n  j = np.argmin(np.abs(lon_diff))\n  nearest_gridmet_lat = dataset['lat'][i]\n  nearest_gridmet_lon = dataset['lon'][j]\n  \n  selected_data = dataset.sel(day=date_value, lat=nearest_gridmet_lat, lon=nearest_gridmet_lon)\n  \n  tmmn_check_values = selected_data.air_temperature.values\n  \n  if str(tmmn_check_values) != str(row[\"air_temperature_tmmn\"]):\n    print(\"Failed sanity check. Gridmet doesn't match\")\n    exit(1)\n\ndef check_elevation(row):\n  lat = row[\"lat\"]\n  lon = row[\"lon\"]\n  \n  pass\n\ndef check_amsr(row):\n  \n  pass\n\ndef check_snow_cover_area(row):\n  pass\n\ndef check_passive_microwave(row):\n  pass\n\ndef check_snotel_cdec(row):\n  \n  pass\n\n\ndef check_observed_columns():\n  # Assuming 'work_dir' is the path to your working directory\n  dask_df = dd.read_csv(f\"{work_dir}/final_merged_data_3yrs_all_active_stations_v1.csv_sorted.csv\")\n\n  # Print the shape and head of the Dask DataFrame\n  print(dask_df.shape[0].compute(), 'rows and', dask_df.shape[1].compute(), 'columns')\n  print(dask_df.head().compute())\n\n  # Count the number of empty rows in 'swe_value'\n  empty_rows_count = dask_df['swe_value'].isnull().sum().compute()\n  print(f\"Number of empty rows in 'swe_value': {empty_rows_count}\")\n\nif __name__ == \"__main__\":\n  # random_sample.apply(check_gridmet, axis=1)\n  # random_sample.apply(check_elevation, axis=1)\n  # random_sample.apply(check_amsr, axis=1)\n  # random_sample.apply(check_snow_cover_area, axis=1)\n  # random_sample.apply(check_passive_microwave, axis=1)\n#   random_sample.apply(check_snotel_cdec, axis=1)\n  check_observed_columns()\n\n  print(\"If it reaches here, everything is good. The training.csv passed all our sanity cheks!\")\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ggy7gf",
  "name" : "fSCA_training",
  "description" : null,
  "code" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import date_to_julian, work_dir\n\n# change directory before running the code\nos.chdir(\"/home/chetana/fsca/\")\n\nstart_date = datetime(2018, 1, 1)\nend_date = datetime(2022, 12, 31)\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \"h12v04\", \"h12v05\",\n             \"h13v04\", \"h13v05\", \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\n\n\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n  hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n  # Specific subdataset name you're interested in\n  target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n  # Create a name for the output file based on the HDF file name and subdataset\n  output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n  output_path = os.path.join(output_folder, output_file_name)\n\n  if os.path.exists(output_path):\n    pass\n    #print(f\"The file {output_path} exists. skip.\")\n  else:\n    for subdataset in hdf_ds.GetSubDatasets():\n      # Check if the subdataset is the one we want to convert\n      if target_subdataset_name in subdataset[0]:\n        ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n        # Convert to GeoTIFF\n        gdal.Translate(output_path, ds)\n        ds = None\n        break  # Exit the loop after converting the target subdataset\n\n  hdf_ds = None\n\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n  file_lst = list()\n  for file in os.listdir(folder_path):\n    file_lst.append(file)\n    if file.lower().endswith(\".hdf\"):\n      hdf_file = os.path.join(folder_path, file)\n      convert_hdf_to_geotiff(hdf_file, output_folder)\n      #print(f\"Converted {file} to GeoTIFF\")\n  return file_lst\n\n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['gdal_translate', '-b', '1', '-outsize', '100%', '100%', '-scale', '0', '255', '200', '200', f\"{modis_day_wise}/fsca_template.tif\", output_file]\n    print(\"Running \", gdal_command)\n    subprocess.run(gdal_command)\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    gdal_command = ['gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", gdal_command)\n    subprocess.run(gdal_command)\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", gdal_command)\n    subprocess.run(gdal_command)\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command)\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                          cloud_hosted=True, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\n    \nif __name__ == \"__main__\":\n  download_tiles_and_merge(start_date, end_date)\n    \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c2qa9u",
  "name" : "fsCA_testing",
  "description" : null,
  "code" : "import os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nos.chdir(f\"{homedir}/fsca/\")\n\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MOD_Grid_Snow_500m:NDSI_Snow_Cover\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n            if target_subdataset_name in subdataset[0]:\n                ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n                gdal.Translate(output_path, ds)\n                ds = None\n                break\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            # print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  tif_files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') and julian_date in f]\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    gdal_command = ['/usr/bin/gdal_translate', \n                    '-b', '1', \n                    '-outsize', '100%', '100%', \n                    '-scale', '0', '255', '200', '200', \n                    f\"{modis_day_wise}/fsca_template.tif\", \n                    output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    target_output_tif = f'{modis_day_wise}/{current_date}__snow_cover.tif'\n    \n    if os.path.exists(target_output_tif):\n        file_size_bytes = os.path.getsize(target_output_tif)\n        print(f\"file_size_bytes: {file_size_bytes}\")\n        print(f\"The file {target_output_tif} exists. skip.\")\n    else:\n        print(f\"The file {target_output_tif} does not exist.\")\n        print(\"start to download files from NASA server to local\")\n        earthaccess.login(strategy=\"netrc\")\n        results = earthaccess.search_data(short_name=\"MOD10A1\", \n                                          cloud_hosted=False, \n                                          bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                          temporal=(current_date, current_date))\n        earthaccess.download(results, input_folder)\n        print(\"done with downloading, start to convert HDF to geotiff..\")\n\n        convert_all_hdf_in_folder(input_folder, output_folder)\n        print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n        merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n        print(f\"saved the merged tifs to {target_output_tif}\")\n    #delete_files_in_folder(input_folder)  # cleanup\n    #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['fsca'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'fsca'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  fsca_cols = [col for col in df.columns if col.startswith('fsca')]\n  print(\"fsca_cols are: \", fsca_cols)\n  \n  df['cumulative_fsca'] = df[fsca_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"fsca\", 'cumulative_fsca']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_fsca'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'fsca' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'fsca', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"fsca\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"fsca\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['fsca']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"fsca\": f\"fsca_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"fsca\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"fsca\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_fsca'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['fsca'] > 100):\n      raise ValueError(\"Error: shouldn't have fsca > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"fsca\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_output.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date)\n  \n  add_time_series_columns(start_date, end_date, force=True)\n  \n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "lnrsop",
  "name" : "fsca_py",
  "description" : null,
  "code" : "import requests\nfrom bs4 import BeautifulSoup\nimport os\nimport subprocess\nimport csv\nfrom datetime import datetime, timedelta\n\noutput_csv_file = \"/home/chetana/gridmet_test_run/mod10a1_snow_cover.csv\"\n\n# Function to extract snow cover value at a given lat lon\ndef extract_snow_cover_value(geotiff_path, lon, lat):\n    gdallocationinfo_cmd = [\n        \"gdallocationinfo\",\n        \"-valonly\",\n        geotiff_path,\n        str(lon),\n        str(lat)\n    ]\n    result = subprocess.run(gdallocationinfo_cmd, stdout=subprocess.PIPE, text=True)\n    return result.stdout.strip()\n\n# Define the date range for data extraction\nstart_date = datetime(2018,1, 1)\nend_date = datetime(2018, 1, 3)\n\n# Load the CSV file with latitude and longitude coordinates\ncsv_file_path = \"/home/chetana/gridmet_test_run/station_cell_mapping.csv\"\n\n# CSV file header\ncsv_header = [\"Date\", \"Latitude\", \"Longitude\", \"Snow Cover Value\"]\n\n# Loop through the date range\ncurrent_date = start_date\nwhile current_date <= end_date:\n    extracted_data = list()\n    # URL and reference link with dynamic date\n    date_str = current_date.strftime(\"%Y.%m.%d\")\n    url = f\"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A1.061/{date_str}/\"\n    reference_link = f\"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A1.061/{date_str}/\"\n    \n    # Send an HTTP GET request to the URL\n    response = requests.get(url)\n    \n    # Check if the request was successful (HTTP status code 200)\n    if response.status_code == 200:\n        # Parse the HTML content using BeautifulSoup\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        \n        # Find all <a> tags (links) on the page\n        links = soup.find_all(\"a\")\n        \n        # Filter the links to keep only those with the .hdf extension\n        all_hdf_files = [link.get(\"href\") for link in links if link.get(\"href\").endswith(\".hdf\")]\n        \n        # Define the sinusoidal tiles of interest\n        sinusoidal_tiles = [\"h08v04\", \"h08v05\", \n                            \"h09v04\", \"h09v05\", \n                            \"h10v04\", \"h10v05\", \n                            \"h11v04\", \"h11v05\", \n                            \"h12v04\", \"h12v05\", \n                            \"h13v04\", \"h13v05\", \n                            \"h15v04\", \"h16v03\", \n                            \"h16v04\"]\n        \n        # Filter the HDF files based on sinusoidal tiles\n        filtered_hdf_files = [hdf_file for hdf_file in all_hdf_files if any(tile in hdf_file for tile in sinusoidal_tiles)]\n        \n        # List to store the paths of converted GeoTIFF files\n        geotiff_files = []\n        \n        # Loop through the filtered HDF files and download/convert/delete them\n        for hdf_file in filtered_hdf_files:\n            # Construct the complete URL for the HDF file\n            hdf_url = reference_link + hdf_file\n            \n            # Define the local filename to save the HDF file\n            local_hdf_filename = hdf_file\n            \n            # Send an HTTP GET request to download the HDF file\n            hdf_response = requests.get(hdf_url)\n            \n            # Check if the download was successful (HTTP status code 200)\n            if hdf_response.status_code == 200:\n                with open(local_hdf_filename, \"wb\") as f:\n                    f.write(hdf_response.content)\n                print(f\"Downloaded {local_hdf_filename}\")\n                \n                # Construct the output GeoTIFF file path and name in the same directory\n                local_geotiff_filename = os.path.splitext(local_hdf_filename)[0] + \".tif\"\n                \n                # Run the gdal_translate command to convert HDF to GeoTIFF\n                gdal_translate_cmd = [\n                    \"gdal_translate\",\n                    \"-of\", \"GTiff\",\n                    f\"HDF4_EOS:EOS_GRID:{local_hdf_filename}:MOD_Grid_Snow_500m:NDSI_Snow_Cover\",\n                    local_geotiff_filename\n                ]\n                \n                # Execute the gdal_translate command\n                subprocess.run(gdal_translate_cmd)\n                \n                # Append the path of the converted GeoTIFF to the list\n                geotiff_files.append(local_geotiff_filename)\n                \n                # Delete the original HDF file\n#                 os.remove(local_hdf_filename)\n                \n                #print(f\"Converted and deleted: {local_hdf_filename}\")\n            else:\n              pass\n                #print(f\"Failed to download {local_hdf_filename}\")\n        \n        # Merge all the GeoTIFF files into a single GeoTIFF\n        merged_geotiff = \"merged_geotiff.tif\"\n        gdal_merge_cmd = [\n            \"gdal_merge.py\",\n            \"-o\", merged_geotiff,\n            \"-of\", \"GTiff\"\n        ] + geotiff_files\n        \n        subprocess.run(gdal_merge_cmd)\n        \n        print(f\"Merged all GeoTIFF files into {merged_geotiff}\")\n        \n        # Loop through the CSV file with latitude and longitude coordinates\n        with open(csv_file_path, \"r\") as csv_file:\n            csv_reader = csv.reader(csv_file)\n            next(csv_reader)  # Skip the header row\n            \n            for row in csv_reader:\n                lat = float(row[3])  # Assuming latitude is in the first column\n                lon = float(row[4])  # Assuming longitude is in the second column\n                \n                # Extract snow cover value\n                snow_cover_value = extract_snow_cover_value(merged_geotiff, lon, lat)\n                \n                # Append the extracted data to the list\n                extracted_data.append([date_str, lat, lon, snow_cover_value])\n        \n        # Delete the merged GeoTIFF file\n#         os.remove(merged_geotiff)\n        print(f\"Deleted {merged_geotiff}\")\n        \n        # Delete individual GeoTIFF files after a successful merge\n        #for geotiff_file in geotiff_files:\n        #    try:\n        #        os.remove(geotiff_file)\n        #        print(f\"Deleted {geotiff_file}\")\n        #    except Exception as e:\n        #        pass\n        \n        # Append the extracted data to the CSV file after each date\n        with open(output_csv_file, \"a\", newline=\"\") as csv_file:\n            csv_writer = csv.writer(csv_file)\n            csv_writer.writerows(extracted_data)\n        \n        print(f\"Extracted data appended to {output_csv_file}\")\n    \n    else:\n        print(\"Failed to fetch the HTML content.\")\n    \n    # Move to the next date\n    current_date += timedelta(days=1)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c8isgf",
  "name" : "fSCA_training_extract_data",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nimport rasterio\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport concurrent.futures\nfrom snowcast_utils import homedir, work_dir, train_start_date, train_end_date\nfrom datetime import datetime, timedelta\nimport dask.dataframe as dd\nimport numpy as np\n\nworking_dir = f\"{homedir}/fsca\"\nfolder_path = f\"{working_dir}/final_output/\"\nnew_base_station_list_file = f\"{work_dir}/all_snotel_cdec_stations_active_in_westus.csv\"\ncell_to_modis_mapping = f\"{working_dir}/training_cell_to_modis_mapper_original_snotel_stations.csv\"\nnon_station_random_points_file = f\"{work_dir}/non_station_random_points_in_westus.csv\"\nonly_active_ghcd_station_in_west_conus_file = f\"{working_dir}/active_ghcnd_station_only_list.csv\"\nghcd_station_to_modis_mapper_file = f\"{working_dir}/active_ghcnd_mapper_modis.csv\"\nall_training_points_with_snotel_ghcnd_file = f\"{work_dir}/all_training_points_snotel_ghcnd_in_westus.csv\"\nmodis_day_wise = f\"{working_dir}/final_output/\"\nos.makedirs(modis_day_wise, exist_ok=True)\n\n\ndef map_modis_to_station(row, src):\n  drow, dcol = src.index(row[\"lon\"], row[\"lat\"])\n  return drow, dcol\n\n\ndef generate_random_non_station_points():\n  # Load the GeoTIFF file\n  sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n  print(f\"loading geotiff {sample_modis_tif}\")\n  with rasterio.open(sample_modis_tif) as src:\n    # Get the raster metadata\n    bounds = src.bounds\n    transform = src.transform\n    width = src.width\n    height = src.height\n\n    # Read the raster values as a numpy array\n    raster_array = src.read(1)  # Assuming it's a single-band raster\n\n    # Generate random points\n    random_points = []\n    while len(random_points) < 4000:\n      # Generate random coordinates within the bounds of the raster\n      random_x = np.random.uniform(bounds.left, bounds.right)\n      random_y = np.random.uniform(bounds.bottom, bounds.top)\n\n      # Convert random coordinates to pixel coordinates\n      col, row = ~transform * (random_x, random_y)\n\n      # Ensure the generated pixel coordinates are within the raster bounds\n      if 0 <= row < height and 0 <= col < width:\n        # Get the value at the generated pixel coordinates\n        value = raster_array[int(row), int(col)]\n\n        # Check if the value is not 239\n        if value != 239 and value != 255:\n          # Append the coordinates to the list\n          random_points.append((random_x, random_y, col, row))\n\n    # Assuming random_points is a list of tuples where each tuple contains latitude and longitude\n    random_points = [(lat, lon, col, row) for lon, lat, col, row in random_points]  # Swap the order to (latitude, longitude)\n\n    # Create a DataFrame from the random_points list\n    random_points_df = pd.DataFrame(random_points, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n\n    # Save the DataFrame to a CSV file\n    random_points_df.to_csv(non_station_random_points_file, index=False)\n    print(f\"random points are saved to {non_station_random_points_file}\")\n\n    \n\n\ndef prepare_modis_grid_mapper_training():\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(cell_to_modis_mapping):\n    print(f\"The file {cell_to_modis_mapping} exists. skip.\")\n  else:\n    print(f\"start to generate {cell_to_modis_mapping}\")\n    station_df = pd.read_csv(new_base_station_list_file)\n    print(\"original station_df describe() = \", station_df.describe())\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      #station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n      print(\"Spatial Extent (Bounding Box):\", src.bounds)\n      # Get the affine transformation matrix\n      transform = src.transform\n\n      # Extract the spatial extent using the affine transformation\n      left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n      # Print the spatial extent\n      print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n      \n      station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n        src.transform, \n        station_df[\"longitude\"], \n        station_df[\"latitude\"])\n      \n      # print(f\"Saving mapper csv file: {cell_to_modis_mapping}\")\n      station_df.to_csv(cell_to_modis_mapping, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n      \n      print(\"after mapped modis station_df.describe() = \", station_df.describe())\n\ndef merge_station_and_non_station_to_one_csv():\n  print(f\"new_base_station_list_file = {new_base_station_list_file}\")\n  print(f\"cell_to_modis_mapping = {cell_to_modis_mapping}\")\n  print(f\"non_station_random_points_file = {non_station_random_points_file}\")\n  df1 = pd.read_csv(cell_to_modis_mapping)\n  df2 = pd.read_csv(non_station_random_points_file)\n  combined_df = pd.concat([df1, df2], ignore_index=True)\n  combined_df.to_csv(all_training_points_with_station_and_non_station_file, index=False)\n\n  print(f\"Combined CSV saved to {all_training_points_with_station_and_non_station_file}\")\n\ndef merge_snotel_ghcnd_station_to_one_csv():\n  print(f\"new_base_station_list_file = {new_base_station_list_file}\")\n  print(f\"cell_to_modis_mapping = {cell_to_modis_mapping}\")\n  print(f\"ghcnd_to_modis_mapping = {ghcd_station_to_modis_mapper_file}\")\n  df1 = pd.read_csv(cell_to_modis_mapping)\n  df2 = pd.read_csv(ghcd_station_to_modis_mapper_file)\n  combined_df = pd.concat([df1, df2], ignore_index=True)\n  combined_df.to_csv(all_training_points_with_snotel_ghcnd_file, index=False)\n\n  print(f\"Combined CSV saved to {all_training_points_with_snotel_ghcnd_file}\")\n\ndef prepare_ghcnd_station_mapping_training():\n  if os.path.exists(ghcd_station_to_modis_mapper_file):\n    print(f\"The file {ghcd_station_to_modis_mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {ghcd_station_to_modis_mapper_file}\")\n    station_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n    station_df = station_df.rename(columns={'Latitude': 'latitude', \n                                            'Longitude': 'longitude'})\n    print(\"original station_df describe() = \", station_df.describe())\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01__snow_cover.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      #station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n      print(\"Spatial Extent (Bounding Box):\", src.bounds)\n      # Get the affine transformation matrix\n      transform = src.transform\n\n      # Extract the spatial extent using the affine transformation\n      left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n      # Print the spatial extent\n      print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n      \n      station_df['modis_y'], station_df['modis_x'] = rasterio.transform.rowcol(\n        src.transform, \n        station_df[\"longitude\"],\n        station_df[\"latitude\"])\n      \n      # print(f\"Saving mapper csv file: {cell_to_modis_mapping}\")\n      station_df.to_csv(ghcd_station_to_modis_mapper_file, index=False, columns=['latitude', 'longitude', 'modis_x', 'modis_y'])\n      print(f\"the new mapper to the ghcnd is saved to {ghcd_station_to_modis_mapper_file}\")\n      print(\"after mapped modis station_df.describe() = \", station_df.describe())\n  \ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n  if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n    return None\n  row, col = src.index(lat, lon)\n  if (0 <= row < src.height) and (0 <= col < src.width):\n    return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n  else:\n    return None\n\ndef get_band_value(row, src):\n  if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n    # print(\"src.height = \", src.height, \" - \", row[\"modis_y\"])\n    # print(\"src.width = \", src.width, \" - \", row[\"modis_x\"])\n    # print(row)\n    valid_value =  src.read(1, \n                            window=((int(row[\"modis_y\"]),\n                                     int(row[\"modis_y\"])+1), \n                                    (int(row[\"modis_x\"]),\n                                     int(row[\"modis_x\"])+1)))\n    # print(\"valid_value[0,0] = \", valid_value[0,0])\n    return valid_value[0,0]\n  else:\n    return None\n          \ndef process_file(file_path, current_date_str, outfile):\n  print(f\"processing {file_path}\")\n  station_df = pd.read_csv(all_training_points_with_snotel_ghcnd_file)\n  # print(\"station_df.head() = \", station_df.head())\n\n  # Apply get_band_value for each row in the DataFrame\n  with rasterio.open(file_path) as src:\n    # Apply get_band_value for each row in the DataFrame\n    # Get the affine transformation matrix\n    transform = src.transform\n\n    # Extract the spatial extent using the affine transformation\n    left, bottom, right, top = rasterio.transform.array_bounds(src.height, src.width, transform)\n\n    # Print the spatial extent\n    # print(\"Spatial Extent (Bounding Box):\", (left, bottom, right, top))\n\n    station_df['fsca'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    \n  # Prepare final data\n  station_df['date'] = current_date_str\n  station_df.to_csv(outfile, index=False, \n                    columns=['date', 'latitude', 'longitude', 'fsca'])\n  print(f\"Saved to csv: {outfile}\")\n\ndef merge_csv(start_date, end_date):\n  import glob\n  # Find CSV files within the specified date range\n  csv_files = glob.glob(folder_path + '*_training_output_station_corrected.csv')\n  relevant_csv_files = []\n\n  for c in csv_files:\n    # Extract the date from the file name\n    # print(\"c = \", c)\n    file_name = os.path.basename(c)\n    date_str = file_name.split('_')[0]  # Assuming the date is part of the file name\n    # print(\"date_str = \", date_str)\n    file_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n\n    # Check if the file date is within the specified range\n    if start_date <= file_date <= end_date:\n      relevant_csv_files.append(c)\n#       # Read and concatenate only relevant CSV files\n#       df = []\n#       for c in relevant_csv_files:\n#         tmp = pd.read_csv(c, low_memory=False, usecols=['date', 'latitude', 'longitude', 'fsca'])\n#         df.append(tmp)\n\n#         combined_df = pd.concat(df, ignore_index=True)\n\n  # Initialize a Dask DataFrame\n  print(\"start to use dask to read all csv files\")\n  dask_df = dd.read_csv(relevant_csv_files)\n\n  # Save the merged DataFrame to a CSV file\n  output_file = f'{working_dir}/fsca_final_training_all.csv'\n  # Write the Dask DataFrame to a single CSV file\n  print(f\"saving all csvs into one file: {output_file}\")\n  dask_df.to_csv(output_file, index=False, single_file=True)\n  #combined_df.to_csv(output_file, index=False)\n\n  #print(combined_df.describe())\n  print(f\"Merged data saved to {output_file}\")\n  \ndef main():\n  \n  start_date = datetime.strptime(train_start_date, \"%Y-%m-%d\")\n  \n  end_date = datetime.strptime(train_end_date, \"%Y-%m-%d\")\n  \n  prepare_modis_grid_mapper_training()\n  prepare_ghcnd_station_mapping_training()\n  # running this function will generate a new set of random points\n  # generate_random_non_station_points()\n  #merge_station_and_non_station_to_one_csv()\n  merge_snotel_ghcnd_station_to_one_csv()\n  \n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  for i in date_list:\n    current_date = i.strftime(\"%Y-%m-%d\")\n    #print(f\"extracting data for {current_date}\")\n    outfile = os.path.join(modis_day_wise, f'{current_date}_training_output_station_with_ghcnd.csv')\n    if os.path.exists(outfile):\n      print(f\"The file {outfile} exists. skip.\")\n      pass\n    else:\n      process_file(f'{modis_day_wise}/{current_date}__snow_cover.tif', current_date, outfile)\n  \n  merge_csv(start_date, end_date)\n\nif __name__ == \"__main__\":\n  main()\n  print(\"fsca Data extraction complete.\")\n  \n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "16qpco",
  "name" : "data_ghcnd_station_only",
  "description" : null,
  "code" : "\"\"\"\nThis process need to retrieve the snow depth ground station data from GHCNd.\n\n\"\"\"\n\nimport pandas as pd\nimport requests\nimport re\nfrom io import StringIO\nfrom snowcast_utils import work_dir, southwest_lat, southwest_lon, northeast_lat, northeast_lon, train_start_date, train_end_date\nimport dask\nimport dask.dataframe as dd\n\nworking_dir = work_dir\nall_ghcd_station_file = f\"{working_dir}/all_ghcn_station_list.csv\"\nonly_active_ghcd_station_in_west_conus_file = f\"{working_dir}/active_station_only_list.csv\"\nsnowdepth_csv_file = f'{only_active_ghcd_station_in_west_conus_file}_all_vars.csv'\nmask_non_snow_days_ghcd_csv_file =  f'{only_active_ghcd_station_in_west_conus_file}_all_vars_masked_non_snow.csv'\n\ndef download_convert_and_read():\n  \n    url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/ghcnd-inventory.txt\"\n    # Download the text file from the URL\n    response = requests.get(url)\n    if response.status_code != 200:\n        print(\"Error: Failed to download the file.\")\n        return None\n    \n    # Parse the text content into columns using regex\n    pattern = r\"(\\S+)\\s+\"\n    parsed_data = re.findall(pattern, response.text)\n    print(\"len(parsed_data) = \", len(parsed_data))\n    \n    # Create a new list to store the rows\n    rows = []\n    for i in range(0, len(parsed_data), 6):\n        rows.append(parsed_data[i:i+6])\n    \n    print(\"rows[0:20] = \", rows[0:20])\n    # Convert the rows into a CSV-like format\n    csv_data = \"\\n\".join([\",\".join(row) for row in rows])\n    \n    # Save the CSV-like string to a file\n    with open(all_ghcd_station_file, \"w\") as file:\n        file.write(csv_data)\n    \n    # Read the CSV-like data into a pandas DataFrame\n    column_names = ['Station', 'Latitude', 'Longitude', 'Variable', 'Year_Start', 'Year_End']\n    df = pd.read_csv(all_ghcd_station_file, header=None, names=column_names)\n    print(df.head())\n    \n    # Remove rows where the last column is not equal to \"2024\"\n    df = df[(df['Year_End'] == 2024) & (df['Variable']=='SNWD')]\n    print(\"Removed non-active stations: \", df.head())\n    \n    # Filter rows within the latitude and longitude ranges\n    df = df[\n      (df['Latitude'] >= southwest_lat) & (df['Latitude'] <= northeast_lat) &\n      (df['Longitude'] >= southwest_lon) & (df['Longitude'] <= northeast_lon)\n    ]\n    \n    df.to_csv(only_active_ghcd_station_in_west_conus_file, index=False)\n    print(f\"saved to {only_active_ghcd_station_in_west_conus_file}\")\n    \n    \n    return df\n\n  \ndef get_snow_depth_observations_from_ghcn():\n    \n    new_base_df = pd.read_csv(only_active_ghcd_station_in_west_conus_file)\n    print(new_base_df.shape)\n    \n    start_date = train_start_date\n    end_date = train_end_date\n\t\n    # Create an empty Pandas DataFrame with the desired columns\n    result_df = pd.DataFrame(columns=[\n      'station_name', \n      'date', \n      'lat', \n      'lon', \n      'snow_depth',\n    ])\n    \n    train_start_date_obj = pd.to_datetime(train_start_date)\n    train_end_date_obj = pd.to_datetime(train_end_date)\n\n    # Function to process each station\n    @dask.delayed\n    def process_station(station):\n        station_name = station['Station']\n        print(f\"retrieving for {station_name}\")\n        station_lat = station['Latitude']\n        station_long = station['Longitude']\n        try:\n          url = f\"https://www.ncei.noaa.gov/data/global-historical-climatology-network-daily/access/{station_name}.csv\"\n          response = requests.get(url)\n          df = pd.read_csv(StringIO(response.text))\n          #\"STATION\",\"DATE\",\"LATITUDE\",\"LONGITUDE\",\"ELEVATION\",\"NAME\",\"PRCP\",\"PRCP_ATTRIBUTES\",\"SNOW\",\"SNOW_ATTRIBUTES\",\"SNWD\",\"SNWD_ATTRIBUTES\",\"TMAX\",\"TMAX_ATTRIBUTES\",\"TMIN\",\"TMIN_ATTRIBUTES\",\"PGTM\",\"PGTM_ATTRIBUTES\",\"WDFG\",\"WDFG_ATTRIBUTES\",\"WSFG\",\"WSFG_ATTRIBUTES\",\"WT03\",\"WT03_ATTRIBUTES\",\"WT08\",\"WT08_ATTRIBUTES\",\"WT16\",\"WT16_ATTRIBUTES\"\n          columns_to_keep = ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'SNWD']\n          df = df[columns_to_keep]\n          # Convert the date column to datetime objects\n          df['DATE'] = pd.to_datetime(df['DATE'])\n          # Filter rows based on the training period\n          df = df[(df['DATE'] >= train_start_date_obj) & (df['DATE'] <= train_end_date_obj)]\n          # print(df.head())\n          return df\n        except Exception as e:\n          print(\"An error occurred:\", e)\n\n    # List of delayed computations for each station\n    delayed_results = [process_station(row) for _, row in new_base_df.iterrows()]\n\n    # Compute the delayed results\n    result_lists = dask.compute(*delayed_results)\n\n    # Concatenate the lists into a Pandas DataFrame\n    result_df = pd.concat(result_lists, ignore_index=True)\n\n    # Print the final Pandas DataFrame\n    print(result_df.head())\n\n    # Save the DataFrame to a CSV file\n    result_df.to_csv(snowdepth_csv_file, index=False)\n    print(f\"All the data are saved to {snowdepth_csv_file}\")\n#     result_df.to_csv(csv_file, index=False)\n\ndef mask_out_all_non_zero_snowdepth_days():\n    print(f\"reading {snowdepth_csv_file}\")\n    df = pd.read_csv(snowdepth_csv_file)\n    # Create the new column 'swe_value' and assign values based on conditions\n    df['swe_value'] = 0  # Initialize all values to 0\n\n    # Assign NaN to 'swe_value' where 'snow_depth' is non-zero\n    df.loc[df['SNWD'] != 0, 'swe_value'] = -999\n\n    # Display the first few rows of the DataFrame\n    print(df.head())\n    df.to_csv(mask_non_snow_days_ghcd_csv_file, index=False)\n    print(f\"The masked non snow var file is saved to {mask_non_snow_days_ghcd_csv_file}\")\n\nif __name__ == \"__main__\":\n    #download_convert_and_read()\n    #get_snow_depth_observations_from_ghcn()\n    mask_out_all_non_zero_snowdepth_days()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "1xdwd6",
  "name" : "mod_water_mask",
  "description" : null,
  "code" : "\n# retrieve the MOD44Wv061 and cut a water mask for the current day.\n\nimport os\nimport subprocess\nimport threading\nfrom datetime import datetime\nfrom datetime import timedelta\nfrom pyproj import Transformer\nfrom rasterio.enums import Resampling\nimport numpy as np\n\nimport requests\nimport earthaccess\nfrom osgeo import gdal\nfrom snowcast_utils import work_dir, homedir, test_start_date, date_to_julian\nimport pandas as pd\nimport rasterio\nimport shutil\nimport time\nfrom convert_results_to_images import plot_all_variables_in_one_csv\nimport dask\nfrom dask import delayed\nimport dask.multiprocessing\nimport pyproj\n\n# change directory before running the code\nhomedir = f\"{homedir}/water_mask/\"\nos.makedirs(homedir, exist_ok=True)\nos.chdir(homedir)\n\n\ntile_list = [\"h08v04\", \"h08v05\", \"h09v04\", \"h09v05\", \n             \"h10v04\", \"h10v05\", \"h11v04\", \"h11v05\", \n             \"h12v04\", \"h12v05\", \"h13v04\", \"h13v05\", \n             \"h15v04\", \"h16v03\", \"h16v04\", ]\ninput_folder = os.getcwd() + \"/temp/\"\nos.makedirs(input_folder, exist_ok=True)\noutput_folder = os.getcwd() + \"/output_folder/\"\nmodis_day_wise = os.getcwd() + \"/final_output/\"\nos.makedirs(output_folder, exist_ok=True)\nos.makedirs(modis_day_wise, exist_ok=True)\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\nmapper_file = os.path.join(modis_day_wise, f'modis_to_dem_mapper.csv')\n\n\n@dask.delayed\ndef convert_hdf_to_geotiff(hdf_file, output_folder):\n    hdf_ds = gdal.Open(hdf_file, gdal.GA_ReadOnly)\n\n    target_subdataset_name = \"MCD12Q1:LC_Prop3\"\n    output_file_name = os.path.splitext(os.path.basename(hdf_file))[0] + \".tif\"\n    output_path = os.path.join(output_folder, output_file_name)\n\n    if os.path.exists(output_path):\n        return f\"The file {output_path} exists. skip.\"\n    else:\n        for subdataset in hdf_ds.GetSubDatasets():\n          print(\"subdataset = \", subdataset)\n          if target_subdataset_name in subdataset[0]:\n              ds = gdal.Open(subdataset[0], gdal.GA_ReadOnly)\n              if \"Assessment\" in subdataset[0]:\n                gdal.Translate(f\"{output_path}_assessment.tif\", ds)\n                print(f\"The layer {target_subdataset_name}_assessment.tif is extracted to {output_path}\")\n              else:\n                gdal.Translate(output_path, ds)\n                print(f\"The layer {target_subdataset_name} is extracted to {output_path}\")\n              ds = None\n\n    hdf_ds = None\n    return f\"Converted {os.path.basename(hdf_file)} to GeoTIFF\"\n\ndef convert_all_hdf_in_folder(folder_path, output_folder):\n    file_list = []\n    delayed_tasks = []\n\n    for file in os.listdir(folder_path):\n        output_file_name = os.path.splitext(os.path.basename(file))[0] + \".tif\"\n        output_path = os.path.join(output_folder, output_file_name)\n        if os.path.exists(output_path):\n            print(f\"The file {output_path} exists. skip.\")\n            continue\n        else:\n            file_list.append(file)\n            if file.lower().endswith(\".hdf\"):\n                hdf_file = os.path.join(folder_path, file)\n                task = convert_hdf_to_geotiff(hdf_file, output_folder)\n                delayed_tasks.append(task)\n\n    results = dask.compute(*delayed_tasks, scheduler=\"processes\")\n\n    return file_list, results\n\ndef get_env_var_for_gdalwarp():\n    if \"PROJ_LIB\" in os.environ:\n        os.environ.pop(\"PROJ_LIB\")\n        print(f\"Environment variable PROJ_LIB removed.\")\n    if \"GDAL_DATA\" in os.environ:\n        os.environ.pop(\"GDAL_DATA\")\n        print(f\"Environment variable GDAL_DATA removed.\")\n    return os.environ\n  \n\ndef merge_tifs(folder_path, target_date, output_file):\n  julian_date = date_to_julian(target_date)\n  print(\"target julian date\", julian_date)\n  print(\"target_date[:4] = \", target_date[:4])\n  tif_files = [ os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.tif') \n    and \"assessment\" not in f and target_date[:4] in f]\n  print(\"tif_files = \", tif_files)\n  if len(tif_files) == 0:\n    print(f\"uh-oh, didn't find HDFs for date {target_date}\")\n    print(\"generate a new csv file with empty values for each point\")\n    # I dont have the mask template tif\n    # gdal_command = ['/usr/bin/gdal_translate', \n    #                 '-b', '1', \n    #                 '-outsize', '100%', '100%', \n    #                 '-scale', '0', '255', '200', '200', \n    #                 f\"{modis_day_wise}/water_mask_template.tif\", \n    #                 output_file]\n    # print(\"Running \", \" \".join(gdal_command))\n    # subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    #raise ValueError(f\"uh-oh, didn't find HDFs for date {target_date}\")\n  else:\n    # gdal_command = ['gdal_merge.py', '-o', output_file, '-of', 'GTiff', '-r', 'cubic'] + tif_files\n    #if 'PROJ_LIB' in os.environ:\n    #    del os.environ['PROJ_LIB']\n    print(\"pyproj.datadir.get_data_dir() = \", pyproj.datadir.get_data_dir())\n    gdal_command = ['/usr/bin/gdalwarp', '-r', 'min', ] + tif_files + [f\"{output_file}_500m.tif\"]\n    print(\"Running \", ' '.join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n    # gdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    gdal_command = ['/usr/bin/gdalwarp', '-t_srs', 'EPSG:4326', '-tr', '0.036', '0.036', '-cutline', f'{work_dir}/template.shp', '-crop_to_cutline', '-overwrite', f\"{output_file}_500m.tif\", output_file]\n    print(\"Running \", \" \".join(gdal_command))\n    subprocess.run(gdal_command, env=get_env_var_for_gdalwarp())\n\n\ndef list_files(directory):\n  return [os.path.abspath(os.path.join(directory, f)) for f in os.listdir(directory) if\n          os.path.isfile(os.path.join(directory, f))]\n\n\ndef merge_tiles(date, hdf_files):\n  path = f\"data/{date}/\"\n  files = list_files(path)\n  print(files)\n  merged_filename = f\"data/{date}/merged.tif\"\n  merge_command = [\"/usr/bin/gdal_merge.py\", \"-o\", merged_filename, \"-of\", \"GTiff\"] + files\n  try:\n    subprocess.run(merge_command, env=get_env_var_for_gdalwarp())\n    print(f\"Merged tiles into {merged_filename}\")\n  except subprocess.CalledProcessError as e:\n    print(f\"Error merging tiles: {e}\")\n\n\ndef download_url(date, url):\n  file_name = url.split('/')[-1]\n  if os.path.exists(f'data/{date}/{file_name}'):\n    print(f'File: {file_name} already exists, SKIPPING')\n    return\n  try:\n    os.makedirs('data/', exist_ok=True)\n    os.makedirs(f'data/{date}', exist_ok=True)\n    response = requests.get(url, stream=True)\n    with open(f'data/{date}/{file_name}', 'wb') as f:\n      for chunk in response.iter_content(chunk_size=8192):\n        if chunk:\n          f.write(chunk)\n\n    print(f\"Downloaded {file_name}\")\n  except Exception as e:\n    print(f\"Error downloading {url}: {e}\")\n\n\ndef download_all(date, urls):\n  threads = []\n\n  for url in urls:\n    thread = threading.Thread(target=download_url, args=(date, url,))\n    thread.start()\n    threads.append(thread)\n\n  for thread in threads:\n    thread.join()\n\n\ndef delete_files_in_folder(folder_path):\n  if not os.path.exists(folder_path):\n    print(\"Folder does not exist.\")\n    return\n\n  for filename in os.listdir(folder_path):\n    file_path = os.path.join(folder_path, filename)\n    try:\n      if os.path.isfile(file_path) or os.path.islink(file_path):\n        os.unlink(file_path)\n      else:\n        print(f\"Skipping {filename}, as it is not a file.\")\n    except Exception as e:\n      print(f\"Failed to delete {file_path}. Reason: {e}\")\n\n\ndef download_tiles_and_merge(start_date, end_date):\n  date_list = [start_date + timedelta(days=i) for i in range((end_date - start_date).days + 1)]\n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  # Subtract one year from the current date\n  one_year_ago = start_date - timedelta(days=365)\n  target_output_tif = f'{modis_day_wise}/{start_date.year}__water_mask.tif'\n  \n  if os.path.exists(target_output_tif):\n      file_size_bytes = os.path.getsize(target_output_tif)\n      print(f\"file_size_bytes: {file_size_bytes}\")\n      print(f\"The file {target_output_tif} exists. skip.\")\n  else:\n      print(f\"The file {target_output_tif} does not exist.\")\n      print(\"start to download files from NASA server to local\")\n      earthaccess.login(strategy=\"netrc\")\n      results = earthaccess.search_data(short_name=\"MCD12Q1\",\n                                        cloud_hosted=True, \n                                        bounding_box=(-124.77, 24.52, -66.95, 49.38),\n                                        temporal=(\n                                          one_year_ago.strftime(\"%Y-%m-%d\"), \n                                          start_date.strftime(\"%Y-%m-%d\")))\n      earthaccess.download(results, input_folder)\n      print(\"done with downloading, start to convert HDF to geotiff..\")\n\n      convert_all_hdf_in_folder(input_folder, output_folder)\n      print(\"done with conversion, start to merge geotiff tiles to one tif per day..\")\n\n      merge_tifs(folder_path=output_folder, target_date = current_date, output_file=target_output_tif)\n      print(f\"saved the merged tifs to {target_output_tif}\")\n  #delete_files_in_folder(input_folder)  # cleanup\n  #delete_files_in_folder(output_folder)  # cleanup\n\ndef get_value_at_coords(src, lat, lon, band_number=1):\n#     transformer = Transformer.from_crs(\"EPSG:4326\", src.crs, always_xy=True)\n#     east, north = transformer.transform(lon, lat)\n    if not (src.bounds.left <= lat <= src.bounds.right and src.bounds.bottom <= lon <= src.bounds.top):\n      return None\n    row, col = src.index(lat, lon)\n    if (0 <= row < src.height) and (0 <= col < src.width):\n      return src.read(band_number, window=((row, row+1), (col, col+1)), resampling=Resampling.nearest)[0, 0]\n    else:\n      return None\n\ndef get_band_value(row, src):\n    #row, col = src.index(row[\"Latitude\"], row[\"Longitude\"])\n    #print(row, col, src.height, src.width)\n    if (row[\"modis_y\"] < src.height) and (row[\"modis_x\"] < src.width):\n      valid_value =  src.read(1, \n                              window=((row[\"modis_y\"], \n                                       row[\"modis_y\"]+1), \n                                      (row[\"modis_x\"], \n                                       row[\"modis_x\"]+1))\n                             )\n      #print(\"extracted value array: \", valid_value)\n      #print(\"Found a valid value: \",row, valid_value, src.height, src.width)\n      return valid_value[0,0]\n    else:\n      return None\n          \ndef process_file(file_path, current_date):\n    \"\"\"\n    Process a raster file, extract values for specific coordinates, and save the result in a CSV file.\n\n    Parameters:\n    - file_path (str): Path to the raster file to be processed.\n    - current_date (str): Current date to be associated with the processed data.\n\n    Returns:\n    - str: Path to the saved CSV file containing the processed data.\n    \"\"\"\n\n    # Read the station DataFrame from a mapper file (assuming `mapper_file` is defined elsewhere)\n    station_df = pd.read_csv(mapper_file)\n    print(f\"Opening {file_path}\")\n\n    # Open the raster file using rasterio\n    with rasterio.open(file_path) as src:\n        # Apply get_band_value for each row in the DataFrame\n        station_df['lc_prop3'] = station_df.apply(get_band_value, axis=1, args=(src,))\n\n    # Filter out None values\n    valid_data = station_df[station_df['lc_prop3'].notna()]\n\n    # Prepare final data\n    valid_data['date'] = current_date\n    output_file = os.path.join(modis_day_wise, f'{current_date[:4]}_output.csv')\n    print(f\"Saving CSV file: {output_file}\")\n    valid_data.to_csv(output_file, index=False, columns=['date', 'Latitude', 'Longitude', 'lc_prop3'])\n    \n    return output_file\n\n\ndef merge_cumulative_csv(start_date, end_date, force):\n  \n  current_date = start_date\n  target_date = end_date\n  \n  input_time_series_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output_with_time_series.csv\"\n  target_cumulative_file = f\"{modis_day_wise}/{end_date.strftime('%Y-%m-%d')}_output.csv_cumulative.csv\"\n  \n  if os.path.exists(target_cumulative_file) and not force:\n    print(f\"file already exists {target_cumulative_file}\")\n    return\n  \n  df = pd.read_csv(input_time_series_file)\n\n  # add all the columns together and save to new csv\n  # Adding all columns except latitude and longitude\n  df = df.apply(pd.to_numeric, errors='coerce')\n\n  #new_df = new_df.head(2000)\n\n  lc_prop3_cols = [col for col in df.columns if col.startswith('lc_prop3')]\n  print(\"lc_prop3_cols are: \", lc_prop3_cols)\n  \n  df['cumulative_lc_prop3'] = df[lc_prop3_cols].sum(axis=1)\n\n  df = df.loc[:, ['Latitude', 'Longitude', f\"lc_prop3\", 'cumulative_lc_prop3']]\n  df[\"date\"] = end_date\n\n  print(\"new_df final shape: \", df.head())\n  df.to_csv(target_cumulative_file, index=False)\n  print(f\"new df is saved to {target_cumulative_file}\")\n\n  print(df['cumulative_lc_prop3'].describe(include='all'))\n\ndef add_cumulative_column(df, column_name):\n  df[f'cumulative_{column_name}'] = df[column_name].cumsum()\n  return df\n\n# Function to perform polynomial interpolation and fill in missing values\ndef interpolate_missing_and_add_cumulative_inplace(row, column_name, degree=1):\n  \"\"\"\n  Interpolate missing values in a Pandas Series using polynomial interpolation\n  and add a cumulative column.\n\n  Parameters:\n    - row (pd.Series): The input row containing the data to be interpolated.\n    - column_name (str): The name of the column to be interpolated.\n    - degree (int, optional): The degree of the polynomial fit. Default is 1 (linear).\n\n  Returns:\n    - pd.Series: The row with interpolated values and a cumulative column.\n\n  Raises:\n    - ValueError: If there are unexpected null values after interpolation.\n\n  Note:\n    - For 'SWE' column, values above 240 are treated as gaps and set to 240.\n    - For 'lc_prop3' column, values above 100 are treated as gaps and set to 100.\n\n  Examples:\n    ```python\n    # Example usage:\n    interpolated_row = interpolate_missing_and_add_cumulative_inplace(my_row, 'lc_prop3', degree=2)\n    ```\n\n  \"\"\"\n  \n  # Extract X series (column names)\n  x_key = row.index\n  x = np.arange(len(x_key))\n\n  # Extract Y series (values from the first row)\n  y = row\n  \n\n  # Create a mask for missing values\n  if column_name == \"SWE\":\n    mask = (y > 240) | y.isnull()\n  elif column_name == \"lc_prop3\":\n    y = y.replace([225, 237, 239], 0)\n    y[y < 0] = 0\n    mask = (y > 100) | y.isnull()\n  else:\n    mask = y.isnull()\n\n  # Check if all elements in the mask array are True\n  all_true = np.all(mask)\n\n  if all_true or len(np.where(~mask)[0]) == 1:\n    row.values[:] = 0\n  else:\n    # Perform interpolation\n    #new_y = np.interp(x, x[~mask], y[~mask])\n    # Replace missing values with interpolated values\n    #df[column_name] = new_y\n    \n    try:\n      # Coefficients of the polynomial fit\n      coefficients = np.polyfit(x[~mask], y[~mask], deg=degree)\n\n      # Perform polynomial interpolation\n      interpolated_values = np.polyval(coefficients, x)\n\n      # Merge using np.where\n      merged_array = np.where(mask, interpolated_values, y)\n\n      row = pd.Series(merged_array, index=x_key)\n    except Exception as e:\n      # Print the error message and traceback\n      import traceback\n      traceback.print_exc()\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(f\"Error: {e}\")\n      raise e\n      \n    if column_name == \"AMSR_SWE\":\n      row = row.clip(upper=240, lower=0)\n    elif column_name == \"lc_prop3\":\n      row = row.clip(upper=100, lower=0)\n\n    if row.isnull().any():\n      print(\"x:\", x)\n      print(\"y:\", y)\n      print(\"mask:\", mask)\n      print(\"why row still has values > 100\", row)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n\n  \n  # create the cumulative column after interpolation\n  row[f\"cumulative_{column_name}\"] = row.sum()\n  return row\n\n\ndef add_time_series_columns(start_date, end_date, force=True):\n  \"\"\"\n  Converts data from a cleaned CSV file into a time series format.\n\n  This function reads a cleaned CSV file, sorts the data, fills in missing values using polynomial interpolation,\n  and creates a time series dataset for specific columns. The resulting time series data is saved to a new CSV file.\n\n  Parameters:\n    - start_date (str): The start date of the time series data in the format 'YYYY-MM-DD'.\n    - end_date (str): The end date of the time series data in the format 'YYYY-MM-DD'.\n    - force (bool, optional): If True, forces the regeneration of time series data even if the output file exists.\n\n    Returns:\n    - None: The function primarily generates time series data and saves it to a CSV file with side effects.\n\n  Example:\n    ```python\n    add_time_series_columns('2022-01-01', '2022-12-31', force=True)\n    ```\n\n  \"\"\"\n  current_date = start_date\n  target_date = end_date\n  target_date_str = target_date.strftime('%Y-%m-%d')\n  \n  output_csv = f\"{modis_day_wise}/{target_date_str}_output_with_time_series.csv\"\n  print(f\"add_time_series_columns target csv: {output_csv}\")\n        \n  if os.path.exists(output_csv) and not force:\n    print(f\"{output_csv} already exists, skipping..\")\n    return\n  \n  backup_time_series_csv_path = f\"{modis_day_wise}/{target_date_str}_output_with_time_series_backup.csv\"\n  \n  columns_to_be_time_series = ['lc_prop3']\n\n  # Read the all the column merged CSV\n  \n  date_keyed_objects = {}\n  data_dict = {}\n  new_df = None\n  while current_date <= end_date:\n    \n    current_date_str = current_date.strftime('%Y-%m-%d')\n  \n    data_dict[current_date_str] = f\"{modis_day_wise}/{current_date_str}_output.csv\"\n    #print(data_dict[current_date_str])\n    current_df = pd.read_csv(data_dict[current_date_str])\n    current_df.drop([\"date\"], axis=1, inplace=True)\n    \n    if current_date != end_date:\n      current_df.rename(columns={\"lc_prop3\": f\"lc_prop3_{current_date_str}\"}, inplace=True)\n    #print(current_df.head())\n    \n    if new_df is None:\n      new_df = current_df\n    else:\n      new_df = pd.merge(new_df, current_df, on=['Latitude', 'Longitude'])\n      #new_df = new_df.append(current_df, ignore_index=True)\n\n    current_date += timedelta(days=1)\n\n  print(\"new_df.columns = \", new_df.columns)\n  print(new_df.head())\n  \n  df = new_df\n        \n  gap_filled_csv = f\"{output_csv}_gap_filled.csv\"\n  if os.path.exists(gap_filled_csv) and not force:\n    print(f\"{gap_filled_csv} already exists, skipping..\")\n    df = pd.read_csv(gap_filled_csv)\n    print(df[\"lc_prop3\"].describe())\n  else:\n  \n    #df.sort_values(by=['Latitude', 'Longitude'], inplace=True)\n    print(\"All current columns: \", df.columns)\n    \n    \n    print(\"Start to fill in the missing values\")\n    print(\"all the df shape: \", df.shape)\n    #grouped = df.groupby(['Latitude', 'Longitude'])\n    #num_groups = len(grouped.groups)\n    #print(f\"Number of groups: {num_groups}\")\n    filled_data = pd.DataFrame()\n    \n    num_days = 7\n  \n    \n    # Apply the function to each group\n#     no_loc_df = df.drop([\"Latitude\", \"Longitude\"], axis=1)\n#     filled_data = no_loc_df.apply(lambda row: process_group_value_filling(row, num_days, target_date_str ), axis=1)\n#     filled_data[\"Latitude\"] = df[\"Latitude\"]\n#     filled_data[\"Longitude\"] = df[\"Longitude\"]\n    \n    filtered_columns = df.filter(like=\"lc_prop3\")\n    print(filtered_columns.columns)\n    filtered_columns = filtered_columns.mask(filtered_columns > 100)\n    filtered_columns.interpolate(axis=1, method='linear', inplace=True)\n    filtered_columns.fillna(0, inplace=True)\n\n    sum_column = filtered_columns.sum(axis=1)\n    # Define a specific name for the new column\n    df[f'cumulative_lc_prop3'] = sum_column\n    df[filtered_columns.columns] = filtered_columns\n\n    if filtered_columns.isnull().any().any():\n      print(\"filtered_columns :\", filtered_columns)\n      raise ValueError(\"Single group: shouldn't have null values here\")\n    \n    filled_data = df\n    filled_data[\"date\"] = target_date_str\n    #filled_data.fillna(0, inplace=True)\n  \n    if any(filled_data['lc_prop3'] > 100):\n      raise ValueError(\"Error: shouldn't have lc_prop3 > 100 at this point\")\n\n    print(\"Finished correctly \", filled_data.head())\n    filled_data.to_csv(gap_filled_csv, index=False)\n    print(f\"New filled values csv is saved to {output_csv}_gap_filled.csv\")\n    df = filled_data\n  \n  result = df\n  print(result['date'].unique())\n  print(result.shape)\n  print(result[\"lc_prop3\"].describe())\n  result.to_csv(output_csv, index=False)\n  print(f\"New data is saved to {output_csv}\")\n  shutil.copy(output_csv, backup_time_series_csv_path)\n  print(f\"File is backed up to {backup_time_series_csv_path}\")\n  cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  shutil.copy(output_csv, cumulative_file_path)\n  \n#   input_time_series_file = f\"{modis_day_wise}/{test_start_date}_output_with_time_series.csv_gap_filled.csv\"\n#   target_cumulative_file = f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n#   shutil.copy(input_time_series_file, target_cumulative_file)\n  \n\n\ndef map_modis_to_station(row, src):\n#   transformer = Transformer.from_crs(\"EPSG:4326\", \n#                                      src.crs, \n#                                      always_xy=True)\n#   east, north = transformer.transform(row[\"Longitude\"], \n#                                       row[\"Latitude\"])\n  drow, dcol = src.index(row[\"Longitude\"], row[\"Latitude\"])\n  return drow, dcol\n  \n  \ndef prepare_modis_grid_mapper():\n  \"\"\"\n  Prepares a mapper file to map coordinates between station coordinates and MODIS grid coordinates.\n\n    This function performs the following steps:\n    1. Checks if the mapper file already exists. If yes, the function skips the generation process.\n    2. Reads station coordinates from a CSV file (`western_us_coords`) containing 'Longitude' and 'Latitude'.\n    3. Uses a sample MODIS TIFF file (`sample_modis_tif`) to map MODIS grid coordinates ('modis_x' and 'modis_y') to station coordinates.\n    4. Saves the resulting mapper file as a CSV (`mapper_file`) containing columns 'Latitude', 'Longitude', 'modis_x', and 'modis_y'.\n\n    Note: Ensure that necessary functions like `map_modis_to_station` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily generates the mapper file with side effects.\n\n    Example:\n    ```python\n    prepare_modis_grid_mapper()\n    ```\n\n  \"\"\"\n  # actually, not sure this applied for modis. The tile HDF must be exactly same extent to make this work. Otherwise, the mapper won't get usable. \n  \n  if os.path.exists(mapper_file):\n    print(f\"The file {mapper_file} exists. skip.\")\n  else:\n    print(f\"start to generate {mapper_file}\")\n    station_df = pd.read_csv(western_us_coords, low_memory=False, usecols=['Longitude', 'Latitude'])\n\n    sample_modis_tif = f\"{modis_day_wise}/2022-10-01_water_mask.tif\"\n\n    with rasterio.open(sample_modis_tif) as src:\n      # Apply get_band_value for each row in the DataFrame\n      station_df['modis_y'], station_df['modis_x'] = zip(*station_df.apply(map_modis_to_station, axis=1, args=(src,)))\n\n\n      print(f\"Saving mapper csv file: {mapper_file}\")\n      station_df.to_csv(mapper_file, index=False, columns=['Latitude', 'Longitude', 'modis_x', 'modis_y'])\n    \ndef extract_data_for_testing():\n  \"\"\"\n    Extracts and processes MODIS data for testing purposes within a specified date range.\n\n    This function performs the following steps:\n    1. Determines the start and end dates based on the `test_start_date`.\n    2. Prepares the MODIS grid mapper using `prepare_modis_grid_mapper`.\n    3. Downloads, tiles, and merges MODIS data between the determined start and end dates using `download_tiles_and_merge`.\n    4. Iterates through each day in the date range, extracting data and saving it as day-wise CSV files using `process_file`.\n    5. Adds time series columns to the extracted data using `add_time_series_columns`.\n    6. Creates a cumulative CSV file by copying the output file with time series information.\n\n    Note: Ensure that necessary functions like `prepare_modis_grid_mapper`, `download_tiles_and_merge`, \n    `process_file`, and `add_time_series_columns` are defined and available in the same scope.\n\n    Returns:\n    - None: The function primarily performs data extraction and processing with side effects.\n\n    Example:\n    ```python\n    extract_data_for_testing()\n    ```\n\n  \"\"\"\n  print(\"get test_start_date = \", test_start_date)\n  end_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  print(end_date)\n  if end_date.month < 10:\n    past_october_1 = datetime(end_date.year - 1, 10, 1)\n  else:\n    past_october_1 = datetime(end_date.year, 10, 1)\n  \n  start_date = past_october_1\n  print(f\"The start_date of the water year {start_date}\")\n  \n  # prepare_modis_grid_mapper()\n  \n  download_tiles_and_merge(start_date, end_date)\n  \n  \n  current_date = start_date.strftime(\"%Y-%m-%d\")\n  print(f\"extracting data for {current_date}\")\n  outfile = os.path.join(modis_day_wise, f'{start_date.year}_output.csv')\n  if os.path.exists(outfile):\n    print(f\"The file {outfile} exists. skip.\")\n  else:\n    process_file(f'{modis_day_wise}/{start_date.year}__water_mask.tif', current_date)\n  \n  # add_time_series_columns(start_date, end_date, force=True)\n  \n\nif __name__ == \"__main__\":\n  extract_data_for_testing()\n\n  # cumulative_file_path =  f\"{modis_day_wise}/{test_start_date}_output.csv_cumulative.csv\"\n  # plot_all_variables_in_one_csv(cumulative_file_path, f\"{cumulative_file_path}.png\")\n\n  # SnowCover is missing from 10-12 to 10-23\n  #download_tiles_and_merge(datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"), datetime.strptime(\"2022-10-24\", \"%Y-%m-%d\"))\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "uw1w1u",
  "name" : "prepare_water_mask_template",
  "description" : null,
  "code" : "# prepare the template tifs for the water mask from MODIS\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport os\nimport subprocess\nimport csv\nfrom datetime import datetime, timedelta\n\noutput_csv_file = \"/home/chetana/gridmet_test_run/mod44w_water_mask.csv\"\n\n# Function to extract snow cover value at a given lat lon\ndef extract_snow_cover_value(geotiff_path, lon, lat):\n    gdallocationinfo_cmd = [\n        \"gdallocationinfo\",\n        \"-valonly\",\n        geotiff_path,\n        str(lon),\n        str(lat)\n    ]\n    result = subprocess.run(gdallocationinfo_cmd, stdout=subprocess.PIPE, text=True)\n    return result.stdout.strip()\n\ndef generate_template():\n    # Define the date range for data extraction\n    start_date = datetime(2018,1, 1)\n    end_date = datetime(2018, 1, 3)\n\n    # Load the CSV file with latitude and longitude coordinates\n    csv_file_path = \"/home/chetana/gridmet_test_run/station_cell_mapping.csv\"\n\n    # CSV file header\n    csv_header = [\"Date\", \"Latitude\", \"Longitude\", \"Snow Cover Value\"]\n\n    # Loop through the date range\n    current_date = start_date\n    while current_date <= end_date:\n        extracted_data = list()\n        # URL and reference link with dynamic date\n        date_str = current_date.strftime(\"%Y.%m.%d\")\n        url = f\"https://e4ftl01.cr.usgs.gov/MOLT/MOD44W.061/{date_str}/\"\n        reference_link = f\"https://e4ftl01.cr.usgs.gov/MOLT/MOD10A1.061/{date_str}/\"\n        \n        print(\"url = \", url)\n        # Send an HTTP GET request to the URL\n        response = requests.get(url)\n        \n        # Check if the request was successful (HTTP status code 200)\n        if response.status_code == 200:\n            # Parse the HTML content using BeautifulSoup\n            soup = BeautifulSoup(response.text, \"html.parser\")\n            \n            # Find all <a> tags (links) on the page\n            links = soup.find_all(\"a\")\n            \n            # Filter the links to keep only those with the .hdf extension\n            all_hdf_files = [link.get(\"href\") for link in links if link.get(\"href\").endswith(\".hdf\")]\n            \n            # Define the sinusoidal tiles of interest\n            sinusoidal_tiles = [\"h08v04\", \"h08v05\", \n                                \"h09v04\", \"h09v05\", \n                                \"h10v04\", \"h10v05\", \n                                \"h11v04\", \"h11v05\", \n                                \"h12v04\", \"h12v05\", \n                                \"h13v04\", \"h13v05\", \n                                \"h15v04\", \"h16v03\", \n                                \"h16v04\"]\n            \n            # Filter the HDF files based on sinusoidal tiles\n            filtered_hdf_files = [hdf_file for hdf_file in all_hdf_files if any(tile in hdf_file for tile in sinusoidal_tiles)]\n            \n            # List to store the paths of converted GeoTIFF files\n            geotiff_files = []\n            \n            # Loop through the filtered HDF files and download/convert/delete them\n            for hdf_file in filtered_hdf_files:\n                # Construct the complete URL for the HDF file\n                hdf_url = reference_link + hdf_file\n                \n                # Define the local filename to save the HDF file\n                local_hdf_filename = hdf_file\n                \n                # Send an HTTP GET request to download the HDF file\n                print(\"hdf_url = \", hdf_url)\n                hdf_response = requests.get(hdf_url)\n                \n                # Check if the download was successful (HTTP status code 200)\n                if hdf_response.status_code == 200:\n                    with open(local_hdf_filename, \"wb\") as f:\n                        f.write(hdf_response.content)\n                    print(f\"Downloaded {local_hdf_filename}\")\n                    \n                    # Construct the output GeoTIFF file path and name in the same directory\n                    local_geotiff_filename = os.path.splitext(local_hdf_filename)[0] + \".tif\"\n                    \n                    # Run the gdal_translate command to convert HDF to GeoTIFF\n                    gdal_translate_cmd = [\n                        \"gdal_translate\",\n                        \"-of\", \"GTiff\",\n                        f\"HDF4_EOS:EOS_GRID:{local_hdf_filename}:MOD_Grid_Snow_500m:NDSI_Snow_Cover\",\n                        local_geotiff_filename\n                    ]\n                    \n                    # Execute the gdal_translate command\n                    subprocess.run(gdal_translate_cmd)\n                    \n                    # Append the path of the converted GeoTIFF to the list\n                    geotiff_files.append(local_geotiff_filename)\n                    \n                    # Delete the original HDF file\n    #                 os.remove(local_hdf_filename)\n                    \n                    #print(f\"Converted and deleted: {local_hdf_filename}\")\n                else:\n                    pass\n                    #print(f\"Failed to download {local_hdf_filename}\")\n            \n            # Merge all the GeoTIFF files into a single GeoTIFF\n            merged_geotiff = \"merged_geotiff.tif\"\n            gdal_merge_cmd = [\n                \"gdal_merge.py\",\n                \"-o\", merged_geotiff,\n                \"-of\", \"GTiff\"\n            ] + geotiff_files\n            \n            subprocess.run(gdal_merge_cmd)\n            \n            print(f\"Merged all GeoTIFF files into {merged_geotiff}\")\n            \n            # Loop through the CSV file with latitude and longitude coordinates\n            with open(csv_file_path, \"r\") as csv_file:\n                csv_reader = csv.reader(csv_file)\n                next(csv_reader)  # Skip the header row\n                \n                for row in csv_reader:\n                    lat = float(row[3])  # Assuming latitude is in the first column\n                    lon = float(row[4])  # Assuming longitude is in the second column\n                    \n                    # Extract snow cover value\n                    snow_cover_value = extract_snow_cover_value(merged_geotiff, lon, lat)\n                    \n                    # Append the extracted data to the list\n                    extracted_data.append([date_str, lat, lon, snow_cover_value])\n            \n            # Delete the merged GeoTIFF file\n    #         os.remove(merged_geotiff)\n            print(f\"Deleted {merged_geotiff}\")\n            \n            # Delete individual GeoTIFF files after a successful merge\n            #for geotiff_file in geotiff_files:\n            #    try:\n            #        os.remove(geotiff_file)\n            #        print(f\"Deleted {geotiff_file}\")\n            #    except Exception as e:\n            #        pass\n            \n            # Append the extracted data to the CSV file after each date\n            with open(output_csv_file, \"a\", newline=\"\") as csv_file:\n                csv_writer = csv.writer(csv_file)\n                csv_writer.writerows(extracted_data)\n            \n            print(f\"Extracted data appended to {output_csv_file}\")\n        \n        else:\n            print(\"Failed to fetch the HTML content.\")\n        \n        # Move to the next date\n        current_date += timedelta(days=1)\n\n\nif __name__ == \"__main__\":\n  generate_template()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
