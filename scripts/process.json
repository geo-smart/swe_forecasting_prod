[{
  "id" : "78vedq",
  "name" : "data_sentinel2",
  "description" : null,
  "code" : "# Data Preparation for Sentinel 2\n\nprint(\"Not ready yet..Prepare sentinel 2 into .csv\")\n\nprint('test')",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "mxpyqt",
  "name" : "model_creation_lstm",
  "description" : "python",
  "code" : "# Create LSTM model\n\nprint(\"Create LSTM\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rauqsh",
  "name" : "model_creation_ghostnet",
  "description" : "python",
  "code" : "# GhostNet\n\nprint(\"Create GhostNet\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "u7xh2p",
  "name" : "data_integration",
  "description" : null,
  "code" : "\"\"\"\nThis script reads three CSV files into Dask DataFrames, performs data type conversion,\nand merges them into a final Dask DataFrame. The merged data is then saved to a CSV file.\n\nThe three CSV files include climatology data, training-ready SNOTEL data, and training-ready\nterrain data, each with latitude ('lat'), longitude ('lon'), and date ('date') columns.\n\nAttributes:\n    file_path1 (str): File path of the climatology data CSV file.\n    file_path2 (str): File path of the training-ready SNOTEL data CSV file.\n    file_path3 (str): File path of the training-ready terrain data CSV file.\n\nFunctions:\n    small_function: Reads, processes, and merges the CSV files and saves the result to a CSV file.\n\"\"\"\n\nimport dask.dataframe as dd\n\n# Define the file paths of the three CSV files\nfile_path1 = '/home/chetana/gridmet_test_run/climatology_data.csv'\nfile_path2 = '/home/chetana/gridmet_test_run/training_ready_snotel_data.csv'\nfile_path3 = '/home/chetana/gridmet_test_run/training_ready_terrain.csv'\n\ndef small_function():\n    \"\"\"\n    Reads each CSV file into a Dask DataFrame, performs data type conversion for latitude and longitude,\n    merges the DataFrames based on specific columns, and saves the merged Dask DataFrame to a CSV file.\n\n    Args:\n        None\n\n    Returns:\n        None\n    \"\"\"\n    # Read each CSV file into a Dask DataFrame\n    df1 = dd.read_csv(file_path1)\n    df2 = dd.read_csv(file_path2)\n    df3 = dd.read_csv(file_path3)\n\n    # Perform data type conversion for latitude and longitude columns\n    df1['lat'] = df1['lat'].astype(float)\n    df1['lon'] = df1['lon'].astype(float)\n    df2['lat'] = df2['lat'].astype(float)\n    df2['lon'] = df2['lon'].astype(float)\n    df3['lat'] = df3['lat'].astype(float)\n    df3['lon'] = df3['lon'].astype(float)\n\n    # Merge the first two DataFrames based on 'lat', 'lon', and 'date'\n    merged_df1 = dd.merge(df1, df2, left_on=['lat', 'lon', 'date'], right_on=['lat', 'lon', 'Date'])\n\n    # Merge the third DataFrame based on 'lat' and 'lon'\n    merged_df2 = dd.merge(merged_df1, df3, on=['lat', 'lon'])\n\n    # Save the merged Dask DataFrame directly to a CSV file\n    merged_df2.to_csv('/home/chetana/gridmet_test_run/model_training_data.csv', index=False, single_file=True)\n\n# Uncomment the line below to execute the function\n# small_function()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "e8k4wq",
  "name" : "model_train_validate",
  "description" : null,
  "code" : "\"\"\"\nThis script trains and validates machine learning models for hole analysis.\n\nAttributes:\n    RandomForestHole (class): A class for training and using a Random Forest model.\n    XGBoostHole (class): A class for training and using an XGBoost model.\n    ETHole (class): A class for training and using an Extra Trees model.\n\nFunctions:\n    main(): The main function that trains and validates machine learning models for hole analysis.\n\"\"\"\n\nfrom model_creation_rf import RandomForestHole\nfrom model_creation_xgboost import XGBoostHole\nfrom model_creation_et import ETHole\n\ndef main():\n    print(\"Train Models\")\n\n    # Choose the machine learning models to train (e.g., RandomForestHole, XGBoostHole, ETHole)\n    worm_holes = [ETHole()]\n\n    for hole in worm_holes:\n        # Perform preprocessing for the selected model\n        hole.preprocessing()\n        print(hole.train_x.shape)\n        print(hole.train_y.shape)\n        \n        # Train the machine learning model\n        hole.train()\n        \n        # Test the trained model\n        hole.test()\n        \n        # Evaluate the model's performance\n        hole.evaluate()\n        \n        # Save the trained model\n        hole.save()\n\n    print(\"Finished training and validating all the models.\")\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "h1qp9v",
  "name" : "model_predict",
  "description" : null,
  "code" : "import joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, month_to_season\nimport os\n\ndef load_model(model_path):\n    \"\"\"\n    Load a machine learning model from a file.\n\n    Args:\n        model_path (str): Path to the saved model file.\n\n    Returns:\n        model: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file.\n\n    Args:\n        file_path (str): Path to the CSV file containing the data.\n\n    Returns:\n        pd.DataFrame: A pandas DataFrame containing the loaded data.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data for model prediction.\n\n    Args:\n        data (pd.DataFrame): Input data in the form of a pandas DataFrame.\n\n    Returns:\n        pd.DataFrame: Preprocessed data ready for prediction.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #data['date'] = data['date'].dt.strftime('%j').astype(int)\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n                     'air_temperature_tmmn', 'potential_evapotranspiration',\n                     'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n                     'relative_humidity_rmin', 'precipitation_amount',\n                     'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n                     'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Predict snow water equivalent (SWE) using a machine learning model.\n\n    Args:\n        model: The machine learning model for prediction.\n        data (pd.DataFrame): Input data for prediction.\n\n    Returns:\n        pd.DataFrame: Dataframe with predicted SWE values.\n    \"\"\"\n    data = data.fillna(-999)\n    input_data = data\n    #input_data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin',], axis=1)\n    predictions = model.predict(input_data)\n    data['predicted_swe'] = predictions\n    return data\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge predicted SWE data with the original data.\n\n    Args:\n        original_data (pd.DataFrame): Original input data.\n        predicted_data (pd.DataFrame): Dataframe with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged dataframe.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[\"lat\", \"lon\", \"predicted_swe\"]]\n    #merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    return merged_df\n\ndef predict():\n    \"\"\"\n    Main function for predicting snow water equivalent (SWE).\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"Using model: {model_path}\")\n  \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n  \n    if os.path.exists(output_path):\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('Data preprocessing completed.')\n    print(f'Model used: {model_path}')\n    predicted_data = predict_swe(model, preprocessed_data)\n    predicted_data = merge_data(preprocessed_data, predicted_data)\n    print('Data prediction completed.')\n  \n    predicted_data.to_csv(output_path, index=False)\n    print(\"Prediction successfully done \", output_path)\n\n    if len(predicted_data) == height * width:\n        print(f\"The image width, height match with the number of rows in the CSV. {len(predicted_data)} rows\")\n    else:\n        raise Exception(\"The total number of rows does not match\")\n\npredict()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "urd0nk",
  "name" : "data_terrainFeatures",
  "description" : null,
  "code" : "# Load dependencies\nimport geopandas as gpd\nimport json\nimport geojson\nfrom pystac_client import Client\nimport planetary_computer\nimport xarray\nimport rioxarray\nimport xrspatial\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom pyproj import Proj, transform\nimport os\nimport sys, traceback\nimport requests\n\nhome_dir = os.path.expanduser('~')\nsnowcast_github_dir = f\"{home_dir}/Documents/GitHub/SnowCast/\"\n\n#exit() # this process no longer need to execute, we need to make Geoweaver to specify which process doesn't need to run\n\n# user-defined paths for data-access\ndata_dir = f'{snowcast_github_dir}data/'\ngridcells_file = data_dir+'snowcast_provided/grid_cells_eval.geojson'\nstations_file = data_dir+'snowcast_provided/ground_measures_metadata.csv'\ngridcells_outfile = data_dir+'terrain/gridcells_terrainData_eval.csv'\nstations_outfile = data_dir+'terrain/station_terrainData_eval.csv'\n\nrequests.get('https://planetarycomputer.microsoft.com/api/stac/v1')\n\n# setup client for handshaking and data-access\nprint(\"setup planetary computer client\")\nclient = Client.open(\"https://planetarycomputer.microsoft.com/api/stac/v1\",ignore_conformance=True)\n\n# Load metadata\ngridcellsGPD = gpd.read_file(gridcells_file)\ngridcells = geojson.load(open(gridcells_file))\nstations = pd.read_csv(stations_file)\n\n# instantiate output panda dataframes\ndf_gridcells = df = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                            \"Elevation [m]\",\"Aspect [deg]\",\n                                            \"Curvature [ratio]\",\"Slope [deg]\",\n                                            \"Eastness [unitCirc.]\",\"Northness [unitCirc.]\"))\ndf_station = pd.DataFrame(columns=(\"Longitude [deg]\",\"Latitude [deg]\",\n                                   \"Elevation [m]\",\"Elevation_30 [m]\",\"Elevation_1000 [m]\",\n                                   \"Aspect_30 [deg]\",\"Aspect_1000 [deg]\",\n                                   \"Curvature_30 [ratio]\",\"Curvature_1000 [ratio]\",\n                                   \"Slope_30 [deg]\",\"Slope_1000 [deg]\",\n                                   \"Eastness_30 [unitCirc.]\",\"Northness_30 [unitCirc.]\",\n                                   \"Eastness_1000 [unitCirc.]\",\"Northness_1000 [unitCirc.]\"))\n\ndef prepareGridCellTerrain():\n  # instantiate output panda dataframes\n  # Calculate gridcell characteristics using Copernicus DEM data\n  print(\"Prepare GridCell Terrain data\")\n  for idx,cell in enumerate(gridcells['features']):\n      print(\"Processing grid \", idx)\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Polygon\", \"coordinates\":cell['geometry']['coordinates']},\n      )\n      items = list(search.get_items())\n      print(\"==> Searched items: \", len(items))\n\n      cropped_data = None\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              #xarray.open_rasterio(signed_asset.href)\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n      except:\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          cropped_data = data.rio.clip(gridcellsGPD['geometry'][idx:idx+1])\n\n      # calculate lat/long of center of gridcell\n      longitude = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n      latitude = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n      print(\"reproject data to EPSG:32612\")\n      # reproject the cropped dem data\n      cropped_data = cropped_data.rio.reproject(\"EPSG:32612\")\n\n      # Mean elevation of gridcell\n      mean_elev = cropped_data.mean().values\n      print(\"Elevation: \", mean_elev)\n\n      # Calculate directional components\n      aspect = xrspatial.aspect(cropped_data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(\"Aspect: \", mean_aspect)\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n      print(\"Eastness: \", mean_eastness)\n      print(\"Northness: \", mean_northness)\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(cropped_data)\n      mean_curvature = curvature.mean().values\n      print(\"Curvature: \", mean_curvature)\n\n      # Calculate mean slope\n      slope = xrspatial.slope(cropped_data)\n      mean_slope = slope.mean().values\n      print(\"Slope: \", mean_slope)\n\n      # Fill pandas dataframe\n      df_gridcells.loc[idx] = [longitude,latitude,\n                               mean_elev,mean_aspect,\n                               mean_curvature,mean_slope,\n                               mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n      #     df_gridcells.to_csv(gridcells_outfile)\n\n  # Save output data into csv format\n  df_gridcells.set_index(gridcellsGPD['cell_id'][0:idx+1],inplace=True)\n  df_gridcells.to_csv(gridcells_outfile)\n\ndef prepareStationTerrain():\n  # Calculate terrain characteristics of stations, and surrounding regions using COP 30\n  for idx,station in stations.iterrows():\n      search = client.search(\n          collections=[\"cop-dem-glo-30\"],\n          intersects={\"type\":\"Point\", \"coordinates\":[station['longitude'],station['latitude']]},\n      )\n      items = list(search.get_items())\n      print(f\"Returned {len(items)} items\")\n\n      try:\n          signed_asset = planetary_computer.sign(items[0].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-station['longitude'])\n          ydiff = np.abs(data.y-station['latitude'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n      except:\n          traceback.print_exc(file=sys.stdout)\n          signed_asset = planetary_computer.sign(items[1].assets[\"data\"])\n          data = (\n              xarray.open_rasterio(signed_asset.href)\n              .squeeze()\n              .drop(\"band\")\n              .coarsen({\"y\": 1, \"x\": 1})\n              .mean()\n          )\n          xdiff = np.abs(data.x-station['longitude'])\n          ydiff = np.abs(data.y-station['latitude'])\n          xdiff = np.where(xdiff == xdiff.min())[0][0]\n          ydiff = np.where(ydiff == ydiff.min())[0][0]\n          data = data[ydiff-33:ydiff+33,xdiff-33:xdiff+33].rio.reproject(\"EPSG:32612\")\n\n      # Reproject the station data to better include only 1000m surrounding area\n      inProj = Proj(init='epsg:4326')\n      outProj = Proj(init='epsg:32612')\n      new_x,new_y = transform(inProj,outProj,station['longitude'],station['latitude'])\n\n      # Calculate elevation of station and surroundings\n      mean_elevation = data.mean().values\n      elevation = data.sel(x=new_x,y=new_y,method='nearest')\n      print(elevation.values)\n\n      # Calcuate directional components\n      aspect = xrspatial.aspect(data)\n      aspect_xcomp = np.nansum(np.cos(aspect.values*(np.pi/180)))\n      aspect_ycomp = np.nansum(np.sin(aspect.values*(np.pi/180)))\n      mean_aspect = np.arctan2(aspect_ycomp,aspect_xcomp)*(180/np.pi)\n      if mean_aspect < 0:\n          mean_aspect = 360 + mean_aspect\n      print(mean_aspect)\n      aspect = aspect.sel(x=new_x,y=new_y,method='nearest')\n      print(aspect.values)\n      eastness = np.cos(aspect*(np.pi/180))\n      northness = np.sin(aspect*(np.pi/180))\n      mean_eastness = np.cos(mean_aspect*(np.pi/180))\n      mean_northness = np.sin(mean_aspect*(np.pi/180))\n\n      # Positive curvature = upward convex\n      curvature = xrspatial.curvature(data)\n      mean_curvature = curvature.mean().values\n      curvature = curvature.sel(x=new_x,y=new_y,method='nearest')\n      print(curvature.values)\n\n      # Calculate slope\n      slope = xrspatial.slope(data)\n      mean_slope = slope.mean().values\n      slope = slope.sel(x=new_x,y=new_y,method='nearest')\n      print(slope.values)\n\n      # Fill pandas dataframe\n      df_station.loc[idx] = [station['longitude'],station['latitude'],\n                             station['elevation_m'],elevation.values,mean_elevation,\n                             aspect.values,mean_aspect,\n                             curvature.values,mean_curvature,\n                             slope.values,mean_slope,\n                             eastness.values,northness.values,\n                             mean_eastness,mean_northness]\n\n      # Comment out for debugging/filling purposes\n      # if idx % 250 == 0:\n      #     df_station.set_index(stations['station_id'][0:idx+1],inplace=True)\n      #     df_station.to_csv(stations_outfile)\n\n  # Save output data into CSV format\n  df_station.set_index(stations['station_id'][0:idx+1],inplace=True)\n  df_station.to_csv(stations_outfile)\n\ntry:\n  prepareGridCellTerrain()\n  #prepareStationTerrain()\nexcept:\n  traceback.print_exc(file=sys.stdout)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "525l8q",
  "name" : "data_gee_modis_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport eeauth as e\n\n#exit() # done, uncomment if you want to download new files.\n\ntry:\n    ee.Initialize(e.creds())\nexcept Exception as e:\n    # the following is for the server\n    #service_account = 'eartheginegcloud@earthengine58.iam.gserviceaccount.com'\n#creds = ee.ServiceAccountCredentials(\n    #service_account, '/home/chetana/bhargavi-creds.json')\n    #ee.Initialize(creds)\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\n\norg_name = 'modis'\nproduct_name = f'MODIS/006/MOD10A1'\nvar_name = 'NDSI'\ncolumn_name = 'mod10a1_ndsi'\n\n#org_name = 'sentinel1'\n#product_name = 'COPERNICUS/S1_GRD'\n#var_name = 'VV'\n#column_name = 's1_grd_vv'\n\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n    \n    try:\n      \n  \t  print(station_cell_mapper_df['station_id'][ind], station_cell_mapper_df['cell_id'][ind])\n  \t  current_cell_id = station_cell_mapper_df['cell_id'][ind]\n  \t  print(\"collecting \", current_cell_id)\n  \t  single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/modis/{column_name}_{current_cell_id}.csv\"\n\n  \t  if os.path.exists(single_csv_file):\n  \t    print(\"exists skipping..\")\n  \t    continue\n\n  \t  longitude = station_cell_mapper_df['lon'][ind]\n  \t  latitude = station_cell_mapper_df['lat'][ind]\n\n  \t  # identify a 500 meter buffer around our Point Of Interest (POI)\n  \t  poi = ee.Geometry.Point(longitude, latitude).buffer(30)\n\n  \t  def poi_mean(img):\n  \t      reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi, scale=30)\n  \t      mean = reducer.get(var_name)\n  \t      return img.set('date', img.date().format()).set(column_name,mean)\n        \n  \t  viirs1 = ee.ImageCollection(product_name).filterDate('2013-01-01','2017-12-31')\n  \t  poi_reduced_imgs1 = viirs1.map(poi_mean)\n  \t  nested_list1 = poi_reduced_imgs1.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df1 = pd.DataFrame(nested_list1.getInfo(), columns=['date',column_name])\n      \n  \t  viirs2 = ee.ImageCollection(product_name).filterDate('2018-01-01','2021-12-31')\n  \t  poi_reduced_imgs2 = viirs2.map(poi_mean)\n  \t  nested_list2 = poi_reduced_imgs2.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n  \t  # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n  \t  df2 = pd.DataFrame(nested_list2.getInfo(), columns=['date',column_name])\n      \n\n  \t  df = pd.concat([df1, df2])\n  \t  df['date'] = pd.to_datetime(df['date'])\n  \t  df = df.set_index('date')\n  \t  df['cell_id'] = current_cell_id\n  \t  df['latitude'] = latitude\n  \t  df['longitude'] = longitude\n  \t  df.to_csv(single_csv_file)\n\n  \t  df_list = [all_cell_df, df]\n  \t  all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n  \t  print(e)\n  \t  pass\n    \n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "7temiv",
  "name" : "data_gee_sentinel1_station_only",
  "description" : null,
  "code" : "\n\n# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernal\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\nexit() # uncomment to download new files\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # this must be run in terminal instead of Geoweaver. Geoweaver doesn't support prompt.\n    ee.Initialize()\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n\n#org_name = 'modis'\n#product_name = f'MODIS/006/MOD10A1'\n#var_name = 'NDSI'\n#column_name = 'mod10a1_ndsi'\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nall_cell_df = pd.DataFrame(columns = ['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor ind in station_cell_mapper_df.index:\n  \n    try:\n  \t\n      current_cell_id = station_cell_mapper_df['cell_id'][ind]\n      print(\"collecting \", current_cell_id)\n      single_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}_{current_cell_id}.csv\"\n\n      if os.path.exists(single_csv_file):\n          print(\"exists skipping..\")\n          continue\n\n      longitude = station_cell_mapper_df['lon'][ind]\n      latitude = station_cell_mapper_df['lat'][ind]\n\n      # identify a 500 meter buffer around our Point Of Interest (POI)\n      poi = ee.Geometry.Point(longitude, latitude).buffer(1)\n      viirs = ee.ImageCollection(product_name).filterDate('2013-01-01','2021-12-31').filterBounds(poi).filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')).select('VV')\n      \n      def poi_mean(img):\n          reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n          mean = reducer.get(var_name)\n          return img.set('date', img.date().format()).set(column_name,mean)\n\n      \n      poi_reduced_imgs = viirs.map(poi_mean)\n\n      nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date',column_name]).values().get(0)\n\n      # dont forget we need to call the callback method \"getInfo\" to retrieve the data\n      df = pd.DataFrame(nested_list.getInfo(), columns=['date',column_name])\n\n      df['date'] = pd.to_datetime(df['date'])\n      df = df.set_index('date')\n\n      df['cell_id'] = current_cell_id\n      df['latitude'] = latitude\n      df['longitude'] = longitude\n      df.to_csv(single_csv_file)\n\n      df_list = [all_cell_df, df]\n      all_cell_df = pd.concat(df_list) # merge into big dataframe\n      \n    except Exception as e:\n      \n      print(e)\n      pass\n    \nall_cell_df.to_csv(f\"{homedir}/Documents/GitHub/SnowCast/data/{org_name}/{column_name}.csv\")  \n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rmxece",
  "name" : "data_associate_station_grid_cell",
  "description" : null,
  "code" : "import json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\nimport math\n\n# pd.set_option('display.max_columns', None)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nmodel_dir = f\"{github_dir}/model/\"\ntraining_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_train_features.csv\"\ntesting_feature_file = f\"{github_dir}/data/snowcast_provided/ground_measures_test_features.csv\"\ntrain_labels_file = f\"{github_dir}/data/snowcast_provided/train_labels.csv\"\nground_measure_metadata_file = f\"{github_dir}/data/snowcast_provided/ground_measures_metadata.csv\"\n\nready_for_training_folder = f\"{github_dir}/data/ready_for_training/\"\n\nresult_mapping_file = f\"{ready_for_training_folder}station_cell_mapping.csv\"\n\nif os.path.exists(result_mapping_file):\n    exit()\n\ngridcells = geojson.load(open(gridcells_file))\ntraining_df = pd.read_csv(training_feature_file, header=0)\ntesting_df = pd.read_csv(testing_feature_file, header=0)\nground_measure_metadata_df = pd.read_csv(ground_measure_metadata_file, header=0)\ntrain_labels_df = pd.read_csv(train_labels_file, header=0)\n\nprint(\"training: \", training_df.head())\nprint(\"testing: \", testing_df.head())\nprint(\"ground measure metadata: \", ground_measure_metadata_df.head())\nprint(\"training labels: \", train_labels_df.head())\n\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n\n\n# prepare the training data\n\nstation_cell_mapper_df = pd.DataFrame(columns=[\"station_id\", \"cell_id\", \"lat\", \"lon\"])\n\nground_measure_metadata_df = ground_measure_metadata_df.reset_index()  # make sure indexes pair with number of rows\nfor index, row in ground_measure_metadata_df.iterrows():\n\n    print(row['station_id'], row['name'], row['latitude'], row['longitude'])\n    station_lat = row['latitude']\n    station_lon = row['longitude']\n\n    shortest_dis = 999\n    associated_cell_id = None\n    associated_lat = None\n    associated_lon = None\n\n    for idx, cell in enumerate(gridcells['features']):\n\n        current_cell_id = cell['properties']['cell_id']\n\n        # print(\"collecting \", current_cell_id)\n        cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n        cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n\n        dist = calculateDistance(station_lat, station_lon, cell_lat, cell_lon)\n\n        if dist < shortest_dis:\n            associated_cell_id = current_cell_id\n            shortest_dis = dist\n            associated_lat = cell_lat\n            associated_lon = cell_lon\n\n    station_cell_mapper_df.loc[len(station_cell_mapper_df.index)] = [row['station_id'], associated_cell_id,\n                                                                     associated_lat, associated_lon]\n\nprint(station_cell_mapper_df.head())\nstation_cell_mapper_df.to_csv(f\"{ready_for_training_folder}station_cell_mapping.csv\")\n\n\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "illwc1",
  "name" : "data_gee_modis_real_time",
  "description" : null,
  "code" : "import os\nimport pprint\n\n# import gdal\nimport subprocess\nfrom datetime import datetime, timedelta\n\n# set up your credentials using\n# echo 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\n\nmodis_download_dir = \"/home/chetana/modis_download_folder/\"\nmodis_downloaded_data = modis_download_dir + \"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\ngeo_tiff = modis_download_dir + \"geo-tiff/\"\nvrt_file_dir = modis_download_dir + \"vrt_files/\"\ndir_path = os.path.dirname(os.path.realpath(__file__))\nprint(dir_path)\n\ntile_list = ['h09v04', 'h10v04', 'h11v04', 'h08v04', 'h08v05', 'h09v05', 'h10v05', 'h07v06', 'h08v06', 'h09v06']\n\n\ndef get_files(directory):\n    \"\"\"\n    Get a list of files in a directory and its subdirectories.\n\n    Args:\n        directory (str): The directory to search for files.\n\n    Returns:\n        dict: A dictionary where keys are subdirectory names and values are lists of file paths.\n    \"\"\"\n    file_directory = list()\n    complete_directory_structure = dict()\n    for dirpath, dirnames, filenames in os.walk(directory):\n        for filename in filenames:\n            file_path = os.path.join(dirpath, filename)\n            file_directory.append(file_path)\n            complete_directory_structure[str(dirpath).rsplit('/')[-1]] = file_directory\n\n    return complete_directory_structure\n\n\ndef get_latest_date():\n    \"\"\"\n    Retrieve the latest date from the MODIS data website.\n\n    Returns:\n        datetime: The latest date as a datetime object.\n    \"\"\"\n    all_rows = get_web_row_data()\n\n    latest_date = None\n    for row in all_rows:\n        try:\n            new_date = datetime.strptime(row.text[:-1], '%Y.%m.%d')\n            if latest_date is None or latest_date < new_date:\n                latest_date = new_date\n        except:\n            continue\n\n    print(\"Find the latest date: \", latest_date.strftime(\"%Y.%m.%d\"))\n    second_latest_date = latest_date - timedelta(days=8)\n    return second_latest_date\n\n\ndef get_web_row_data():\n    \"\"\"\n    Fetch and parse the MODIS data website content.\n\n    Returns:\n        list: A list of rows from the website's table.\n    \"\"\"\n    try:\n        from BeautifulSoup import BeautifulSoup\n    except ImportError:\n        from bs4 import BeautifulSoup\n    modis_list_url = \"https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/\"\n    print(\"Source / Product: \" + modis_list_url)\n    if os.path.exists(\"index.html\"):\n        os.remove(\"index.html\")\n    subprocess.run(\n        f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies '\n        f'--no-check-certificate --auth-no-challenge=on -np -e robots=off {modis_list_url}',\n        shell=True, stderr=subprocess.PIPE)\n    index_file = open('index.html', 'r')\n    webContent = index_file.read()\n    parsed_html = BeautifulSoup(webContent, \"html.parser\")\n    all_rows = parsed_html.body.findAll('td', attrs={'class': 'indexcolname'})\n    return all_rows\n\n\ndef download_recent_modis(date=None):\n    \"\"\"\n    Download recent MODIS data.\n\n    Args:\n        date (datetime, optional): A specific date to download. Defaults to None.\n    \"\"\"\n    if date:\n        latest_date_str = date.strftime(\"%Y.%m.%d\")\n    else:\n        latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n    for tile in tile_list:\n        download_cmd = f'wget --load-cookies ~/.urs_cookies --save-cookies ~/.urs_cookies --keep-session-cookies ' \\\n                       f'--no-check-certificate --auth-no-challenge=on -r --reject \"i' \\\n                       f'ndex.html*\" -P {modis_download_dir} -np -e robots=off ' \\\n                       f'https://n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/ -A \"*{tile}*.hdf\" --quiet'\n        # print(download_cmd)\n        p = subprocess.run(download_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        print(\"Downloading tile, \", tile, \" with status code \", \"OK\" if p.returncode == 0 else p.returncode)\n\n\n# def merge_wrap_tif_into_western_us_tif():\n#     latest_date_str = get_latest_date().strftime(\"%Y.%m.%d\")\n#     # traverse the folder and find the new download files\n#     for filename in os.listdir(f\"n5eil01u.ecs.nsidc.org/MOST/MOD10A2.061/{latest_date_str}/\"):\n#         f = os.path.join(directory, filename)\n#         # checking if it is a file\n#         if os.path.isfile(f):\n#             print(f)\n# merge_wrap_tif_into_western_us_tif()\n\ndef hdf_tif_cvt(resource_path, destination_path):\n    \"\"\"\n    Convert HDF files to GeoTIFF format.\n\n    Args:\n        resource_path (str): The path to the source HDF file.\n        destination_path (str): The path to save the converted GeoTIFF file.\n    \"\"\"\n    if not os.path.isfile(resource_path):\n        raise Exception(\"HDF file not found\")\n\n    max_snow_extent_path = destination_path + \"maximum_snow_extent/\"\n    eight_day_snow_cover = destination_path + \"eight_day_snow_cover/\"\n    if not os.path.exists(max_snow_extent_path):\n        os.makedirs(max_snow_extent_path)\n    if not os.path.exists(eight_day_snow_cover):\n        os.makedirs(eight_day_snow_cover)\n\n    tif_file_name_snow_extent = max_snow_extent_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_name_eight_day = eight_day_snow_cover + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name_snow_extent + '_max_snow_extent' + tif_file_extension\n    eight_day_snow_cover_file_name = tif_file_name_eight_day + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Maximum_Snow_Extent\"\n    eight_day_snow_cover = f\"HDF4_EOS:EOS_GRID:\\\"{resource_path}\\\":MOD_Grid_Snow_500m:Eight_Day_Snow_Cover\"\n\n    subprocess.run(f\"gdal_translate {maximum_snow_extent} {maximum_snow_extent_file_name}\", shell=True)\n    subprocess.run(f\"gdal_translate {eight_day_snow_cover} {eight_day_snow_cover_file_name}\", shell=True)\n\n\ndef combine_geotiff_gdal(vrt_array, destination):\n    \"\"\"\n    Combine GeoTIFF files using GDAL.\n\n    Args:\n        vrt_array (list): A list of GeoTIFF file paths to combine.\n        destination (str): The path to save the combined VRT and GeoTIFF files.\n    \"\"\"\n    subprocess.run(f\"gdalbuildvrt {destination} {' '.join(vrt_array)}\", shell=True)\n    tif_name = destination.split('.vrt')[-2] + '.tif'\n    subprocess.run(f\"gdal_translate -of GTiff {destination} {tif_name}\", shell=True)\n\n\ndef hdf_tif_conversion(resource_path, destination_path):\n    \"\"\"\n    Convert HDF files to GeoTIFF format using GDAL.\n\n    Args:\n        resource_path (str): The path to the source HDF file.\n        destination_path (str): The path to save the converted GeoTIFF file.\n    \"\"\"\n    hdf_dataset = gdal.Open(resource_path)\n    if hdf_dataset is None:\n        raise Exception(\"Could not open HDF dataset\")\n\n    maximum_snow_extent = hdf_dataset.GetSubDatasets()[0][0]\n    modis_snow_500m = hdf_dataset.GetSubDatasets()[1][0]\n\n    driver = gdal.GetDriverByName('GTiff')\n\n    tif_file_name = destination_path + resource_path.split('/')[-1].split('.hdf')[0]\n    tif_file_extension = '.tif'\n\n    maximum_snow_extent_file_name = tif_file_name + '_max_snow_extent' + tif_file_extension\n    modis_snow_500m_file_name = tif_file_name + '_modis_snow_500m' + tif_file_extension\n\n    maximum_snow_extent_dataset = gdal.Open(maximum_snow_extent)\n    modis_snow_500m_dataset = gdal.Open(modis_snow_500m)\n\n    if maximum_snow_extent_dataset is None:\n        raise Exception(\"Could not open maximum_snow_extent dataset\")\n\n    if modis_snow_500m_dataset is None:\n        raise Exception(\"Could not open modis_snow_500m dataset\")\n\n    driver.CreateCopy(maximum_snow_extent_file_name, maximum_snow_extent_dataset, 0)\n    driver.CreateCopy(modis_snow_500m_file_name, modis_snow_500m_dataset, 0)\n\n    print(\"HDF to TIF conversion completed successfully.\")\n\n\ndef download_modis_archive(*, start_date, end_date):\n    \"\"\"\n    Download MODIS data for a specified date range.\n\n    Keyword Args:\n        start_date (datetime): The start date of the date range.\n        end_date (datetime): The end date of the date range.\n    \"\"\"\n    all_archive_dates = list()\n\n    all_rows = get_web_row_data()\n    for r in all_rows:\n        try:\n            all_archive_dates.append(datetime.strptime(r.text.replace('/', ''), '%Y.%m.%d'))\n        except:\n            continue\n\n    for a in all_archive_dates:\n        if start_date <= a <= end_date:\n            download_recent_modis(a)\n\n\ndef step_one_download_modis():\n  \"\"\"\n  Step one of the main workflow: Download recent MODIS data.\n  \"\"\"\n  download_recent_modis()\n                   \ndef step_two_merge_modis_western_us():\n  \"\"\"\n  Step two of the main workflow: Merge MODIS data for the western US.\n  \"\"\"\n  download_modis_archive(start_date=datetime(2022, 1, 1), end_date=datetime(2022, 12, 31))\n\n  files = get_files(modis_downloaded_data)\n  for k, v in get_files(modis_downloaded_data).items():\n\n    conversion_path = modis_download_dir + \"geo-tiff/\" + k + \"/\"\n    if not os.path.exists(conversion_path):\n        os.makedirs(conversion_path)\n    for hdf_file in v:\n        # print(hdf_file.split('/')[-1].split('.hdf')[0], 1)\n        hdf_tif_cvt(hdf_file, conversion_path)\n\n  if not os.path.exists(vrt_file_dir):\n    os.makedirs(vrt_file_dir)\n\n\n  directories = [d for d in os.listdir(geo_tiff) if   os.path.isdir(os.path.join(geo_tiff, d))]\n\n  for d in directories:\n    eight_day_snow_cover = geo_tiff + d + '/eight_day_snow_cover'\n    maximum_snow_extent = geo_tiff + d + '/maximum_snow_extent'\n\n    eight_day_abs_path = list()\n    snow_extent_abs_path = list()\n\n    for file in os.listdir(eight_day_snow_cover):\n        file_path = os.path.abspath(os.path.join(eight_day_snow_cover, file))\n        eight_day_abs_path.append(file_path)\n\n    for file in os.listdir(maximum_snow_extent):\n        file_path = os.path.abspath(os.path.join(maximum_snow_extent, file))\n        snow_extent_abs_path.append(file_path)\n\n    combine_geotiff_gdal(eight_day_abs_path, vrt_file_dir + f\"{d}_eight_day.vrt\")\n    combine_geotiff_gdal(snow_extent_abs_path, vrt_file_dir + f\"{d}_snow_extent.vrt\")\n\n                   \n# main workflow is here:\nstep_one_download_modis()\nstep_two_merge_modis_western_us()\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "sjs5by",
  "name" : "data_gee_sentinel1_real_time",
  "description" : null,
  "code" : "# reminder that if you are installing libraries in a Google Colab instance you will be prompted to restart your kernel\n\nfrom all_dependencies import *\nfrom snowcast_utils import *\n\ntry:\n    ee.Initialize()\nexcept Exception as e:\n    ee.Authenticate() # This must be run in the terminal instead of Geoweaver. Geoweaver doesn't support prompts.\n    ee.Initialize()\n\n# Read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n# Read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# Read grid cell\nsubmission_format_file = f\"{github_dir}/data/snowcast_provided/submission_format_eval.csv\"\nsubmission_format_df = pd.read_csv(submission_format_file, header=0, index_col=0)\n\nprint(\"submission_format_df shape: \", submission_format_df.shape)\n\nall_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\nall_cell_coords_df = pd.read_csv(all_cell_coords_file, header=0, index_col=0)\n\n# Start_date = \"2022-04-20\" # Test_start_date\nstart_date = findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1\", \"%Y-%m-%d %H:%M:%S\")\nend_date = test_end_date\n\norg_name = 'sentinel1'\nproduct_name = 'COPERNICUS/S1_GRD'\nvar_name = 'VV'\ncolumn_name = 's1_grd_vv'\n\nfinal_csv_file = f\"{homedir}/Documents/GitHub/SnowCast/data/sat_testing/{org_name}/{column_name}_{start_date}_{end_date}.csv\"\nprint(f\"Results will be saved to {final_csv_file}\")\n\nif os.path.exists(final_csv_file):\n    #print(\"exists skipping..\")\n    #exit()\n    os.remove(final_csv_file)\n\nall_cell_df = pd.DataFrame(columns=['date', column_name, 'cell_id', 'latitude', 'longitude'])\n\nfor current_cell_id in submission_format_df.index:\n\n    try:\n        #print(\"collecting \", current_cell_id)\n\n        longitude = all_cell_coords_df['lon'][current_cell_id]\n        latitude = all_cell_coords_df['lat'][current_cell_id]\n\n        # Identify a 500-meter buffer around our Point Of Interest (POI)\n        poi = ee.Geometry.Point(longitude, latitude).buffer(10)\n\n        viirs = ee.ImageCollection(product_name) \\\n            .filterDate(start_date, end_date) \\\n            .filterBounds(poi) \\\n            .filter(ee.Filter.listContains('transmitterReceiverPolarisation', 'VV')) \\\n            .select('VV')\n\n        def poi_mean(img):\n            reducer = img.reduceRegion(reducer=ee.Reducer.mean(), geometry=poi)\n            mean = reducer.get(var_name)\n            return img.set('date', img.date().format()).set(column_name, mean)\n\n        poi_reduced_imgs = viirs.map(poi_mean)\n\n        nested_list = poi_reduced_imgs.reduceColumns(ee.Reducer.toList(2), ['date', column_name]).values().get(0)\n\n        # Don't forget we need to call the callback method \"getInfo\" to retrieve the data\n        df = pd.DataFrame(nested_list.getInfo(), columns=['date', column_name])\n\n        df['date'] = pd.to_datetime(df['date'])\n        df = df.set_index('date')\n\n        df['cell_id'] = current_cell_id\n        df['latitude'] = latitude\n        df['longitude'] = longitude\n\n        df_list = [all_cell_df, df]\n        all_cell_df = pd.concat(df_list)  # Merge into a big dataframe\n\n    except Exception as e:\n\n        #print(e)\n        pass\n\nall_cell_df.to_csv(final_csv_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "y7nb46",
  "name" : "base_hole",
  "description" : null,
  "code" : "'''\nThe wrapper for all the snowcast_wormhole predictors.\n'''\n\nimport os\nimport joblib\nfrom datetime import datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport shutil\n\nhomedir = os.path.expanduser('~')\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nclass BaseHole:\n    '''\n    Base class for snowcast_wormhole predictors.\n\n    Attributes:\n        all_ready_file (str): The path to the CSV file containing the data for training.\n        classifier: The machine learning model used for prediction.\n        holename (str): The name of the wormhole class.\n        train_x (numpy.ndarray): The training input data.\n        train_y (numpy.ndarray): The training target data.\n        test_x (numpy.ndarray): The testing input data.\n        test_y (numpy.ndarray): The testing target data.\n        test_y_results (numpy.ndarray): The predicted results on the test data.\n        save_file (str): The path to save the trained model.\n    '''\n\n    all_ready_file = f\"{github_dir}/data/ready_for_training/all_ready_new.csv\"\n\n    def __init__(self):\n        '''\n        Initializes a new instance of the BaseHole class.\n        '''\n        self.classifier = self.get_model()\n        self.holename = self.__class__.__name__ \n        self.train_x = None\n        self.train_y = None\n        self.test_x = None\n        self.test_y = None\n        self.test_y_results = None\n        self.save_file = None\n    \n    def save(self):\n        '''\n        Save the trained model to a joblib file with a timestamp.\n\n        Returns:\n            None\n        '''\n        now = datetime.now()\n        date_time = now.strftime(\"%Y%d%m%H%M%S\")\n        self.save_file = f\"{github_dir}/model/wormhole_{self.holename}_{date_time}.joblib\"\n        \n        print(f\"Saving model to {self.save_file}\")\n        joblib.dump(self.classifier, self.save_file)\n        # copy a version to the latest file placeholder\n        latest_copy_file = f\"{github_dir}/model/wormhole_{self.holename}_latest.joblib\"\n        shutil.copy(self.save_file, latest_copy_file)\n        print(f\"a copy of the model is saved to {latest_copy_file}\")\n  \n    def preprocessing(self):\n        '''\n        Preprocesses the data for training and testing.\n\n        Returns:\n            None\n        '''\n        all_ready_pd = pd.read_csv(self.all_ready_file, header=0, index_col=0)\n        print(\"all columns: \", all_ready_pd.columns)\n        all_ready_pd = all_ready_pd[all_cols]\n        all_ready_pd = all_ready_pd.dropna()\n        train, test = train_test_split(all_ready_pd, test_size=0.2)\n        self.train_x, self.train_y = train[input_columns].to_numpy().astype('float'), train[['swe_value']].to_numpy().astype('float')\n        self.test_x, self.test_y = test[input_columns].to_numpy().astype('float'), test[['swe_value']].to_numpy().astype('float')\n  \n    def train(self):\n        '''\n        Trains the machine learning model.\n\n        Returns:\n            None\n        '''\n        self.classifier.fit(self.train_x, self.train_y)\n  \n    def test(self):\n        '''\n        Tests the machine learning model on the testing data.\n\n        Returns:\n            numpy.ndarray: The predicted results on the testing data.\n        '''\n        self.test_y_results = self.classifier.predict(self.test_x)\n        return self.test_y_results\n  \n    def predict(self, input_x):\n        '''\n        Makes predictions using the trained model on new input data.\n\n        Args:\n            input_x (numpy.ndarray): The input data for prediction.\n\n        Returns:\n            numpy.ndarray: The predicted results.\n        '''\n        return self.classifier.predict(input_x)\n  \n    def evaluate(self):\n        '''\n        Evaluates the performance of the machine learning model.\n\n        Returns:\n            None\n        '''\n        pass\n  \n    def get_model(self):\n        '''\n        Get the machine learning model.\n\n        Returns:\n            object: The machine learning model.\n        '''\n        pass\n  \n    def post_processing(self):\n        '''\n        Perform post-processing on the model's predictions.\n\n        Returns:\n            None\n        '''\n        pass\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "a8p3n7",
  "name" : "data_gee_gridmet_station_only",
  "description" : null,
  "code" : "import os\nimport glob\nimport urllib.request\nfrom datetime import date\n\nimport pandas as pd\nimport xarray as xr\nfrom pathlib import Path\nfrom snowcast_utils import work_dir\nimport warnings\n\n# Suppress FutureWarnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nstart_date = date(2019, 1, 1)\nend_date = date(2022, 12, 31)\n\nyear_list = [start_date.year + i for i in range(end_date.year - start_date.year + 1)]\n\nworking_dir = work_dir\nstations = pd.read_csv(f'{working_dir}/station_cell_mapping.csv')\ngridmet_save_location = f'{working_dir}/gridmet_climatology'\nfinal_merged_csv = f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv'\n\n\ndef get_files_in_directory():\n    f = list()\n    for files in glob.glob(gridmet_save_location + \"/*.nc\"):\n        f.append(files)\n    return f\n\n\ndef download_file(url, save_location):\n    try:\n        print(\"download_file\")\n        with urllib.request.urlopen(url) as response:\n            file_content = response.read()\n        file_name = os.path.basename(url)\n        save_path = os.path.join(save_location, file_name)\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\n\ndef gridmet_climatology():\n    folder_name = gridmet_save_location\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            print(\"downloading\", download_link)\n            if not os.path.exists(os.path.join(folder_name, var + '_' + '%s' % y + '.nc')):\n                download_file(download_link, folder_name)\n\n\ndef get_gridmet_variable(file_name):\n    print(f\"reading values from {file_name}\")\n    result_data = []\n    ds = xr.open_dataset(file_name)\n    var_to_extract = list(ds.keys())\n    print(var_to_extract)\n    var_name = var_to_extract[0]\n    \n    df = pd.DataFrame(columns=['day', 'lat', 'lon', var_name])\n    \n    csv_file = f'{gridmet_save_location}/{Path(file_name).stem}.csv'\n    if os.path.exists(csv_file):\n    \tprint(f\"The file '{csv_file}' exists.\")\n        return\n\n    for idx, row in stations.iterrows():\n        lat = row['lat']\n        lon = row['lon']\n\t\t\n        subset_data = ds.sel(lat=lat, lon=lon, method='nearest')\n        subset_data['lat'] = lat\n        subset_data['lon'] = lon\n        # print('subset data:', lat, lon, subset_data.values())\n        converted_df = subset_data.to_dataframe()\n        #print(\"converted_df: \", converted_df.head())\n        #print(\"converted_df columns: \", converted_df.columns)\n        converted_df = converted_df.reset_index(drop=False)\n        #print(\"convert to columns: \", converted_df.columns)\n        converted_df = converted_df.drop('crs', axis=1)\n        df = pd.concat([df, converted_df], ignore_index=True)\n        \n    result_df = df\n    print(\"got result_df : \", result_df.head())\n    result_df.to_csv(csv_file, index=False)\n    print(f'completed extracting data for {file_name}')\n\n\ndef merge_similar_variables_from_different_years():\n    files = os.listdir(gridmet_save_location)\n    file_groups = {}\n\n    for filename in files:\n        base_name, year_ext = os.path.splitext(filename)\n        parts = base_name.split('_')\n        if len(parts) == 2 and year_ext == '.csv':\n            file_groups.setdefault(parts[0], []).append(filename)\n\n    for base_name, file_list in file_groups.items():\n        if len(file_list) > 1:\n            dfs = []\n            for filename in file_list:\n                df = pd.read_csv(os.path.join(gridmet_save_location, filename))\n                dfs.append(df)\n            merged_df = pd.concat(dfs, ignore_index=True)\n            merged_filename = f\"{base_name}_merged.csv\"\n            merged_df.to_csv(os.path.join(gridmet_save_location, merged_filename), index=False)\n            print(f\"Merged {file_list} into {merged_filename}\")\n\n\ndef merge_all_variables_together():\n    merged_df = None\n    file_paths = []\n\n    for filename in os.listdir(gridmet_save_location):\n        if filename.endswith(\"_merged.csv\"):\n            file_paths.append(os.path.join(gridmet_save_location, filename))\n\t\n    rmin_merged_path = os.path.join(gridmet_save_location, 'rmin_merged.csv')\n    rmax_merged_path = os.path.join(gridmet_save_location, 'rmax_merged.csv')\n    tmmn_merged_path = os.path.join(gridmet_save_location, 'tmmn_merged.csv')\n    tmmx_merged_path = os.path.join(gridmet_save_location, 'tmmx_merged.csv')\n    \n    df_rmin = pd.read_csv(rmin_merged_path)\n    df_rmax = pd.read_csv(rmax_merged_path)\n    df_tmmn = pd.read_csv(tmmn_merged_path)\n    df_tmmx = pd.read_csv(tmmx_merged_path)\n    \n    df_rmin.rename(columns={'relative_humidity': 'relative_humidity_rmin'}, inplace=True)\n    df_rmax.rename(columns={'relative_humidity': 'relative_humidity_rmax'}, inplace=True)\n    df_tmmn.rename(columns={'air_temperature': 'air_temperature_tmmn'}, inplace=True)\n    df_tmmx.rename(columns={'air_temperature': 'air_temperature_tmmx'}, inplace=True)\n    \n    df_rmin.to_csv(os.path.join(gridmet_save_location, 'rmin_merged.csv'))\n    df_rmax.to_csv(os.path.join(gridmet_save_location, 'rmax_merged.csv'))\n    df_tmmn.to_csv(os.path.join(gridmet_save_location, 'tmmn_merged.csv'))\n    df_tmmx.to_csv(os.path.join(gridmet_save_location, 'tmmx_merged.csv'))\n    \n    if file_paths:\n        merged_df = pd.read_csv(file_paths[0])\n        for file_path in file_paths[1:]:\n            df = pd.read_csv(file_path)\n            merged_df = pd.concat([merged_df, df], axis=1)\n        merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n        merged_df.to_csv(final_merged_csv, index=False)\n\n\ngridmet_climatology()\nnc_files = get_files_in_directory()\n\nfor nc in nc_files:\n    get_gridmet_variable(nc)\nmerge_similar_variables_from_different_years()\nmerge_all_variables_together()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "smsdr0",
  "name" : "data_gee_gridmet_real_time",
  "description" : null,
  "code" : "import os\nimport urllib\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Download the NetCDF files from the Idaho HTTP site daily or for the time period matching the MODIS period.\n# Download site: https://www.northwestknowledge.net/metdata/data/\n\ndownload_source = \"https://www.northwestknowledge.net/metdata/data/\"\ngridmet_download_dir = \"/home/chetana/terrian_data/\"\n\ndef download_gridmet():\n    \"\"\"\n    Download GridMET NetCDF files from the specified source to the download directory.\n\n    This function fetches NetCDF files from the provided website and saves them in the specified download directory.\n\n    Returns:\n        None\n    \"\"\"\n    if not os.path.exists(gridmet_download_dir):\n        os.makedirs(gridmet_download_dir)\n\n    soup = BeautifulSoup(requests.get(download_source).text, \"html.parser\")\n    tag_links = soup.find_all('a')\n    for t in tag_links:\n        if '.nc' in t.text and not 'eddi' in t.text and not os.path.isfile(gridmet_download_dir + t.get(\"href\")):\n            print(f'Downloading {t.get(\"href\")}')\n            urllib.request.urlretrieve(download_source + t.get('href'), gridmet_download_dir + t.get(\"href\"))\n\ndownload_gridmet()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "4i0sop",
  "name" : "model_creation_xgboost",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the XGBoostHole class, which is used to train and evaluate an Extra Trees Regression model for hole analysis.\n\nAttributes:\n    XGBoostHole (class): A class for training and using an Extra Trees Regression model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n\"\"\"\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nfrom model_creation_rf import RandomForestHole\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nclass XGBoostHole(RandomForestHole):\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        etmodel = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                    max_depth=None, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    min_samples_leaf=1,\n                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n                    n_estimators=100, n_jobs=-1, oob_score=False,\n                    random_state=123, verbose=0, warm_start=False)\n        return etmodel\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b63prf",
  "name" : "testing_data_integration",
  "description" : null,
  "code" : "import os\nimport pandas as pd\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\n\ndef merge_all_gridmet_amsr_csv_into_one(gridmet_csv_folder, dem_all_csv, testing_all_csv):\n    \"\"\"\n    Merge all GridMET and AMSR CSV files into one combined CSV file.\n\n    Args:\n        gridmet_csv_folder (str): The folder containing GridMET CSV files.\n        dem_all_csv (str): Path to the DEM (Digital Elevation Model) CSV file.\n        testing_all_csv (str): Path to save the merged CSV file.\n\n    Returns:\n        None\n    \"\"\"\n    # List of file paths for the CSV files\n    csv_files = []\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    for file in os.listdir(gridmet_csv_folder):\n        if file.endswith('.csv') and test_start_date in file:\n            csv_files.append(os.path.join(gridmet_csv_folder, file))\n\n    # Initialize an empty list to store all dataframes\n    dfs = []\n\n    # Read each CSV file into separate dataframes\n    for file in csv_files:\n        df = pd.read_csv(file, encoding='utf-8', index_col=False)\n        dfs.append(df)\n\n    dem_df = pd.read_csv(f\"{work_dir}/dem_all.csv\", encoding='utf-8', index_col=False)\n    dfs.append(dem_df)\n\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    amsr_df = pd.read_csv(f'{work_dir}/testing_ready_amsr_{date}.csv', index_col=False)\n    amsr_df.rename(columns={'gridmet_lat': 'Latitude', 'gridmet_lon': 'Longitude'}, inplace=True)\n    dfs.append(amsr_df)\n\n    # Merge the dataframes based on the latitude and longitude columns\n    merged_df = dfs[0]  # Start with the first dataframe\n    for i in range(1, len(dfs)):\n        print(dfs[i].shape)\n        merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude'])\n\n    # Save the merged dataframe to a new CSV file\n    merged_df.to_csv(testing_all_csv, index=False)\n    print(f\"All input CSV files are merged to {testing_all_csv}\")\n    print(merged_df.columns)\n    print(merged_df[\"AMSR_SWE\"].describe())\n    print(merged_df[\"vpd\"].describe())\n    print(merged_df[\"pr\"].describe())\n    print(merged_df[\"tmmx\"].describe())\n\nif __name__ == \"__main__\":\n    # Replace with the actual path to your folder\n    gridmet_csv_folder = f\"{work_dir}/gridmet_climatology/\"\n    merge_all_gridmet_amsr_csv_into_one(f\"{work_dir}/testing_output/\",\n                                        f\"{work_dir}/dem_all.csv\",\n                                        f\"{work_dir}/testing_all_ready.csv\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "zh38b6",
  "name" : "snowcast_utils",
  "description" : null,
  "code" : "from datetime import date, datetime, timedelta\nimport json\nimport math\nimport numpy as np\nimport os\nimport pandas as pd\n# import ee\n#import seaborn as sns\nimport matplotlib.pyplot as plt\n#import geopandas as gpd\n#import geojson\n\ntoday = date.today()\n\n# dd/mm/YY\nd1 = today.strftime(\"%Y-%m-%d\")\nprint(\"today date =\", d1)\n\ntrain_start_date = \"2019-01-01\"\ntrain_end_date = \"2022-12-31\"\n\ntest_start_date = \"2023-05-17\"\ntest_end_date = d1\nprint(\"test start date: \", test_start_date)\nprint(\"test end date: \", test_end_date)\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n\nwork_dir = \"/home/chetana/gridmet_test_run\"\n\n# Define a function to convert the month to season\ndef month_to_season(month):\n    if 3 <= month <= 5:\n        return 1\n    elif 6 <= month <= 8:\n        return 2\n    elif 9 <= month <= 11:\n        return 3\n    else:\n        return 4\n\ndef calculateDistance(lat1, lon1, lat2, lon2):\n    \"\"\"\n    Calculate the distance (Euclidean) between two sets of coordinates (lat1, lon1) and (lat2, lon2).\n    \n    Parameters:\n    - lat1 (float): Latitude of the first point.\n    - lon1 (float): Longitude of the first point.\n    - lat2 (float): Latitude of the second point.\n    - lon2 (float): Longitude of the second point.\n    \n    Returns:\n    - float: The Euclidean distance between the two points.\n    \"\"\"\n    lat1 = float(lat1)\n    lon1 = float(lon1)\n    lat2 = float(lat2)\n    lon2 = float(lon2)\n    return math.sqrt((lat1 - lat2) ** 2 + (lon1 - lon2) ** 2)\n\ndef create_cell_location_csv():\n    \"\"\"\n    Create a CSV file containing cell locations from a GeoJSON file.\n    \"\"\"\n    # read grid cell\n    gridcells_file = f\"{github_dir}/data/snowcast_provided/grid_cells_eval.geojson\"\n    all_cell_coords_file = f\"{github_dir}/data/snowcast_provided/all_cell_coords_file.csv\"\n    if os.path.exists(all_cell_coords_file):\n        os.remove(all_cell_coords_file)\n\n    grid_coords_df = pd.DataFrame(columns=[\"cell_id\", \"lat\", \"lon\"])\n    print(grid_coords_df.head())\n    gridcells = geojson.load(open(gridcells_file))\n    for idx, cell in enumerate(gridcells['features']):\n        current_cell_id = cell['properties']['cell_id']\n        cell_lon = np.unique(np.ravel(cell['geometry']['coordinates'])[0::2]).mean()\n        cell_lat = np.unique(np.ravel(cell['geometry']['coordinates'])[1::2]).mean()\n        grid_coords_df.loc[len(grid_coords_df.index)] = [current_cell_id, cell_lat, cell_lon]\n\n    # grid_coords_np = grid_coords_df.to_numpy()\n    # print(grid_coords_np.shape)\n    grid_coords_df.to_csv(all_cell_coords_file, index=False)\n    # np.savetxt(all_cell_coords_file, grid_coords_np[:, 1:], delimiter=\",\")\n    # print(grid_coords_np.shape)\n\ndef get_latest_date_from_an_array(arr, date_format):\n    \"\"\"\n    Get the latest date from an array of date strings.\n    \n    Parameters:\n    - arr (list): List of date strings.\n    - date_format (str): Date format for parsing the date strings.\n    \n    Returns:\n    - str: The latest date string.\n    \"\"\"\n    return max(arr, key=lambda x: datetime.strptime(x, date_format))\n\ndef findLastStopDate(target_testing_dir, data_format):\n    \"\"\"\n    Find the last stop date from CSV files in a directory.\n    \n    Parameters:\n    - target_testing_dir (str): Directory containing CSV files.\n    - data_format (str): Date format for parsing the date strings.\n    \n    Returns:\n    - str: The latest stop date.\n    \"\"\"\n    date_list = []\n    for filename in os.listdir(target_testing_dir):\n        f = os.path.join(target_testing_dir, filename)\n        # checking if it is a file\n        if os.path.isfile(f) and \".csv\" in f:\n            pdf = pd.read_csv(f, header=0, index_col=0)\n            date_list = np.concatenate((date_list, pdf.index.unique()))\n    latest_date = get_latest_date_from_an_array(date_list, data_format)\n    print(latest_date)\n    date_time_obj = datetime.strptime(latest_date, data_format)\n    return date_time_obj.strftime(\"%Y-%m-%d\")\n\ndef convert_date_from_1900(day_value):\n    \"\"\"\n    Convert a day value since 1900 to a date string in the format \"YYYY-MM-DD\".\n    \n    Parameters:\n    - day_value (int): Number of days since January 1, 1900.\n    \n    Returns:\n    - str: Date string in \"YYYY-MM-DD\" format.\n    \"\"\"\n    reference_date = datetime(1900, 1, 1)\n    result_date = reference_date + timedelta(days=day_value)\n    return result_date.strftime(\"%Y-%m-%d\")\n\ndef convert_date_to_1900(date_string):\n    \"\"\"\n    Convert a date string in the format \"YYYY-MM-DD\" to a day value since 1900.\n    \n    Parameters:\n    - date_string (str): Date string in \"YYYY-MM-DD\" format.\n    \n    Returns:\n    - int: Number of days since January 1, 1900.\n    \"\"\"\n    input_date = datetime.strptime(date_string, \"%Y-%m-%d\")\n    reference_date = datetime(1900, 1, 1)\n    delta = input_date - reference_date\n    day_value = delta.days\n    return day_value\n\nday_index = convert_date_to_1900(test_start_date)\n#create_cell_location_csv()\n#findLastStopDate(f\"{github_dir}/data/sim_testing/gridmet/\", \"%Y-%m-%d %H:%M:%S\")\n#findLastStopDate(f\"{github_dir}/data/sat_testing/sentinel1/\", \"%Y-%m-%d %H:%M:%S\")\n#findLastStopDate(f\"{github_dir}/data/sat_testing/modis/\", \"%Y-%m-%d\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "wdh394",
  "name" : "model_create_kehan",
  "description" : null,
  "code" : "\nfrom BaseHole import *\n\nclass KehanModel(BaseHole):\n\t\n  def preprocessing():\n    pass  \n  \n  def train():\n    pass\n  \n  def test():\n    pass",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "p87wh1",
  "name" : "data_snotel_real_time",
  "description" : null,
  "code" : "from datetime import datetime\nfrom metloom.pointdata import SnotelPointData\n\n# First Python script in Geoweaver\nimport os\nimport urllib.request, urllib.error, urllib.parse\nimport sys\n\nprint(sys.path)\n\ntry:\n    from BeautifulSoup import BeautifulSoup\nexcept ImportError:\n    from bs4 import BeautifulSoup\n\nnohrsc_url_format_string = \"https://www.nohrsc.noaa.gov/nearest/index.html?city={lat}%2C{lon}&county=&l=5&u=e&y={year}&m={month}&d={day}\"\n\ntest_noaa_query_url = nohrsc_url_format_string.format(lat=40.05352381745094, lon=-106.04027196859343, year=2022, month=5, day=4)\n\nprint(test_noaa_query_url)\n\nresponse = urllib.request.urlopen(test_noaa_query_url)\nwebContent = response.read().decode('UTF-8')\n\nprint(webContent)\n\nparsed_html = BeautifulSoup(webContent)\nprint(parsed_html.body.find('div', attrs={'class':'container'}).text)\n\n# Example of using the SnotelPointData class\n# snotel_point = SnotelPointData(\"713:CO:SNTL\", \"MyStation\")\n# df = snotel_point.get_daily_data(\n#     datetime(2020, 1, 2), datetime(2020, 1, 20),\n#     [snotel_point.ALLOWED_VARIABLES.SWE]\n# )\n# print(df)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ilbqzg",
  "name" : "all_dependencies",
  "description" : null,
  "code" : "from sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport json\nimport pandas as pd\nimport ee\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport geopandas as gpd\nimport geojson\nimport numpy as np\nimport os.path\n\n#pd.set_option('display.max_columns', None)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "do86ae",
  "name" : "data_WUS_UCLA_SR",
  "description" : "python",
  "code" : "import os\n\nprint(\"get UCLA data and prepare it into csv\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gkhtc0",
  "name" : "data_nsidc_4km_swe",
  "description" : null,
  "code" : "# 2019 first https://nsidc.org/data/nsidc-0719/versions/1#anchor-1\n\n# TODO: change LAT LONG TO GRID CELL COORDS\n# TODO: adjust using grid cell geojson in data integration\n# TODO: adjust to make model validation working (model_train_validate)\n\n\"\"\"\nBroxton, P., X. Zeng, and N. Dawson. 2019. Daily 4 km Gridded SWE and Snow Depth from\nAssimilated In-Situ and Modeled Data over the Conterminous US, Version 1. 2019-2021.\nBoulder, Colorado USA. NASA National Snow and Ice Data Center Distributed Active Archive Center.\nhttps://doi.org/10.5067/0GGPB220EX6A. 11/02/2022.\n\nTo enable wget to directly download netcdf from NSIDC, use:\n\necho 'machine urs.earthdata.nasa.gov login <uid> password <password>' >> ~/.netrc\nchmod 0600 ~/.netrc\n\n\"\"\"\n\nfrom math import cos, asin, sqrt, radians\nimport pandas as pd\nimport numpy as np\nimport os.path\nimport netCDF4 as nc\nimport datetime\nimport geojson\nfrom sklearn import neighbors as sk\nimport sys\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\n\n# read grid cell\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\n# read grid cell\ngrid_cells = f\"{github_dir}/data/snowcast_provided/grid_cells.geojson\"\nstation_cell_mapper_file = f\"{github_dir}/data/ready_for_training/station_cell_mapping.csv\"\nstation_cell_mapper_df = pd.read_csv(station_cell_mapper_file)\n# open nsidc data file (netCDF)\n# crs, lat, lon, time, time_str, DEPTH, SWE, SWE_MASK\n# change to make it work\nend_year = 2019\n# https://daacdata.apps.nsidc.org/pub/DATASETS/nsidc0719_SWE_Snow_Depth_v1/4km_SWE_Depth_WY2019_v01.nc\nnsidc_data_file = f\"{homedir}/Documents/data/4km_SWE_Depth_WY{end_year}_v01.nc\"\nnsidc_data_ds = nc.Dataset(nsidc_data_file)\n\nprint(nsidc_data_ds)\nfor dim in nsidc_data_ds.dimensions.values():\n    print(dim)\nfor var in nsidc_data_ds.variables.values():\n    print(var)\n\n# dates based on Water Year 2019 (not normal year)\norg_name = 'nsidc'\nproduct_name = 'NSIDC'\nstart_date = '2018-10-01'\nend_date = '2019-09-30'\n\ndfolder = f\"{homedir}/Documents/GitHub/SnowCast/data/sim_training/{org_name}/\"\nif not os.path.exists(dfolder):\n    os.makedirs(dfolder)\n\n# Removes duplicate indices\nscmd = set(station_cell_mapper_df['cell_id'])\n\nlat = nsidc_data_ds.variables['lat'][:]\nlon = nsidc_data_ds.variables['lon'][:]\ndepth = nsidc_data_ds.variables['DEPTH']\nswe = nsidc_data_ds.variables['SWE']\ntime = nsidc_data_ds.variables['time']\ncolumns = ['Year', 'Month', 'Day', 'Lat', 'Lon', 'SWE', 'Depth']\n\nstart_date_dt = datetime.datetime.strptime(start_date, \"%Y-%m-%d\")\n# conversion factor so we can get days from 0-364 for array\ndays_1900_start = int((start_date_dt - datetime.datetime(1900,1,1)).days)\n\nall_cells_df = pd.DataFrame(columns=columns)\nind = 0\n\n\n# haversine formula\ndef coord_distance(lat1, lon1, lat2, lon2):\n    p = 0.017453292519943295\n    hav = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p)*cos(lat2*p) * (1-cos((lon2-lon1)*p)) / 2\n    return 12742 * asin(sqrt(hav))\n\n\n# inefficient and bad, don't use this\ndef find_nearest(find_lat, find_lng):\n    min_dist = 999999999\n    curr_min_lat_idx = 0\n    curr_min_lon_idx = 0\n\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng) < min_dist:\n                if depth[23, lat_idx, lon_idx] != '--':\n                    min_dist = coord_distance(lat[lat_idx], lon[lon_idx], find_lat, find_lng)\n                    curr_min_lat_idx = lat_idx\n                    curr_min_lon_idx = lon_idx\n\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\n# for generating the list of all valid lat long pairs\ndef gen_pairs():\n    temp = []\n    lat_len = len(lat)\n    lon_len = len(lon)\n    # iterate through lat and long to find closest val\n    for lat_idx in range(lat_len):\n        for lon_idx in range(lon_len):\n            if depth[23, lat_idx, lon_idx] != '--':\n                temp.append((lat[lat_idx], lon[lon_idx]))\n    temp = np.array(temp)\n    print(temp)\n    np.save(f\"{dfolder}/valid_pairs.npy\", temp)\n\n\n# use balltree to find closest neighbors, convert to radians first so the haversine thing works correctly\n# (that's why there's a separate rad thing)\ndef find_nearest_2(find_lat, find_lng):\n    ball_tree = sk.BallTree(lat_lon_pairs_rad, metric=\"haversine\")\n\n    dist, ind = ball_tree.query([(radians(find_lat), radians(find_lng))], return_distance=True)\n    print(dist)\n    print(ind)\n    print(lat_lon_pairs[ind])\n    curr_min_lat_idx = lat_lon_pairs[ind][0][0][0]\n    curr_min_lon_idx = lat_lon_pairs[ind][0][0][1]\n    return curr_min_lat_idx, curr_min_lon_idx\n\n\ndef turn_nsidc_nc_to_csv():\n    # generate valid pairs, or just load if they already exist\n    if not os.path.exists(f\"{dfolder}/valid_pairs.npy\"):\n        print(\"file doesn't exist, generating new\")\n        gen_pairs()\n    lat_lon_pairs = np.load(f\"{dfolder}/valid_pairs.npy\")\n    lat_lon_pairs_rad = np.array([[radians(x[0]), radians(x[1])] for x in lat_lon_pairs])\n\n    # comment out if bulk writing!!\n    # all_cells_df.to_csv(f\"{dfolder}/test.csv\", index=False)\n\n    for ind, current_cell_id in enumerate(scmd):\n        # comment out if bulk writing\n        # all_cells_df = pd.DataFrame(columns=columns)\n\n        # Location information\n        longitude = station_cell_mapper_df['lon'][ind]\n        latitude = station_cell_mapper_df['lat'][ind]\n\n    #     print(latitude)\n    #     print(longitude)\n\n        # find closest lat long\n        lat_val, lon_val = find_nearest_2(latitude, longitude)\n        lat_idx = np.where(lat == lat_val)[0]\n        lon_idx = np.where(lon == lon_val)[0]\n    #     print(lat_val)\n    #     print(lon_val)\n\n        depth_time = depth[:, lat_idx, lon_idx]\n        swe_time = swe[:, lat_idx, lon_idx]\n\n        for ele in time:\n            time_index = int(ele.data - days_1900_start)\n            time_index_dt = datetime.datetime(1900, 1, 1, 0, 0) + datetime.timedelta(int(ele.data))\n            depth_val = depth_time[time_index][0][0]\n            swe_val = swe_time[time_index][0][0]\n\n            all_cells_df.loc[len(all_cells_df.index)] = [time_index_dt.year, time_index_dt.month, time_index_dt.day, lat_val, lon_val, swe_val, depth_val]\n\n        # comment out if bulk writing\n        # all_cells_df.to_csv(f\"{dfolder}/test.csv\", mode='a', header=False, index=False)\n\n    # uncomment to bulk write at end of program\n    all_cells_df.to_csv(f\"{dfolder}/{end_year}nsidc_data.csv\")\n\n    print(\"finished\")\n\n# call this method to extract the \n#turn_nsidc_nc_to_csv()",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "lbd6cp",
  "name" : "model_creation_et",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the ETHole class, which is used for training and evaluating an Extra Trees Regressor model for hole analysis.\n\nAttributes:\n    ETHole (class): A class for training and using an Extra Trees Regressor model for hole analysis.\n\nFunctions:\n    custom_loss(y_true, y_pred): A custom loss function that penalizes errors for values greater than 10.\n    get_model(): Returns the Extra Trees Regressor model with specified hyperparameters.\n    create_sample_weights(y, scale_factor): Creates sample weights based on target values and a scaling factor.\n    preprocessing(): Preprocesses the training data, including data cleaning and feature extraction.\n    train(): Trains the Extra Trees Regressor model.\n    post_processing(): Performs post-processing, including feature importance analysis and visualization.\n\"\"\"\n\nimport pandas as pd\nimport joblib\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom model_creation_rf import RandomForestHole\nfrom snowcast_utils import work_dir, month_to_season\n\n\nworking_dir = work_dir\n\nclass ETHole(RandomForestHole):\n  \n    def custom_loss(y_true, y_pred):\n        \"\"\"\n        A custom loss function that penalizes errors for values greater than 10.\n\n        Args:\n            y_true (numpy.ndarray): True target values.\n            y_pred (numpy.ndarray): Predicted target values.\n\n        Returns:\n            numpy.ndarray: Custom loss values.\n        \"\"\"\n        errors = np.abs(y_true - y_pred)\n        \n        return np.where(y_true > 10, 2 * errors, errors)\n\n    def get_model(self):\n        \"\"\"\n        Returns the Extra Trees Regressor model with specified hyperparameters.\n\n        Returns:\n            ExtraTreesRegressor: The Extra Trees Regressor model.\n        \"\"\"\n        return ExtraTreesRegressor(n_estimators=200, \n                                   max_depth=None,\n                                   random_state=42, \n                                   min_samples_split=2,\n                                   min_samples_leaf=1,\n                                   n_jobs=5\n                                  )\n\n    def create_sample_weights(self, y, scale_factor, columns):\n        \"\"\"\n        Creates sample weights based on target values and a scaling factor.\n\n        Args:\n            y (numpy.ndarray): Target values.\n            scale_factor (float): Scaling factor for sample weights.\n\n        Returns:\n            numpy.ndarray: Sample weights.\n        \"\"\"\n        return (y - np.min(y)) / (np.max(y) - np.min(y)) * scale_factor\n        # Create a weight vector to assign weights to features - this is not a good idea\n#         feature_weights = {'date': 0.1, 'SWE': 1.5, 'wind_speed': 1.5, 'precipitation_amount': 2.0}\n#         default_weight = 1.0\n\n#         # Create an array of sample weights based on feature_weights\n#         sample_weights = np.array([feature_weights.get(feature, default_weight) for feature in columns])\n        #return sample_weights\n\n      \n    def preprocessing(self):\n        \"\"\"\n        Preprocesses the training data, including data cleaning and feature extraction.\n        \"\"\"\n        #training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned.csv'\n        training_data_path = f'{working_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n        print(\"preparing training data from csv: \", training_data_path)\n        data = pd.read_csv(training_data_path)\n        print(data.head())\n        \n        data['date'] = pd.to_datetime(data['date'])\n        #reference_date = pd.to_datetime('1900-01-01')\n        #data['date'] = (data['date'] - reference_date).dt.days\n        # just use julian day\n        #data['date'] = data['date'].dt.strftime('%j').astype(int)\n        # just use the season to reduce the bias on month or dates\n        data['date'] = data['date'].dt.month.apply(month_to_season)\n        \n        data.replace('--', pd.NA, inplace=True)\n        data.fillna(-999, inplace=True)\n        \n        data = data[(data['swe_value'] != -999)]\n        \n        print(\"get swe statistics\")\n        print(data[\"swe_value\"].describe())\n        \n        data = data.drop('Unnamed: 0', axis=1)\n        data = data.drop('level_0', axis=1)\n        #data = data.drop(['date'], axis=1)\n        \n#         (['lat', 'lon', 'SWE', 'Flag', 'air_temperature_tmmn',\n# 'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n# 'relative_humidity_rmax', 'relative_humidity_rmin',\n# 'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',\n# 'elevation', 'slope', 'curvature', 'aspect', 'eastness', 'northness']\n        \n        \n        X = data.drop('swe_value', axis=1)\n        print('required features:', X.columns)\n        y = data['swe_value']\n        \n        print(\"input features and order: \", X.columns)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        \n        self.weights = self.create_sample_weights(y_train, scale_factor=30.0, columns=X.columns)\n\n        self.train_x, self.train_y = X_train.to_numpy(), y_train.to_numpy()\n        self.test_x, self.test_y = X_test.to_numpy(), y_test.to_numpy()\n        self.feature_names = X_train.columns\n        \n    def train(self):\n        \"\"\"\n        Trains the Extra Trees Regressor model.\n        \"\"\"\n        self.classifier.fit(self.train_x, self.train_y, sample_weight=self.weights)\n\n    def post_processing(self):\n        \"\"\"\n        Performs post-processing, including feature importance analysis and visualization.\n        \"\"\"\n        feature_importances = self.classifier.feature_importances_\n        feature_names = self.feature_names\n        sorted_indices = np.argsort(feature_importances)[::-1]\n        sorted_importances = feature_importances[sorted_indices]\n        sorted_feature_names = feature_names[sorted_indices]\n\n        plt.figure(figsize=(10, 6))\n        plt.bar(range(len(feature_names)), sorted_importances, tick_label=sorted_feature_names)\n        plt.xticks(rotation=90)\n        plt.xlabel('Feature')\n        plt.ylabel('Feature Importance')\n        plt.title('Feature Importance Plot (ET model)')\n        plt.tight_layout()\n        feature_png = f'{work_dir}/testing_output/et-model-feature-importance-latest.png'\n        plt.savefig(feature_png)\n        print(f\"Feature image is saved {feature_png}\")\n\n# Instantiate ETHole class and perform tasks\nhole = ETHole()\nhole.preprocessing()\nhole.train()\nhole.test()\nhole.evaluate()\nhole.save()\nhole.post_processing()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "br9etb",
  "name" : "data_snotel_station_only",
  "description" : null,
  "code" : "import math\nimport json\nimport requests\nimport pandas as pd\nimport csv\nimport io\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\n\ndef read_json_file(file_path):\n    '''\n    Read and parse a JSON file.\n\n    Args:\n        file_path (str): The path to the JSON file.\n\n    Returns:\n        dict: The parsed JSON data.\n    '''\n    with open(file_path, 'r', encoding='utf-8-sig') as json_file:\n        data = json.load(json_file)\n        return data\n\ndef haversine(lat1, lon1, lat2, lon2):\n    '''\n    Calculate the Haversine distance between two sets of latitude and longitude coordinates.\n\n    Args:\n        lat1 (float): Latitude of the first point.\n        lon1 (float): Longitude of the first point.\n        lat2 (float): Latitude of the second point.\n        lon2 (float): Longitude of the second point.\n\n    Returns:\n        float: The Haversine distance between the two points in kilometers.\n    '''\n    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n    d_lat = lat2 - lat1\n    d_long = lon2 - lon1\n    a = math.sin(d_lat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(d_long / 2) ** 2\n    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n    distance = 6371 * c  # Earth's radius in kilometers\n    return distance\n\ndef find_nearest_location(locations, target_lat, target_lon):\n    '''\n    Find the nearest location in a list of locations to a target latitude and longitude.\n\n    Args:\n        locations (list): List of locations, each represented as a dictionary.\n        target_lat (float): Target latitude.\n        target_lon (float): Target longitude.\n\n    Returns:\n        dict: The nearest location from the list.\n    '''\n    n_location = None\n    min_distance = float('inf')\n    for location in locations:\n        lat = location['location']['lat']\n        lon = location['location']['lng']\n        distance = haversine(lat, lon, target_lat, target_lon)\n        if distance < min_distance:\n            min_distance = distance\n            n_location = location\n    return n_location\n\ndef csv_to_json(csv_text):\n    '''\n    Convert CSV text to JSON format.\n\n    Args:\n        csv_text (str): The CSV text to convert.\n\n    Returns:\n        str: The JSON representation of the CSV data.\n    '''\n    lines = csv_text.splitlines()\n    header = lines[0]\n    field_names = header.split(',')\n    json_list = []\n    for line in lines[1:]:\n        values = line.split(',')\n        row_dict = {}\n        for i, field_name in enumerate(field_names):\n            row_dict[field_name] = values[i]\n            json_list.append(row_dict)\n            json_string = json.dumps(json_list)\n            return json_string\n\ndef remove_commented_lines(text):\n    '''\n    Remove lines starting with '#' from the input text.\n\n    Args:\n        text (str): The input text.\n\n    Returns:\n        str: The input text with lines starting with '#' removed.\n    '''\n    lines = text.split(os.linesep)\n    cleaned_lines = []\n    for line in lines:\n        if not line.startswith('#'):\n            cleaned_lines.append(line)\n    cleaned_text = os.linesep.join(cleaned_lines)\n    return cleaned_text\n\ndef start_to_collect_snotel():\n    '''\n    Start the process of collecting SNOTEL data and saving it to a CSV file.\n    '''\n    csv_file = f'{working_dir}/training_data_ready_snotel_3_yrs.csv'\n    start_date = \"2019-01-01\"\n    end_date = \"2022-12-12\"\n\n    if os.path.exists(csv_file):\n        print(f\"The file '{csv_file}' exists.\")\n        return\n\n    station_mapping = pd.read_csv(f'{working_dir}/station_cell_mapping.csv')\n\n    result_df = pd.DataFrame(columns=['date', 'lat', 'lon', 'swe_value'])\n    for index, row in station_mapping.iterrows():\n        print(index, ' / ', len(station_mapping), ' iterations completed.')\n        station_locations = read_json_file(f'{working_dir}/snotelStations.json')\n        nearest_location = find_nearest_location(station_locations, row['lat'], row['lon'])\n\n        location_name = nearest_location['name']\n        location_triplet = nearest_location['triplet']\n        location_elevation = nearest_location['elevation']\n        location_station_lat = nearest_location['location']['lat']\n        location_station_long = nearest_location['location']['lng']\n\n        url = f\"https://wcc.sc.egov.usda.gov/reportGenerator/view_csv/customSingleStationReport/daily/{location_triplet}%7Cid%3D%22%22%7Cname/{start_date},{end_date}%2C0/WTEQ%3A%3Avalue%2CWTEQ%3A%3Adelta%2CSNWD%3A%3Avalue%2CSNWD%3A%3Adelta%2CTOBS%3A%3Avalue\"\n\n        r = requests.get(url)\n        text = remove_commented_lines(r.text)\n        reader = csv.DictReader(io.StringIO(text))\n        json_data = json.loads(json.dumps(list(reader)))\n        for entry in json_data:\n            required_data = {'date': entry['Date'], 'lat': row['lat'], 'lon': row['lon'],\n                             'swe_value': entry['Snow Water Equivalent (in) Start of Day Values']}\n            result_df.loc[len(result_df.index)] = required_data\n\n    # Save the DataFrame to a CSV file\n    result_df.to_csv(csv_file, index=False)\n\nstart_to_collect_snotel()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c2xkhz",
  "name" : "model_creation_rf",
  "description" : null,
  "code" : "\"\"\"\nThis script defines the RandomForestHole class, which is used for training and evaluating a Random Forest Regressor model for hole analysis.\n\nAttributes:\n    RandomForestHole (class): A class for training and using a Random Forest Regressor model for hole analysis.\n\nFunctions:\n    get_model(): Returns the Random Forest Regressor model with specified hyperparameters.\n    evaluate(): Evaluates the performance of the trained model and returns metrics such as MAE, MSE, R2, and RMSE.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom base_hole import BaseHole\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\n\nhomedir = os.path.expanduser('~')\ngithub_dir = os.path.join(homedir, 'Documents', 'GitHub', 'SnowCast')\n\nclass RandomForestHole(BaseHole):\n  \n    def get_model(self):\n        \"\"\"\n        Returns the Random Forest Regressor model with specified hyperparameters.\n\n        Returns:\n            Pipeline: The Random Forest Regressor model wrapped in a scikit-learn pipeline.\n        \"\"\"\n        rfc_pipeline = Pipeline(steps=[\n            ('data_scaling', StandardScaler()),\n            ('model', RandomForestRegressor(max_depth=15,\n                                           min_samples_leaf=0.004,\n                                           min_samples_split=0.008,\n                                           n_estimators=25))\n        ])\n        return rfc_pipeline\n\n    def evaluate(self):\n        \"\"\"\n        Evaluates the performance of the trained model and returns metrics such as MAE, MSE, R2, and RMSE.\n\n        Returns:\n            dict: A dictionary containing MAE, MSE, R2, and RMSE metrics.\n        \"\"\"\n        mae = metrics.mean_absolute_error(self.test_y, self.test_y_results)\n        mse = metrics.mean_squared_error(self.test_y, self.test_y_results)\n        r2 = metrics.r2_score(self.test_y, self.test_y_results)\n        rmse = math.sqrt(mse)\n\n        print(\"The random forest model performance for testing set\")\n        print(\"--------------------------------------\")\n        print('MAE is {}'.format(mae))\n        print('MSE is {}'.format(mse))\n        print('R2 score is {}'.format(r2))\n        print('RMSE is {}'.format(rmse))\n        return {\"mae\": mae, \"mse\": mse, \"r2\": r2, \"rmse\": rmse}\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "doinnd",
  "name" : "model_creation_pycaret",
  "description" : null,
  "code" : "import pandas as pd\nimport autokeras as ak\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Load the data from a CSV file\ndf = pd.read_csv('/home/chetana/gridmet_test_run/five_years_data.csv')\n\n# Remove rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize a label encoder\nlabel_encoder = LabelEncoder()\n\n# Drop unnecessary columns from the DataFrame\ndf.drop(df.filter(regex=\"Unname\"), axis=1, inplace=True)\ndf.drop('Date', inplace=True, axis=1)\ndf.drop('mapping_cell_id', inplace=True, axis=1)\ndf.drop('cell_id', inplace=True, axis=1)\ndf.drop('station_id', inplace=True, axis=1)\ndf.drop('mapping_station_id', inplace=True, axis=1)\ndf.drop('station_triplet', inplace=True, axis=1)\ndf.drop('station_name', inplace=True, axis=1)\n\n# Rename columns for better readability\ndf.rename(columns={\n    'Change In Snow Water Equivalent (in)': 'swe_change',\n    'Snow Depth (in) Start of Day Values': 'swe_value',\n    'Change In Snow Depth (in)': 'snow_depth_change',\n    'Air Temperature Observed (degF) Start of Day Values': 'snotel_air_temp',\n    'Elevation [m]': 'elevation',\n    'Aspect [deg]': 'aspect',\n    'Curvature [ratio]': 'curvature',\n    'Slope [deg]': 'slope',\n    'Eastness [unitCirc.]': 'eastness',\n    'Northness [unitCirc.]': 'northness'\n}, inplace=True)\n\n# Split the data into features (X) and target variable (y)\nX = df.drop(columns=['swe_value'])\ny = df['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the AutoKeras regressor\nreg = ak.StructuredDataRegressor(max_trials=10, overwrite=True)\nreg.fit(X_train, y_train, epochs=10)\n\n# Evaluate the AutoKeras regressor on the test set\npredictions = reg.predict(X_test)\n\n# Calculate and print evaluation metrics\nrmse = mean_squared_error(y_test, predictions, squared=False)\nr2 = r2_score(y_test, predictions)\nprint('RMSE:', rmse)\nprint('R2 Score:', r2)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "b7a4fu",
  "name" : "model_creation_autokeras",
  "description" : null,
  "code" : "import pandas as pd\nimport autokeras as ak\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import LabelEncoder\n\n# Read the data from the CSV file\nprint(\"start to read data\")\ndf = pd.read_csv('/home/chetana/gridmet_test_run/five_years_data.csv')\ndf.dropna(inplace=True)\n\n# Initialize a label encoder\nprint(\"create labelencoder\")\nlabel_encoder = LabelEncoder()\n\n# Drop unnecessary columns from the DataFrame\ndf.drop(df.filter(regex=\"Unname\"), axis=1, inplace=True)\ndf.drop('Date', inplace=True, axis=1)\ndf.drop('mapping_cell_id', inplace=True, axis=1)\ndf.drop('cell_id', inplace=True, axis=1)\ndf.drop('station_id', inplace=True, axis=1)\ndf.drop('mapping_station_id', inplace=True, axis=1)\ndf.drop('station_triplet', inplace=True, axis=1)\ndf.drop('station_name', inplace=True, axis=1)\n\n# Rename columns for clarity\nprint(\"rename columns\")\ndf.rename(columns={\n    'Change In Snow Water Equivalent (in)': 'swe_change',\n    'Snow Depth (in) Start of Day Values': 'swe_value',\n    'Change In Snow Depth (in)': 'snow_depth_change',\n    'Air Temperature Observed (degF) Start of Day Values': 'snotel_air_temp',\n    'Elevation [m]': 'elevation',\n    'Aspect [deg]': 'aspect',\n    'Curvature [ratio]': 'curvature',\n    'Slope [deg]': 'slope',\n    'Eastness [unitCirc.]': 'eastness',\n    'Northness [unitCirc.]': 'northness'\n}, inplace=True)\n\n# Split the data into features and target variable\nX = df.drop(columns=['swe_value'])\ny = df['swe_value']\n\n# Split the data into train and test sets\nprint(\"Split the data into train and test sets\")\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize and train the AutoKeras regressor\nreg = ak.StructuredDataRegressor(max_trials=10, overwrite=True)\nreg.fit(X_train, y_train, epochs=10)\n\n# Evaluate the AutoKeras regressor on the test set\nprint(\"Evaluate the AutoKeras regressor on the test set\")\npredictions = reg.predict(X_test)\nrmse = mean_squared_error(y_test, predictions, squared=False)\nr2 = r2_score(y_test, predictions)\n\n# Print the evaluation metrics\nprint('RMSE:', rmse)\nprint('R2 Score:', r2)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "gnpbdq",
  "name" : "model_creation_autopytorch",
  "description" : null,
  "code" : "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport autopytorch as apt\n\n# Load the data from a CSV file\ndf = pd.read_csv('/home/chetana/gridmet_test_run/five_years_data.csv')\n\n# Remove rows with missing values\ndf.dropna(inplace=True)\n\n# Initialize a label encoder\nlabel_encoder = LabelEncoder()\n\n# Drop unnecessary columns from the DataFrame\ndf.drop(df.filter(regex=\"Unname\"), axis=1, inplace=True)\ndf.drop('Date', inplace=True, axis=1)\ndf.drop('mapping_cell_id', inplace=True, axis=1)\ndf.drop('cell_id', inplace=True, axis=1)\ndf.drop('station_id', inplace=True, axis=1)\ndf.drop('mapping_station_id', inplace=True, axis=1)\ndf.drop('station_triplet', inplace=True, axis=1)\ndf.drop('station_name', inplace=True, axis=1)\n\n# Rename columns for better readability\ndf.rename(columns={\n    'Change In Snow Water Equivalent (in)': 'swe_change',\n    'Snow Depth (in) Start of Day Values': 'swe_value',\n    'Change In Snow Depth (in)': 'snow_depth_change',\n    'Air Temperature Observed (degF) Start of Day Values': 'snotel_air_temp',\n    'Elevation [m]': 'elevation',\n    'Aspect [deg]': 'aspect',\n    'Curvature [ratio]': 'curvature',\n    'Slope [deg]': 'slope',\n    'Eastness [unitCirc.]': 'eastness',\n    'Northness [unitCirc.]': 'northness'\n}, inplace=True)\n\n# Split the dataset into features (X) and target variable (y)\nX = df.drop('swe_value', axis=1)\ny = df['swe_value']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the Auto-PyTorch configuration\nconfig = apt.AutoNetRegressionConfig()\n\n# Initialize and train the Auto-PyTorch regressor\nreg = apt.AutoNetRegressor(config=config)\nreg.fit(X_train, y_train)\n\n# Evaluate the model\npredictions = reg.predict(X_test)\nrmse = mean_squared_error(y_test, predictions, squared=False)\nr2 = r2_score(y_test, predictions)\n\n# Print the evaluation metrics\nprint(\"RMSE:\", rmse)\nprint(\"R2 Score:\", r2)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "oon4sb",
  "name" : "western_us_dem.py",
  "description" : null,
  "code" : "import numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport warnings\nimport rasterio\nimport csv\nfrom rasterio.transform import Affine\nfrom scipy.ndimage import sobel, gaussian_filter\n\nmile_to_meters = 1609.34\n\n# Set the warning filter globally to ignore the FutureWarning\nwarnings.simplefilter(\"ignore\", FutureWarning)\n\ndef lat_lon_to_pixel(lat, lon, geotransform):\n    \"\"\"\n    Convert latitude and longitude to pixel coordinates using a geotransform.\n\n    Args:\n        lat (float): Latitude.\n        lon (float): Longitude.\n        geotransform (tuple): Geotransform coefficients (e.g., (origin_x, pixel_width, 0, origin_y, 0, pixel_height)).\n\n    Returns:\n        tuple: Pixel coordinates (x, y).\n    \"\"\"\n    x = int((lon - geotransform[0]) / geotransform[1])\n    y = int((lat - geotransform[3]) / geotransform[5])\n    return x, y\n\ndef calculate_slope_aspect_for_single(elevation_data, pixel_size_x, pixel_size_y):\n    \"\"\"\n    Calculate slope and aspect for a single pixel using elevation data.\n\n    Args:\n        elevation_data (array): Elevation data.\n        pixel_size_x (float): Pixel size in the x-direction.\n        pixel_size_y (float): Pixel size in the y-direction.\n\n    Returns:\n        tuple: Slope (in degrees), aspect (in degrees).\n    \"\"\"\n    # Calculate slope using the Sobel operator\n    slope_x = np.gradient(elevation_data, pixel_size_x, axis=1)\n    slope_y = np.gradient(elevation_data, pixel_size_y, axis=0)\n    slope_rad = np.arctan(np.sqrt(slope_x ** 2 + slope_y ** 2))\n    slope_deg = np.degrees(slope_rad)\n\n    # Calculate aspect (direction of the steepest descent)\n    aspect_rad = np.arctan2(slope_y, -slope_x)\n    aspect_deg = (np.degrees(aspect_rad) + 360) % 360\n\n    return slope_deg, aspect_deg\n\ndef save_as_geotiff(data, output_file, src_file):\n    \"\"\"\n    Save data as a GeoTIFF file with metadata from the source file.\n\n    Args:\n        data (array): Data to be saved.\n        output_file (str): Path to the output GeoTIFF file.\n        src_file (str): Path to the source GeoTIFF file to inherit metadata from.\n    \"\"\"\n    with rasterio.open(src_file) as src_dataset:\n        profile = src_dataset.profile\n        transform = src_dataset.transform\n\n        # Update the data type, count, and set the transform for the new dataset\n        profile.update(dtype=rasterio.float32, count=1, transform=transform)\n\n        # Create the new GeoTIFF file\n        with rasterio.open(output_file, 'w', **profile) as dst_dataset:\n            # Write the data to the new GeoTIFF\n            dst_dataset.write(data, 1)\n\ndef print_statistics(data):\n    \"\"\"\n    Print basic statistics of a data array.\n\n    Args:\n        data (array): Data array to calculate statistics for.\n    \"\"\"\n    # Calculate multiple statistics in one line\n    data = data[~np.isnan(data)]\n    mean, median, min_val, max_val, sum_val, std_dev, variance = [np.mean(data), np.median(data), np.min(data), np.max(data), np.sum(data), np.std(data), np.var(data)]\n\n    # Print the calculated statistics\n    print(\"Mean:\", mean)\n    print(\"Median:\", median)\n    print(\"Minimum:\", min_val)\n    print(\"Maximum:\", max_val)\n    print(\"Sum:\", sum_val)\n    print(\"Standard Deviation:\", std_dev)\n    print(\"Variance:\", variance)\n\ndef calculate_slope_aspect(dem_file):\n    \"\"\"\n    Calculate slope and aspect from a DEM (Digital Elevation Model) file.\n\n    Args:\n        dem_file (str): Path to the DEM GeoTIFF file.\n\n    Returns:\n        tuple: Slope array (in degrees), aspect array (in degrees).\n    \"\"\"\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Calculate the slope and aspect using numpy\n        dx, dy = np.gradient(dem_data, transform[0], transform[4])\n        slope = np.degrees(np.arctan(np.sqrt(dx**2 + dy**2)))\n        aspect = np.degrees(np.arctan2(-dy, dx))\n\n        # Adjust aspect values to range from 0 to 360 degrees\n        aspect[aspect < 0] += 360\n\n    return slope, aspect\n\ndef calculate_curvature(elevation_data, pixel_size_x, pixel_size_y):\n    \"\"\"\n    Calculate curvature from elevation data using the Laplacian operator.\n\n    Args:\n        elevation_data (array): Elevation data.\n        pixel_size_x (float): Pixel size in the x-direction.\n        pixel_size_y (float): Pixel size in the y-direction.\n\n    Returns:\n        array: Curvature data.\n    \"\"\"\n    # Calculate curvature using the Laplacian operator\n    curvature_x = np.gradient(np.gradient(elevation_data, pixel_size_x, axis=1), pixel_size_x, axis=1)\n    curvature_y = np.gradient(np.gradient(elevation_data, pixel_size_y, axis=0), pixel_size_y, axis=0)\n    curvature = curvature_x + curvature_y\n\n    return curvature\n\ndef calculate_gradients(dem_file):\n    \"\"\"\n    Calculate Northness and Eastness gradients from a DEM (Digital Elevation Model) file.\n\n    Args:\n        dem_file (str): Path to the DEM GeoTIFF file.\n\n    Returns:\n        tuple: Northness array, Eastness array (both in radians).\n    \"\"\"\n    with rasterio.open(dem_file) as dataset:\n        # Read the DEM data as a numpy array\n        dem_data = dataset.read(1)\n\n        # Calculate the gradients along the North and East directions\n        dy, dx = np.gradient(dem_data, dataset.res[0], dataset.res[1])\n\n        # Calculate the Northness and Eastness\n        northness = np.arctan(dy / np.sqrt(dx**2 + dy**2))\n        eastness = np.arctan(dx / np.sqrt(dx**2 + dy**2))\n\n    return northness, eastness\n\ndef geotiff_to_csv(geotiff_file, csv_file, column_name):\n    \"\"\"\n    Convert a GeoTIFF file to a CSV file containing latitude, longitude, and image values.\n\n    Args:\n        geotiff_file (str): Path to the input GeoTIFF file.\n        csv_file (str): Path to the output CSV file.\n        column_name (str): Name for the image value column.\n    \"\"\"\n    # Open the GeoTIFF file\n    with rasterio.open(geotiff_file) as dataset:\n        # Get the pixel values as a 2D array\n        data = dataset.read(1)\n\n        if column_name == \"Elevation\":\n            # Convert miles to meters (if applicable)\n            data = data * mile_to_meters\n\n        # Get the geotransform to convert pixel coordinates to geographic coordinates\n        transform = dataset.transform\n\n        # Get the width and height of the GeoTIFF\n        height, width = data.shape\n\n        # Open the CSV file for writing\n        with open(csv_file, 'w', newline='') as csvfile:\n            csvwriter = csv.writer(csvfile)\n\n            # Write the CSV header\n            csvwriter.writerow(['Latitude', 'Longitude', 'x', 'y', column_name])\n\n            # Loop through each pixel and extract latitude, longitude, and image value\n            for y in range(height):\n                for x in range(width):\n                    # Get the pixel value\n                    image_value = data[y, x]\n\n                    # Convert pixel coordinates to geographic coordinates\n                    lon, lat = transform * (x, y)\n\n                    # Write the data to the CSV file\n                    csvwriter.writerow([lat, lon, x, y, image_value])\n\ndef read_elevation_data(file_path, result_dem_csv_path, result_dem_feature_csv_path):\n    \"\"\"\n    Read and process elevation data from a CSV file and save it to another CSV file with additional features.\n\n    Args:\n        file_path (str): Path to the input CSV file containing elevation data.\n        result_dem_csv_path (str): Path to the output CSV file for elevation data.\n        result_dem_feature_csv_path (str): Path to the output CSV file for elevation data with additional features.\n\n    Returns:\n        DataFrame: Merged dataframe with elevation and additional features.\n    \"\"\"\n    neighborhood_size = 4\n    df = pd.read_csv(file_path)\n    \n    dataset = rasterio.open(geotiff_file)\n    data = dataset.read(1)\n\n    # Get the width and height of the GeoTIFF\n    height, width = data.shape\n    \n    # Create an empty DataFrame with column names\n    columns = ['lat', 'lon', 'elevation', 'slope', 'aspect', 'curvature', 'northness', 'eastness']\n    all_df = pd.DataFrame(columns=columns)\n    \n    all_df.to_csv(result_dem_feature_csv_path)\n    print(f\"DEM and other columns are saved to file {result_dem_feature_csv_path}\")\n    return all_df\n\n# Usage example:\nresult_dem_csv_path = \"/home/chetana/gridmet_test_run/dem_template.csv\"\nresult_dem_feature_csv_path = \"/home/chetana/gridmet_test_run/dem_all.csv\"\n\ndem_file = \"/home/chetana/gridmet_test_run/dem_file.tif\"\nslope_file = '/home/chetana/gridmet_test_run/slope_file.tif'\naspect_file = '/home/chetana/gridmet_test_run/aspect_file.tif'\ncurvature_file = '/home/chetana/gridmet_test_run/curvature_file.tif'\nnorthness_file = '/home/chetana/gridmet_test_run/northness_file.tif'\neastness_file = '/home/chetana/gridmet_test_run/eastness_file.tif'\n\nslope, aspect = calculate_slope_aspect(dem_file)\ncurvature = calculate_curvature(dem_file)\nnorthness, eastness = calculate_gradients(dem_file)\n\n# Save the slope and aspect as new GeoTIFF files\nsave_as_geotiff(slope, slope_file, dem_file)\nsave_as_geotiff(aspect, aspect_file, dem_file)\nsave_as_geotiff(curvature, curvature_file, dem_file)\nsave_as_geotiff(northness, northness_file, dem_file)\nsave_as_geotiff(eastness, eastness_file, dem_file)\n\ngeotiff_to_csv(dem_file, dem_file+\".csv\", \"Elevation\")\ngeotiff_to_csv(slope_file, slope_file+\".csv\", \"Slope\")\ngeotiff_to_csv(aspect_file, aspect_file+\".csv\", \"Aspect\")\ngeotiff_to_csv(curvature_file, curvature_file+\".csv\", \"Curvature\")\ngeotiff_to_csv(northness_file, northness_file+\".csv\", \"Northness\")\ngeotiff_to_csv(eastness_file, eastness_file+\".csv\", \"Eastness\")\n\n# List of file paths for the CSV files\ncsv_files = [dem_file+\".csv\", slope_file+\".csv\", aspect_file+\".csv\", \n             curvature_file+\".csv\", northness_file+\".csv\", eastness_file+\".csv\"]\n\n# Initialize an empty list to store all dataframes\ndfs = []\n\n# Read each CSV file into separate dataframes\nfor file in csv_files:\n    df = pd.read_csv(file, encoding='utf-8')\n    dfs.append(df)\n\n# Merge the dataframes based on the latitude and longitude columns\nmerged_df = dfs[0]  # Start with the first dataframe\nfor i in range(1, len(dfs)):\n    merged_df = pd.merge(merged_df, dfs[i], on=['Latitude', 'Longitude', 'x', 'y'])\n\n# check the statistics of the columns\nfor column in merged_df.columns:\n    merged_df[column] = pd.to_numeric(merged_df[column], errors='coerce')\n    print(merged_df[column].describe())\n    \n# Save the merged dataframe to a new CSV file\nmerged_df.to_csv(result_dem_feature_csv_path, index=False)\nprint(f\"New dem features are updated in {result_dem_feature_csv_path}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "fa7e4u",
  "name" : "download_srtm_1arcsec (caution!)",
  "description" : null,
  "code" : "#!/bin/bash\n\nGREP_OPTIONS=''\n\ncookiejar=$(mktemp cookies.XXXXXXXXXX)\nnetrc=$(mktemp netrc.XXXXXXXXXX)\nchmod 0600 \"$cookiejar\" \"$netrc\"\nfunction finish {\n  rm -rf \"$cookiejar\" \"$netrc\"\n}\n\ntrap finish EXIT\nWGETRC=\"$wgetrc\"\n\nprompt_credentials() {\n    echo \"Enter your Earthdata Login or other provider supplied credentials\"\n    read -p \"Username (jensengmu): \" username\n    username=${username:-jensengmu}\n    read -s -p \"Password: \" password\n    echo \"machine urs.earthdata.nasa.gov login $username password $password\" >> $netrc\n    echo\n}\n\nexit_with_error() {\n    echo\n    echo \"Unable to Retrieve Data\"\n    echo\n    echo $1\n    echo\n    echo \"https://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip\"\n    echo\n    exit 1\n}\n\nprompt_credentials\n  detect_app_approval() {\n    approved=`curl -s -b \"$cookiejar\" -c \"$cookiejar\" -L --max-redirs 5 --netrc-file \"$netrc\" https://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip -w '\\n%{http_code}' | tail  -1`\n    if [ \"$approved\" -ne \"200\" ] && [ \"$approved\" -ne \"301\" ] && [ \"$approved\" -ne \"302\" ]; then\n        # User didn't approve the app. Direct users to approve the app in URS\n        exit_with_error \"Please ensure that you have authorized the remote application by visiting the link below \"\n    fi\n}\n\nsetup_auth_curl() {\n    # Firstly, check if it require URS authentication\n    status=$(curl -s -z \"$(date)\" -w '\\n%{http_code}' https://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip | tail -1)\n    if [[ \"$status\" -ne \"200\" && \"$status\" -ne \"304\" ]]; then\n        # URS authentication is required. Now further check if the application/remote service is approved.\n        detect_app_approval\n    fi\n}\n\nsetup_auth_wget() {\n    # The safest way to auth via curl is netrc. Note: there's no checking or feedback\n    # if login is unsuccessful\n    touch ~/.netrc\n    chmod 0600 ~/.netrc\n    credentials=$(grep 'machine urs.earthdata.nasa.gov' ~/.netrc)\n    if [ -z \"$credentials\" ]; then\n        cat \"$netrc\" >> ~/.netrc\n    fi\n}\n\nfetch_urls() {\n  if command -v curl >/dev/null 2>&1; then\n      setup_auth_curl\n      while read -r line; do\n        # Get everything after the last '/'\n        filename=\"${line##*/}\"\n\n        # Strip everything after '?'\n        stripped_query_params=\"${filename%%\\?*}\"\n\n        curl -f -b \"$cookiejar\" -c \"$cookiejar\" -L --netrc-file \"$netrc\" -g -o $stripped_query_params -- $line && echo || exit_with_error \"Command failed with error. Please retrieve the data manually.\"\n      done;\n  elif command -v wget >/dev/null 2>&1; then\n      # We can't use wget to poke provider server to get info whether or not URS was integrated without download at least one of the files.\n      echo\n      echo \"WARNING: Can't find curl, use wget instead.\"\n      echo \"WARNING: Script may not correctly identify Earthdata Login integrations.\"\n      echo\n      setup_auth_wget\n      while read -r line; do\n        # Get everything after the last '/'\n        filename=\"${line##*/}\"\n\n        # Strip everything after '?'\n        stripped_query_params=\"${filename%%\\?*}\"\n\n        wget --load-cookies \"$cookiejar\" --save-cookies \"$cookiejar\" --output-document $stripped_query_params --keep-session-cookies -- $line && echo || exit_with_error \"Command failed with error. Please retrieve the data manually.\"\n      done;\n  else\n      exit_with_error \"Error: Could not find a command-line downloader.  Please install curl or wget\"\n  fi\n}\n\nfetch_urls <<'EDSCEOF'\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E072.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N35E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E050.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N37W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E057.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S33E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E059.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E071.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S24E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S35E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S19E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32W018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E071.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E045.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S08E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E042.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N40W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S24E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N04E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S24E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E049.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N33E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S19E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N04E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N35W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E044.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S17E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E040.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E063.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S15E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E041.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N10E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S32E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N35E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S16E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N04E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N03E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S14E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N13E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15W016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N30E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E038.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N36W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N27W019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S05E055.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N07E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E046.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S23E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S22E043.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N23E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N31E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S03E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E033.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S08E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N32W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S28E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W017.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S13E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S08E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21W010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E047.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N22W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E014.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E022.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N21E005.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S31E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14W011.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E015.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N19W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12W008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N01E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S07E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S02E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S21E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E035.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E032.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S10E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E007.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E048.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S18E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N12E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N29E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N28E012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N26E010.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N24E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39W003.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S06E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E021.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S12E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E028.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N14E024.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20E016.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N40W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N41E002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S01E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E034.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S11E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N06E020.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08W002.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N08E037.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18W006.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W012.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S29E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N18E019.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N17E018.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N02E039.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S26E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E023.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S27E030.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N11E000.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S04E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N09E027.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N40E013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S25E025.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S09E036.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N16W013.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N00E031.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N20W009.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N38W029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N05W001.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/S20E029.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N15E008.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N34W004.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N25E026.SRTMGL1.hgt.zip\nhttps://e4ftl01.cr.usgs.gov//DP133/SRTM/SRTMGL1.003/2000.02.11/N39E015.SRTMGL1.hgt.zip\nEDSCEOF\n\n\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "drwmbo",
  "name" : "gridmet_testing",
  "description" : null,
  "code" : "\"\"\"\nScript for downloading specific variables of GridMET climatology data.\n\nThis script downloads specific meteorological variables from the GridMET climatology dataset\nfor a specified year. It uses the netCDF4 library for handling NetCDF files, urllib for downloading files,\nand pandas for data manipulation. The script also removes existing files in the target folder before downloading.\n\n\nUsage:\n    Run this script to download specific meteorological variables for a specified year from the GridMET dataset.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport netCDF4 as nc\nimport urllib.request\nfrom datetime import datetime, timedelta, date\nfrom snowcast_utils import test_start_date, work_dir\nimport matplotlib.pyplot as plt\n\n# Define the folder to store downloaded files\ngridmet_folder_name = f'{work_dir}/gridmet_climatology'\n\nwestern_us_coords = f'{work_dir}/dem_file.tif.csv'\n\n\ngridmet_var_mapping = {\n  \"etr\": \"potential_evapotranspiration\",\n  \"pr\":\"precipitation_amount\",\n  \"rmax\":\"relative_humidity\",\n  \"rmin\":\"relative_humidity\",\n  \"tmmn\":\"air_temperature\",\n  \"tmmx\":\"air_temperature\",\n  \"vpd\":\"mean_vapor_pressure_deficit\",\n  \"vs\":\"wind_speed\",\n}\n# Define the custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\n# Define your value ranges for color mapping\n#value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n#value_ranges = [0.1, 0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.5, 1.8, 2, 2.5, 3]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n  if value_ranges == None:\n    max_value = df_col.max()\n    min_value = df_col.min()\n    if min_value < 0:\n      min_value = 0\n    step_size = (max_value - min_value) / 12\n\n    # Create 10 periods\n    new_value_ranges = [min_value + i * step_size for i in range(12)]\n  # Define your custom function to map data values to colors\n  def map_value_to_color(value):\n    # Iterate through the value ranges to find the appropriate color index\n    for i, range_max in enumerate(new_value_ranges):\n      if value <= range_max:\n        return colors[i]\n\n      # If the value is greater than the largest range, return the last color\n      return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n  color_mapping = [map_value_to_color(value) for value in df_col.values]\n  return color_mapping, new_value_ranges\n\ndef get_current_year():\n    \"\"\"\n    Get the current year.\n\n    Returns:\n        int: The current year.\n    \"\"\"\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\ndef remove_files_in_folder(folder_path):\n    \"\"\"\n    Remove all files in a specified folder.\n\n    Parameters:\n        folder_path (str): Path to the folder to remove files from.\n    \"\"\"\n    # Get a list of files in the folder\n    files = os.listdir(folder_path)\n\n    # Loop through the files and remove them\n    for file in files:\n        file_path = os.path.join(folder_path, file)\n        if os.path.isfile(file_path):\n            os.remove(file_path)\n            print(f\"Deleted file: {file_path}\")\n\ndef download_file(url, target_file_path, variable):\n    \"\"\"\n    Download a file from a URL and save it to a specified location.\n\n    Parameters:\n        url (str): URL of the file to download.\n        target_file_path (str): Path where the downloaded file should be saved.\n        variable (str): Name of the meteorological variable being downloaded.\n    \"\"\"\n    try:\n        with urllib.request.urlopen(url) as response:\n            print(f\"Downloading {url}\")\n            file_content = response.read()\n        save_path = target_file_path\n        with open(save_path, 'wb') as file:\n            file.write(file_content)\n        print(f\"File downloaded successfully and saved as: {save_path}\")\n    except Exception as e:\n        print(f\"An error occurred while downloading the file: {str(e)}\")\n\ndef download_gridmet_of_specific_variables(year_list):\n    \"\"\"\n    Download specific meteorological variables from the GridMET climatology dataset.\n    \"\"\"\n    # Make a directory to store the downloaded files\n    \n\n    base_metadata_url = \"http://www.northwestknowledge.net/metdata/data/\"\n    variables_list = ['tmmn', 'tmmx', 'pr', 'vpd', 'etr', 'rmax', 'rmin', 'vs']\n\n    for var in variables_list:\n        for y in year_list:\n            download_link = base_metadata_url + var + '_' + '%s' % y + '.nc'\n            target_file_path = os.path.join(gridmet_folder_name, var + '_' + '%s' % y + '.nc')\n            if not os.path.exists(target_file_path):\n                download_file(download_link, target_file_path, var)\n            else:\n                print(f\"File {target_file_path} exists\")\n\n\ndef get_current_year():\n    now = datetime.now()\n    current_year = now.year\n    return current_year\n\n\ndef get_file_name_from_path(file_path):\n    # Get the file name from the file path\n    file_name = os.path.basename(file_path)\n    return file_name\n\ndef get_var_from_file_name(file_name):\n    # Assuming the file name format is \"tmmm_year.csv\"\n    var_name = str(file_name.split('_')[0])\n    return var_name\n\ndef get_coordinates_of_template_tif():\n  \t# Load the CSV file and extract coordinates\n    coordinates = []\n    df = pd.read_csv(dem_csv)\n    for index, row in df.iterrows():\n        # Process each row here\n        lon, lat = float(row[\"Latitude\"]), float(row[\"Longitude\"])\n        coordinates.append((lon, lat))\n    return coordinates\n\ndef find_nearest_index(array, value):\n    # Find the index of the element in the array that is closest to the given value\n    return (abs(array - value)).argmin()\n\ndef create_gridmet_to_dem_mapper(nc_file):\n    western_us_dem_df = pd.read_csv(western_us_coords)\n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/gridmet_to_dem_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        gridmet_lat_index = find_nearest_index(latitudes, float(row[\"Latitude\"]))\n        gridmet_lon_index = find_nearest_index(longitudes, float(row[\"Longitude\"]))\n        return latitudes[gridmet_lat_index], longitudes[gridmet_lon_index], gridmet_lat_index, gridmet_lon_index\n    \n      # Use the apply function to apply the custom function to each row\n      western_us_dem_df[['gridmet_lat', 'gridmet_lon', \n                         'gridmet_lat_idx', 'gridmet_lon_idx',]] = western_us_dem_df.apply(lambda row: pd.Series(get_gridmet_var_value(row)), axis=1)\n      western_us_dem_df.rename(columns={\"Latitude\": \"dem_lat\", \n                                        \"Longitude\": \"dem_lon\"}, inplace=True)\n      \n    print(western_us_dem_df.head())\n    \n    # Save the new converted AMSR to CSV file\n    western_us_dem_df.to_csv(target_csv_path, index=False)\n    \n    return western_us_dem_df\n  \n  \ndef get_nc_csv_by_coords_and_variable(nc_file,\n                                      var_name,\n                                      test_start_date):\n    \n    create_gridmet_to_dem_mapper(nc_file)\n  \n    mapper_df = pd.read_csv(f'{work_dir}/gridmet_to_dem_mapper.csv')\n    \n    # get the netcdf file and generate the csv file for every coordinate in the dem_template.csv\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    # Read the NetCDF file\n    with nc.Dataset(nc_file) as nc_file:\n      # Get a list of all variables in the NetCDF file\n      variables = nc_file.variables.keys()\n      \n      # Get the values at each coordinate using rasterio's sample function\n      latitudes = nc_file.variables['lat'][:]\n      longitudes = nc_file.variables['lon'][:]\n      day = nc_file.variables['day'][:]\n      long_var_name = gridmet_var_mapping[var_name]\n      var_col = nc_file.variables[long_var_name][:]\n      print(\"val_col.shape: \", var_col.shape)\n      \n      # Calculate the day of the year\n      day_of_year = selected_date.timetuple().tm_yday\n      day_index = day_of_year - 1\n      print('day_index:', day_index)\n      \n      def get_gridmet_var_value(row):\n        # Perform your custom calculation here\n        lat_index = int(row[\"gridmet_lat_idx\"])\n        lon_index = int(row[\"gridmet_lon_idx\"])\n        var_value = var_col[day_index, lat_index, lon_index]\n        \n        return var_value\n    \n      # Use the apply function to apply the custom function to each row\n      print(mapper_df.columns)\n      print(mapper_df.head())\n      mapper_df[var_name] = mapper_df.apply(get_gridmet_var_value, axis=1)\n      \n      print(\"mapper_df[var_name]: \", mapper_df[var_name].describe())\n      \n      # drop useless columns\n      mapper_df = mapper_df[[\"dem_lat\", \"dem_lon\", var_name]]\n      mapper_df.rename(columns={\"dem_lat\": \"Latitude\",\n                               \"dem_lon\": \"Longitude\"}, inplace=True)\n\n      \n    print(mapper_df.head())\n    return mapper_df\n\n\ndef turn_gridmet_nc_to_csv():\n    \n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    for root, dirs, files in os.walk(gridmet_folder_name):\n        for file_name in files:\n            \n            if str(selected_date.year) in file_name and file_name.endswith(\".nc\"):\n                print(f\"Checking file: {file_name}\")\n                var_name = get_var_from_file_name(file_name)\n                print(\"Variable name:\", var_name)\n                res_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{test_start_date}.csv\"\n\n                if os.path.exists(res_csv):\n                    #os.remove(res_csv)\n                    print(f\"{res_csv} already exists. Skipping..\")\n                    continue\n\n                # Perform operations on each file here\n                netcdf_file_path = os.path.join(root, file_name)\n                print(\"Processing file:\", netcdf_file_path)\n                file_name = get_file_name_from_path(netcdf_file_path)\n\n                df = get_nc_csv_by_coords_and_variable(netcdf_file_path, \n                                                       var_name, test_start_date)\n                df.replace('--', pd.NA, inplace=True)\n                df.to_csv(res_csv, index=False)\n                print(\"gridmet var saved: \", res_csv)\n                \n\ndef plot_gridmet():\n  selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  var_name = \"pr\"\n  test_csv = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{test_start_date}.csv\"\n  gridmet_var_df = pd.read_csv(test_csv)\n  gridmet_var_df.replace('--', pd.NA, inplace=True)\n  gridmet_var_df.dropna(inplace=True)\n  gridmet_var_df['pr'] = pd.to_numeric(gridmet_var_df['pr'], errors='coerce')\n  print(gridmet_var_df.head())\n  #print(gridmet_var_df[\"Latitude\"].describe())\n  #print(gridmet_var_df[\"Longitude\"].describe())\n  print(gridmet_var_df[\"pr\"].describe())\n  \n  colormaplist, value_ranges = create_color_maps_with_value_range(gridmet_var_df[var_name])\n  \n  # Create a scatter plot\n  plt.scatter(gridmet_var_df[\"Longitude\"].values, \n              gridmet_var_df[\"Latitude\"].values, \n              label='Pressure', \n              color=colormaplist, \n              marker='o')\n\n  # Add labels and a legend\n  plt.xlabel('X-axis')\n  plt.ylabel('Y-axis')\n  plt.title('Scatter Plot Example')\n  plt.legend()\n  \n  res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{test_start_date}.png\"\n  plt.savefig(res_png_path)\n  print(f\"test image is saved at {res_png_path}\")\n                \ndef prepare_folder_and_get_year_list():\n  # Check if the folder exists, if not, create it\n  if not os.path.exists(gridmet_folder_name):\n      os.makedirs(gridmet_folder_name)\n\n  selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n  year_list = [selected_date.year]\n\n  # Remove any existing files in the folder\n  if selected_date.year == datetime.now().year:\n      remove_files_in_folder(gridmet_folder_name)  # only redownload when the year is the current year\n  return year_list\n\n# Run the download function\ndownload_gridmet_of_specific_variables(prepare_folder_and_get_year_list())\nturn_gridmet_nc_to_csv()\nplot_gridmet()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2n7b06",
  "name" : "create_output_tif_template",
  "description" : null,
  "code" : "import os\nimport rasterio\nfrom rasterio.transform import from_origin\nimport numpy as np\n\ndef create_western_us_geotiff():\n    \"\"\"\n    Create a GeoTIFF template for the western U.S. region with specified spatial extent and resolution.\n    The resulting GeoTIFF file will contain an empty 2D array with a single band.\n    \"\"\"\n    # Define the spatial extent of the western U.S. (minx, miny, maxx, maxy)\n    minx, miny, maxx, maxy = -125, 25, -100, 49\n\n    # Define the resolution in degrees (4km is approximately 0.036 degrees)\n    resolution = 0.036\n\n    # Calculate the image size (width and height in pixels) based on the spatial extent and resolution\n    width = int((maxx - minx) / resolution)\n    height = int((maxy - miny) / resolution)\n\n    # Create an empty 2D NumPy array with a single band to store the image data\n    data = np.zeros((height, width), dtype=np.float32)\n    \n    # Read the user's home directory\n    homedir = os.path.expanduser('~')\n    print(homedir)\n\n    # Define the output filename\n    output_filename = f\"{homedir}/western_us_geotiff_template.tif\"\n\n    # Create the GeoTIFF file and specify the metadata\n    with rasterio.open(\n        output_filename,\n        'w',\n        driver='GTiff',\n        height=height,\n        width=width,\n        count=1,  # Single band\n        dtype=np.float32,\n        crs='EPSG:4326',  # WGS84\n        transform=from_origin(minx, maxy, resolution, resolution),\n    ) as dst:\n        # Write the data to the raster\n        dst.write(data, 1)\n\nif __name__ == \"__main__\":\n    create_western_us_geotiff()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "bwdy3s",
  "name" : "resample_dem",
  "description" : null,
  "code" : "#!/bin/bash\n# this script will reproject and resample the western US dem, clip it, to match the exact spatial extent and resolution as the template tif\n\ncd /home/chetana/gridmet_test_run\n\nmkdir template_shp/\n\ncp /home/chetana/western_us_geotiff_template.tif template_shp/\n\n# generate the template shape\ngdaltindex template.shp template_shp/*.tif\n\ngdalwarp -s_srs EPSG:4326 -t_srs EPSG:4326 -tr 0.036 0.036  -cutline template.shp -crop_to_cutline -overwrite output_4km.tif output_4km_clipped.tif\n    \ngdalinfo output_4km_clipped.tif\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2wkl6e",
  "name" : "convert_results_to_images",
  "description" : null,
  "code" : "import pandas as pd\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.basemap import Basemap\nfrom datetime import timedelta, datetime\nimport numpy as np\nimport uuid\nfrom snowcast_utils import day_index\nimport matplotlib.colors as mcolors\n\n# Import utility functions and variables from 'snowcast_utils'\nfrom snowcast_utils import work_dir, test_start_date\n\n# Define a custom colormap with specified colors and ranges\ncolors = [\n    (0.8627, 0.8627, 0.8627),  # #DCDCDC - 0 - 1\n    (0.8627, 1.0000, 1.0000),  # #DCFFFF - 1 - 2\n    (0.6000, 1.0000, 1.0000),  # #99FFFF - 2 - 4\n    (0.5569, 0.8235, 1.0000),  # #8ED2FF - 4 - 6\n    (0.4509, 0.6196, 0.8745),  # #739EDF - 6 - 8\n    (0.4157, 0.4706, 1.0000),  # #6A78FF - 8 - 10\n    (0.4235, 0.2784, 1.0000),  # #6C47FF - 10 - 12\n    (0.5529, 0.0980, 1.0000),  # #8D19FF - 12 - 14\n    (0.7333, 0.0000, 0.9176),  # #BB00EA - 14 - 16\n    (0.8392, 0.0000, 0.7490),  # #D600BF - 16 - 18\n    (0.7569, 0.0039, 0.4549),  # #C10074 - 18 - 20\n    (0.6784, 0.0000, 0.1961),  # #AD0032 - 20 - 30\n    (0.5020, 0.0000, 0.0000)   # #800000 - > 30\n]\n\ncmap_name = 'custom_snow_colormap'\ncustom_cmap = mcolors.ListedColormap(colors)\n\n# Define the lat_lon_to_map_coordinates function\ndef lat_lon_to_map_coordinates(lon, lat, m):\n    \"\"\"\n    Convert latitude and longitude coordinates to map coordinates.\n\n    Args:\n        lon (float or array-like): Longitude coordinate(s).\n        lat (float or array-like): Latitude coordinate(s).\n        m (Basemap): Basemap object representing the map projection.\n\n    Returns:\n        tuple: Tuple containing the converted map coordinates (x, y).\n    \"\"\"\n    x, y = m(lon, lat)\n    return x, y\n\n# Define value ranges for color mapping\nfixed_value_ranges = [1, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30]\n\ndef create_color_maps_with_value_range(df_col, value_ranges=None):\n    \"\"\"\n    Create a colormap for value ranges and map data values to colors.\n\n    Args:\n        df_col (pd.Series): A Pandas Series containing data values.\n        value_ranges (list, optional): A list of value ranges for color mapping.\n            If not provided, the ranges will be determined automatically.\n\n    Returns:\n        tuple: Tuple containing the color mapping and the updated value ranges.\n    \"\"\"\n    new_value_ranges = value_ranges\n    if value_ranges is None:\n        max_value = df_col.max()\n        min_value = df_col.min()\n        if min_value < 0:\n            min_value = 0\n        step_size = (max_value - min_value) / 12\n\n        # Create 10 periods\n        new_value_ranges = [min_value + i * step_size for i in range(12)]\n    \n    print(\"new_value_ranges: \", new_value_ranges)\n  \n    # Define a custom function to map data values to colors\n    def map_value_to_color(value):\n        # Iterate through the value ranges to find the appropriate color index\n        for i, range_max in enumerate(new_value_ranges):\n            if value <= range_max:\n                return colors[i]\n\n        # If the value is greater than the largest range, return the last color\n        return colors[-1]\n\n    # Map predicted_swe values to colors using the custom function\n    color_mapping = [map_value_to_color(value) for value in df_col.values]\n    return color_mapping, new_value_ranges\n\ndef convert_csvs_to_images():\n    \"\"\"\n    Convert CSV data to images with color-coded SWE predictions.\n\n    Returns:\n        None\n    \"\"\"\n    global fixed_value_ranges\n    data = pd.read_csv(\"/home/chetana/gridmet_test_run/test_data_predicted.csv\")\n    print(\"statistic of predicted_swe: \", data['predicted_swe'].describe())\n    data['predicted_swe'].fillna(0, inplace=True)\n    \n    for column in data.columns:\n        column_data = data[column]\n        print(column_data.describe())\n    \n    # Create a figure with a white background\n    fig = plt.figure(facecolor='white')\n\n    lon_min, lon_max = -125, -100\n    lat_min, lat_max = 25, 49.5\n\n    m = Basemap(llcrnrlon=lon_min, llcrnrlat=lat_min, urcrnrlon=lon_max, urcrnrlat=lat_max,\n                projection='merc', resolution='i')\n\n    x, y = m(data['lon'].values, data['lat'].values)\n    print(data.columns)\n\n    color_mapping, value_ranges = create_color_maps_with_value_range(data[\"predicted_swe\"], fixed_value_ranges)\n    \n    # Plot the data using the custom colormap\n    plt.scatter(x, y, c=color_mapping, cmap=custom_cmap, s=30, edgecolors='none', alpha=0.7)\n    \n    # Draw coastlines and other map features\n    m.drawcoastlines()\n    m.drawcountries()\n    m.drawstates()\n\n    reference_date = datetime(1900, 1, 1)\n    day_value = day_index\n    \n    result_date = reference_date + timedelta(days=day_value)\n    today = result_date.strftime(\"%Y-%m-%d\")\n    timestamp_string = result_date.strftime(\"%Y-%m-%d\")\n    \n    # Add a title\n    plt.title(f'Predicted SWE in the Western US - {today}', pad=20)\n\n    # Add labels for latitude and longitude on x and y axes with smaller font size\n    plt.xlabel('Longitude', fontsize=6)\n    plt.ylabel('Latitude', fontsize=6)\n\n    # Add longitude values to the x-axis and adjust font size\n    x_ticks_labels = np.arange(lon_min, lon_max + 5, 5)\n    x_tick_labels_str = [f\"{lon:.1f}°W\" if lon < 0 else f\"{lon:.1f}°E\" for lon in x_ticks_labels]\n    plt.xticks(*m(x_ticks_labels, [lat_min] * len(x_ticks_labels)), fontsize=6)\n    plt.gca().set_xticklabels(x_tick_labels_str)\n\n    # Add latitude values to the y-axis and adjust font size\n    y_ticks_labels = np.arange(lat_min, lat_max + 5, 5)\n    y_tick_labels_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_ticks_labels]\n    plt.yticks(*m([lon_min] * len(y_ticks_labels), y_ticks_labels), fontsize=6)\n    plt.gca().set_yticklabels(y_tick_labels_str)\n\n    # Convert map coordinates to latitude and longitude for y-axis labels\n    y_tick_positions = np.linspace(lat_min, lat_max, len(y_ticks_labels))\n    y_tick_positions_map_x, y_tick_positions_map_y = lat_lon_to_map_coordinates([lon_min] * len(y_ticks_labels), y_tick_positions, m)\n    y_tick_positions_lat, _ = m(y_tick_positions_map_x, y_tick_positions_map_y, inverse=True)\n    y_tick_positions_lat_str = [f\"{lat:.1f}°N\" if lat >= 0 else f\"{abs(lat):.1f}°S\" for lat in y_tick_positions_lat]\n    plt.yticks(y_tick_positions_map_y, y_tick_positions_lat_str, fontsize=6)\n\n    # Create custom legend elements using the same colormap\n    legend_elements = [Patch(color=colors[i], label=f\"{value_ranges[i]} - {value_ranges[i+1]-1}\" if i < len(value_ranges) - 1 else f\"> {value_ranges[-1]}\") for i in range(len(value_ranges))]\n\n    # Create the legend outside the map\n    legend = plt.legend(handles=legend_elements, loc='upper left', title='Legend', fontsize=8)\n    legend.set_bbox_to_anchor((1.01, 1)) \n\n    # Remove the color bar\n    #plt.colorbar().remove()\n\n    plt.text(0.98, 0.02, 'Copyright © SWE Wormhole Team',\n             horizontalalignment='right', verticalalignment='bottom',\n             transform=plt.gcf().transFigure, fontsize=6, color='black')\n\n    # Set the aspect ratio to 'equal' to keep the plot at the center\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    # Adjust the bottom and top margins to create more white space between the title and the plot\n    plt.subplots_adjust(bottom=0.15, right=0.80)  # Adjust right margin to accommodate the legend\n    # Show the plot or save it to a file\n    new_plot_path = f'/home/chetana/gridmet_test_run/predicted_swe-{test_start_date}.png'\n    print(f\"The new plot is saved to {new_plot_path}\")\n    plt.savefig(new_plot_path)\n    # plt.show()  # Uncomment this line if you want to display the plot directly instead of saving it to a file\n\ndef convert_csvs_to_images_simple():\n    \"\"\"\n    Convert CSV data to simple scatter plot images for predicted SWE.\n\n    Returns:\n        None\n    \"\"\"\n    selected_date = datetime.strptime(test_start_date, \"%Y-%m-%d\")\n    var_name = \"predicted_swe\"\n    test_csv = \"/home/chetana/gridmet_test_run/test_data_predicted.csv\"\n    result_var_df = pd.read_csv(test_csv)\n    result_var_df.replace('--', pd.NA, inplace=True)\n    result_var_df.dropna(inplace=True)\n    result_var_df[var_name] = pd.to_numeric(result_var_df[var_name], errors='coerce')\n    \n    colormaplist, value_ranges = create_color_maps_with_value_range(result_var_df[var_name], fixed_value_ranges)\n\n    # Create a scatter plot\n    plt.scatter(result_var_df[\"lon\"].values, \n                result_var_df[\"lat\"].values, \n                label='Predicted SWE', \n                c=result_var_df['predicted_swe'], \n                cmap='viridis', \n                s=1, \n                edgecolor='none',\n               )\n\n    # Add a colorbar\n    cbar = plt.colorbar()\n    cbar.set_label('Predicted SWE')  # Label for the colorbar\n    \n    # Add labels and a legend\n    plt.xlabel('Longitude')\n    plt.ylabel('Latitude')\n    plt.title(f'SWE Prediction Map {test_start_date}')\n    plt.legend()\n\n    res_png_path = f\"{work_dir}/testing_output/{str(selected_date.year)}_{var_name}_{test_start_date}.png\"\n    plt.savefig(res_png_path)\n    print(f\"test image is saved at {res_png_path}\")\n    plt.close()\n\n# Uncomment the function call you want to use:\n# convert_csvs_to_images()\nconvert_csvs_to_images_simple()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "i2fynz",
  "name" : "deploy_images_to_website",
  "description" : null,
  "code" : "import distutils.dir_util\nfrom snowcast_utils import work_dir\nimport os\nimport shutil\n\n\nprint(\"move the plots and the results into the http folder\")\n\nsource_folder = f\"{work_dir}/var_comparison/\"\ndestination_folder = f\"/var/www/html/swe_forecasting/plots/\"\n\n# Copy the folder with overwriting existing files/folders\ndistutils.dir_util.copy_tree(source_folder, destination_folder, update=1)\n\nprint(f\"Folder '{source_folder}' copied to '{destination_folder}' with overwriting.\")\n\n\n# copy the png from testing_output to plots\nsource_folder = f\"{work_dir}/testing_output/\"\n\n# Ensure the destination folder exists, create it if necessary\nif not os.path.exists(destination_folder):\n    os.makedirs(destination_folder)\n\n# Loop through the files in the source folder\nfor filename in os.listdir(source_folder):\n    # Check if the file is a PNG file\n    if filename.endswith('.png'):\n        # Build the source and destination file paths\n        source_file = os.path.join(source_folder, filename)\n        destination_file = os.path.join(destination_folder, filename)\n        \n        # Copy the file from the source to the destination\n        shutil.copy(source_file, destination_file)\n        print(f'Copied: {filename}')\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "2o6cp8",
  "name" : "training_feature_selection",
  "description" : null,
  "code" : "import dask.dataframe as dd\n\n# Replace 'data.csv' with the path to your 50GB CSV file\ninput_csv = '/home/chetana/gridmet_test_run/model_training_data.csv'\n\n# List of columns you want to extract\nselected_columns = ['date', 'lat', 'lon', 'etr', 'pr', 'rmax',\n                    'rmin', 'tmmn', 'tmmx', 'vpd', 'vs', \n                    'elevation',\n                    'slope', 'curvature', 'aspect', 'eastness',\n                    'northness', 'Snow Water Equivalent (in) Start of Day Values']\n\n# Read the CSV file into a Dask DataFrame\ndf = dd.read_csv(input_csv, usecols=selected_columns)\n\n# Rename the column as you intended\ndf = df.rename(columns={\"Snow Water Equivalent (in) Start of Day Values\": \"swe_value\"})\n\n# Replace 'output.csv' with the desired output file name\noutput_csv = '/home/chetana/gridmet_test_run/model_training_cleaned.csv'\n\n# Write the selected columns to a new CSV file\ndf.to_csv(output_csv, index=False, single_file=True)  # single_file=True ",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "0n26v2",
  "name" : "amsr_testing_realtime",
  "description" : null,
  "code" : "\"\"\"\nScript for downloading AMSR snow data, converting it to DEM format, and saving as a CSV file.\n\nThis script downloads AMSR snow data, converts it to a format compatible with DEM, and saves it as a CSV file.\nIt utilizes the h5py library to read HDF5 files, pandas for data manipulation, and scipy.spatial.KDTree\nfor finding the nearest grid points. The script also checks if the target CSV file already exists to avoid redundant\ndownloads and processing.\n\nUsage:\n    Run this script to download and convert AMSR snow data for a specific date. It depends on the test_start_date from snowcast_utils to specify which date to download. You can overwrite that.\n\n\"\"\"\n\nimport os\nimport h5py\nimport subprocess\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nfrom snowcast_utils import work_dir, test_start_date\nfrom scipy.spatial import KDTree\nimport time\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n\nwestern_us_coords = '/home/chetana/gridmet_test_run/dem_file.tif.csv'\n\nlatlontree = None\n\ndef find_closest_index_numpy(target_latitude, target_longitude, lat_grid, lon_grid):\n    # Calculate the squared Euclidean distance between the target point and all grid points\n    distance_squared = (lat_grid - target_latitude)**2 + (lon_grid - target_longitude)**2\n    \n    # Find the indices of the minimum distance\n    lat_idx, lon_idx = np.unravel_index(np.argmin(distance_squared), distance_squared.shape)\n    \n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index_tree(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude using KDTree.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    global latlontree\n    \n    if latlontree is None:\n        # Create a KD-Tree from lat_grid and lon_grid\n        lat_grid_cleaned = np.nan_to_num(lat_grid, nan=0.0)  # Replace NaN with 0\n        lon_grid_cleaned = np.nan_to_num(lon_grid, nan=0.0)  # Replace NaN with 0\n        latlontree = KDTree(list(zip(lat_grid_cleaned.ravel(), lon_grid_cleaned.ravel())))\n      \n    # Query the KD-Tree to find the nearest point\n    distance, index = latlontree.query([target_latitude, target_longitude])\n\n    # Convert the 1D index to 2D grid indices\n    lat_idx, lon_idx = np.unravel_index(index, lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    \"\"\"\n    Find the closest grid point indices for a target latitude and longitude.\n\n    Parameters:\n        target_latitude (float): Target latitude.\n        target_longitude (float): Target longitude.\n        lat_grid (numpy.ndarray): Array of latitude values.\n        lon_grid (numpy.ndarray): Array of longitude values.\n\n    Returns:\n        int: Latitude index.\n        int: Longitude index.\n        float: Closest latitude value.\n        float: Closest longitude value.\n    \"\"\"\n    lat_diff = np.float64(np.abs(lat_grid - target_latitude))\n    lon_diff = np.float64(np.abs(lon_grid - target_longitude))\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\n  \ndef prepare_amsr_grid_mapper():\n    df = pd.DataFrame(columns=['amsr_lat', 'amsr_lon', \n                               'amsr_lat_idx', 'amsr_lon_idx',\n                               'gridmet_lat', 'gridmet_lon'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    # Convert the AMSR grid into our gridMET 1km grid\n    western_us_df = pd.read_csv(western_us_coords)\n    for idx, row in western_us_df.iterrows():\n        target_lat = row['Latitude']\n        target_lon = row['Longitude']\n        \n        # compare the performance and find the fastest way to search nearest point\n        closest_lat_idx, closest_lon_idx, closest_lat, closest_lon = find_closest_index(target_lat, target_lon, lat, lon)\n        df.loc[len(df.index)] = [closest_lat, \n                                 closest_lon,\n                                 closest_lat_idx,\n                                 closest_lon_idx,\n                                 target_lat, \n                                 target_lon]\n    \n    # Save the new converted AMSR to CSV file\n    df.to_csv(target_csv_path, index=False)\n  \n    print('AMSR mapper csv is created.')\n\ndef download_amsr_and_convert_grid():\n    \"\"\"\n    Download AMSR snow data, convert it to DEM format, and save as a CSV file.\n    \"\"\"\n    \n    prepare_amsr_grid_mapper()\n    \n    # the mapper\n    target_mapper_csv_path = f'{work_dir}/amsr_to_gridmet_mapper.csv'\n    mapper_df = pd.read_csv(target_mapper_csv_path)\n    print(mapper_df.head())\n    \n    df = pd.DataFrame(columns=['date', 'lat', \n                               'lon', 'AMSR_SWE', \n                               'AMSR_Flag'])\n    date = test_start_date\n    date = date.replace(\"-\", \".\")\n    he5_date = date.replace(\".\", \"\")\n    \n    # Check if the CSV already exists\n    target_csv_path = f'{work_dir}/testing_ready_amsr_{date}.csv'\n    if os.path.exists(target_csv_path):\n        print(f\"File {target_csv_path} already exists, skipping..\")\n        return\n    \n    target_amsr_hdf_path = f\"{work_dir}/amsr_testing/testing_amsr_{date}.he5\"\n    if os.path.exists(target_amsr_hdf_path):\n        print(f\"File {target_amsr_hdf_path} already exists, skip downloading..\")\n    else:\n        cmd = f\"curl --output {target_amsr_hdf_path} -b ~/.urs_cookies -c ~/.urs_cookies -L -n -O https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/{date}/AMSR_U2_L3_DailySnow_B02_{he5_date}.he5\"\n        print(f'Running command: {cmd}')\n        subprocess.run(cmd, shell=True)\n    \n    # Read the HDF\n    file = h5py.File(target_amsr_hdf_path, 'r')\n    hem_group = file['HDFEOS/GRIDS/Northern Hemisphere']\n    lat = hem_group['lat'][:]\n    lon = hem_group['lon'][:]\n    \n    # Replace NaN values with 0\n    lat = np.nan_to_num(lat, nan=0.0)\n    lon = np.nan_to_num(lon, nan=0.0)\n    \n    swe = hem_group['Data Fields/SWE_NorthernDaily'][:]\n    flag = hem_group['Data Fields/Flags_NorthernDaily'][:]\n    date = datetime.strptime(date, '%Y.%m.%d')\n    \n    # Convert the AMSR grid into our DEM 1km grid\n    \n    def get_swe(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_swe = swe[closest_lat_idx, closest_lon_idx]\n        return closest_swe\n    \n    def get_swe_flag(row):\n        # Perform your custom calculation here\n        closest_lat_idx = int(row['amsr_lat_idx'])\n        closest_lon_idx = int(row['amsr_lon_idx'])\n        closest_flag = flag[closest_lat_idx, closest_lon_idx]\n        return closest_flag\n    \n    # Use the apply function to apply the custom function to each row\n    mapper_df['AMSR_SWE'] = mapper_df.apply(get_swe, axis=1)\n    mapper_df['AMSR_Flag'] = mapper_df.apply(get_swe_flag, axis=1)\n    mapper_df['date'] = date\n    mapper_df.rename(columns={'dem_lat': 'lat'}, inplace=True)\n    mapper_df.rename(columns={'dem_lon': 'lon'}, inplace=True)\n    mapper_df = mapper_df.drop(columns=['amsr_lat',\n                                        'amsr_lon',\n                                        'amsr_lat_idx',\n                                        'amsr_lon_idx'])\n    \n    print(\"result df: \", mapper_df.head())\n    # Save the new converted AMSR to CSV file\n    print(f\"saving the new AMSR SWE to csv: {target_csv_path}\")\n    mapper_df.to_csv(target_csv_path, index=False)\n    \n    print('Completed AMSR testing data collection.')\n\n    \n\n# Run the download and conversion function\n#prepare_amsr_grid_mapper()\ndownload_amsr_and_convert_grid()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rvqv35",
  "name" : "perform_download.sh",
  "description" : null,
  "code" : "#!/bin/bash\n\n# Specify the file containing the download links\ninput_file=\"/home/chetana/gridmet_test_run/amsr/download_links.txt\"\n\n# Specify the base wget command with common options\nbase_wget_command=\"wget --http-user=<your_username> --http-password=<your_password> --load-cookies /home/chetana/gridmet_test_run/amsr/mycookies.txt --save-cookies mycookies.txt --keep-session-cookies --no-check-certificate -$\n\n# Specify the output directory for downloaded files\noutput_directory=\"/home/chetana/gridmet_test_run/amsr\"\n\n# Ensure the output directory exists\nmkdir -p \"$output_directory\"\n\n# Loop through each line (URL) in the input file and download it using wget\nwhile IFS= read -r url; do\n    echo \"Downloading: $url\"\n    $base_wget_command -P \"$output_directory\" \"$url\"\ndone < \"$input_file\"",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "vo8bc9",
  "name" : "merge_custom_traning_range",
  "description" : null,
  "code" : "\"\"\"\nThis script performs the following operations:\n1. Reads multiple CSV files into Dask DataFrames with specified chunk sizes and compression.\n2. Repartitions the DataFrames for optimized processing.\n3. Merges the DataFrames based on specified columns.\n4. Saves the merged DataFrame to a CSV file in chunks.\n5. Reads the merged DataFrame, removes duplicate rows, and saves the cleaned DataFrame to a new CSV file.\n\nAttributes:\n    working_dir (str): The directory where the CSV files are located.\n    chunk_size (str): The chunk size used for reading and processing the CSV files.\n\nFunctions:\n    main(): The main function that executes the data processing operations and saves the results.\n\"\"\"\n\nimport dask.dataframe as dd\nimport os\nfrom snowcast_utils import work_dir\n\nworking_dir = work_dir\nchunk_size = '32MB'  # You can adjust this chunk size based on your hardware and data size\n\ndef main():\n    # Read the CSV files with a smaller chunk size and compression\n    amsr = dd.read_csv(f'{working_dir}/training_ready_amsr_3_yrs.csv', blocksize=chunk_size)\n    snotel = dd.read_csv(f'{working_dir}/training_data_ready_snotel_3_yrs.csv', blocksize=chunk_size)\n    gridmet = dd.read_csv(f'{working_dir}/gridmet_climatology/training_ready_gridmet.csv', blocksize=chunk_size)\n    terrain = dd.read_csv(f'{working_dir}/training_ready_terrain.csv', blocksize=chunk_size)\n\n    # Repartition DataFrames for optimized processing\n    amsr = amsr.repartition(partition_size=chunk_size)\n    snotel = snotel.repartition(partition_size=chunk_size)\n    gridmet = gridmet.repartition(partition_size=chunk_size)\n    gridmet = gridmet.rename(columns={'day': 'date'})\n    terrain = terrain.repartition(partition_size=chunk_size)\n\n    # Merge DataFrames based on specified columns\n    merged_df = dd.merge(amsr, snotel, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, gridmet, on=['lat', 'lon', 'date'], how='outer')\n    merged_df = dd.merge(merged_df, terrain, on=['lat', 'lon'], how='outer')\n\n    # Save the merged DataFrame to a CSV file in chunks\n    output_file = os.path.join(working_dir, 'final_merged_data_3_yrs.csv')\n    merged_df.to_csv(output_file, single_file=True, index=False)\n    print('Merge completed.')\n\n    # Read the merged DataFrame, remove duplicate rows, and save the cleaned DataFrame to a new CSV file\n    df = dd.read_csv(f'{work_dir}/final_merged_data_3_yrs.csv')\n    df = df.drop_duplicates(keep='first')\n    df.to_csv(f'{work_dir}/final_merged_data_3yrs_cleaned_v2.csv', single_file=True, index=False)\n    print('Data cleaning completed.')\n\nif __name__ == \"__main__\":\n    main()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6evkh4",
  "name" : "training_data_range",
  "description" : null,
  "code" : "\"\"\"\nThis script loads and processes several CSV files into Dask DataFrames, applies filters, renames columns,\nand saves the resulting DataFrames to new CSV files based on a specified time range.\n\nAttributes:\n    gridmet_20_years_file (str): File path of the GridMET climatology data CSV file.\n    snotel_20_years_file (str): File path of the SNOTEL data CSV file.\n    terrain_file (str): File path of the terrain data CSV file.\n    amsr_3_years_file (str): File path of the AMSR data CSV file.\n    output_file (str): File path where the merged and processed data will be saved.\n\nFunctions:\n    clip_csv_using_time_range: Main function that loads, processes, and saves CSV data based on a specified time range.\n\"\"\"\n\nfrom snowcast_utils import work_dir\nimport dask.dataframe as dd\n\ndef clip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file):\n    \"\"\"\n    Loads, processes, and saves CSV data into Dask DataFrames, applies filters, renames columns, and saves the resulting\n    DataFrames to new CSV files based on a specified time range.\n\n    Args:\n        gridmet_20_years_file (str): File path of the GridMET climatology data CSV file.\n        snotel_20_years_file (str): File path of the SNOTEL data CSV file.\n        terrain_file (str): File path of the terrain data CSV file.\n        amsr_3_years_file (str): File path of the AMSR data CSV file.\n        output_file (str): File path where the merged and processed data will be saved.\n\n    Returns:\n        None\n    \"\"\"\n    # Load CSV files into Dask DataFrames\n    gridmet_df = dd.read_csv(gridmet_20_years_file, blocksize=\"64MB\")\n    snotel_df = dd.read_csv(snotel_20_years_file, blocksize=\"64MB\")\n    terrain_df = dd.read_csv(terrain_file, blocksize=\"64MB\")\n    amsr_df = dd.read_csv(amsr_3_years_file, blocksize=\"64MB\")\n\n    # Filter and rename columns for each DataFrame in a single step\n    # (Code to filter and rename columns...)\n\n    # Save the processed Dask DataFrames to new CSV files\n    gridmet_df.to_csv('/home/chetana/gridmet_test_run/training_ready_gridmet_3_yrs.csv', index=False, single_file=True)\n    amsr_df.to_csv('/home/chetana/gridmet_test_run/training_ready_amsr_3_yrs.csv', index=False, single_file=True)\n    snotel_df.to_csv('/home/chetana/gridmet_test_run/training_ready_snotel_3_yrs.csv', index=False, single_file=True)\n\n# Define file paths and execute the function\ngridmet_20_years_file = f\"{work_dir}/gridmet_climatology/testing_ready_gridmet.csv\"\nsnotel_20_years_file = f\"{work_dir}/training_ready_snotel_data.csv\"\nterrain_file = f'{work_dir}/training_ready_terrain.csv'\namsr_3_years_file = f\"{work_dir}/training_amsr_data.csv\"\noutput_file = f\"{work_dir}/training_ready_merged_data_dd.csv\"\n\nclip_csv_using_time_range(gridmet_20_years_file, snotel_20_years_file, terrain_file, amsr_3_years_file, output_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "76ewp5",
  "name" : "amsr_features",
  "description" : null,
  "code" : "import os\nimport csv\nimport h5py\nimport shutil\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport xarray as xr\n\ndef copy_he5_files(source_dir, destination_dir):\n    '''\n    Copy .he5 files from the source directory to the destination directory.\n\n    Args:\n        source_dir (str): The source directory containing .he5 files to copy.\n        destination_dir (str): The destination directory where .he5 files will be copied.\n\n    Returns:\n        None\n    '''\n    # Get a list of all subdirectories and files in the source directory\n    for root, dirs, files in os.walk(source_dir):\n        for file in files:\n            if file.endswith('.he5'):\n                # Get the absolute path of the source file\n                source_file_path = os.path.join(root, file)\n                # Copy the file to the destination directory\n                shutil.copy(source_file_path, destination_dir)\n\ndef find_closest_index(target_latitude, target_longitude, lat_grid, lon_grid):\n    '''\n    Find the index of the grid cell with the closest coordinates to the target latitude and longitude.\n\n    Args:\n        target_latitude (float): The target latitude.\n        target_longitude (float): The target longitude.\n        lat_grid (numpy.ndarray): An array of latitude values.\n        lon_grid (numpy.ndarray): An array of longitude values.\n\n    Returns:\n        Tuple[int, int, float, float]: A tuple containing the row index, column index, closest latitude, and closest longitude.\n    '''\n    # Compute the absolute differences between target and grid coordinates\n    lat_diff = np.abs(lat_grid - target_latitude)\n    lon_diff = np.abs(lon_grid - target_longitude)\n\n    # Find the indices corresponding to the minimum differences\n    lat_idx, lon_idx = np.unravel_index(np.argmin(lat_diff + lon_diff), lat_grid.shape)\n\n    return lat_idx, lon_idx, lat_grid[lat_idx, lon_idx], lon_grid[lat_idx, lon_idx]\n\ndef extract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file):\n    '''\n    Extract AMSR data values and save them to a CSV file.\n\n    Args:\n        amsr_data_dir (str): The directory containing AMSR .he5 files.\n        output_csv_file (str): The path to the output CSV file.\n\n    Returns:\n        None\n    '''\n    if os.path.exists(output_csv_file):\n        os.remove(output_csv_file)\n    \n    # Open the output CSV file in append mode using the csv module\n    with open(output_csv_file, 'a', newline='') as csvfile:\n        # Create a csv writer object\n        writer = csv.writer(csvfile)\n\n        # If the file is empty, write the header row\n        if os.path.getsize(output_csv_file) == 0:\n            writer.writerow([\"date\", \"lat\", \"lon\", \"amsr_swe\"])\n\n        # Loop through all the .he5 files in the directory\n        for filename in os.listdir(amsr_data_dir):\n            if filename.endswith('.he5'):\n                file_path = os.path.join(amsr_data_dir, filename)\n                print(file_path)\n\n                data_field_ds = xr.open_dataset(file_path, group='/HDFEOS/GRIDS/Northern Hemisphere/Data Fields')\n                swe_df = data_field_ds[\"SWE_NorthernDaily\"].values\n\n                latlon_ds = xr.open_dataset(file_path, group='/HDFEOS/GRIDS/Northern Hemisphere')\n                lat_df = latlon_ds[\"lat\"].values\n                lon_df = latlon_ds[\"lon\"].values\n\n                swe_variable = data_field_ds['SWE_NorthernDaily'].assign_coords(\n                    lat=latlon_ds['lat'],\n                    lon=latlon_ds['lon']\n                )\n\n                date_str = filename.split('_')[-1].split('.')[0]\n                date = datetime.strptime(date_str, '%Y%m%d')\n                df_val = swe_variable.to_dataframe()\n                \n                swe_variable = swe_variable.to_dataframe().reset_index()\n                for idx, row in station_data.iterrows():\n                    desired_lat = row['lat']\n                    desired_lon = row['lon']\n                    swe_variable['distance'] = np.sqrt((swe_variable['lat'] - desired_lat)**2 + (swe_variable['lon'] - desired_lon)**2)\n                    closest_row = swe_variable.loc[swe_variable['distance'].idxmin()]\n                    writer.writerow([date, desired_lat, desired_lon, closest_row['SWE_NorthernDaily']])\n\namsr_data_dir = '/home/chetana/gridmet_test_run/amsr'\noutput_csv_file = '/home/chetana/gridmet_test_run/training_amsr_data_tmp.csv'\n\nstation_cell_mapping = pd.read_csv('/home/chetana/gridmet_test_run/training_test_cords.csv')\n\nextract_amsr_values_save_to_csv(amsr_data_dir, output_csv_file)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "5wzgx5",
  "name" : "amsr_swe_data_download",
  "description" : null,
  "code" : "from datetime import datetime, timedelta\nimport os\nimport subprocess\n\ndef generate_links(start_year, end_year):\n    '''\n    Generate a list of download links for AMSR daily snow data files.\n\n    Args:\n        start_year (int): The starting year.\n        end_year (int): The ending year (inclusive).\n\n    Returns:\n        list: A list of download links for AMSR daily snow data files.\n    '''\n    base_url = \"https://n5eil01u.ecs.nsidc.org/AMSA/AU_DySno.001/\"\n    date_format = \"%Y.%m.%d\"\n    delta = timedelta(days=1)\n\n    start_date = datetime(start_year, 1, 1)\n    end_date = datetime(end_year + 1, 1, 1)\n\n    links = []\n    current_date = start_date\n\n    while current_date < end_date:\n        date_str = current_date.strftime(date_format)\n        link = base_url + date_str + \"/AMSR_U2_L3_DailySnow_B02_\" + date_str + \".he5\"\n        links.append(link)\n        current_date += delta\n\n    return links\n\nif __name__ == \"__main__\":\n    start_year = 2019\n    end_year = 2022\n\n    links = generate_links(start_year, end_year)\n    save_location = \"/home/chetana/gridmet_test_run/amsr\"\n    with open(\"/home/chetana/gridmet_test_run/amsr/download_links.txt\", \"w\") as txt_file:\n      for l in links:\n        txt_file.write(\" \".join(l) + \"\\n\")\n\n    #if not os.path.exists(save_location):\n    #    os.makedirs(save_location)\n\n    #for link in links:\n    #    filename = link.split(\"/\")[-1]\n    #    save_path = os.path.join(save_location, filename)\n    #    curl_cmd = f\"curl -b ~/.urs_cookies -c ~/.urs_cookies -L -n -o {save_path} {link}\"\n    #    subprocess.run(curl_cmd, shell=True, check=True)\n        # print(f\"Downloaded: {filename}\")\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "d4zcq6",
  "name" : "install_dependencies",
  "description" : null,
  "code" : "#!/bin/bash\n\n# change this line if you are creating a new env\nsource /home/chetana/anaconda3/bin/activate\nwhich python\n\n# Use cat to create the requirements.txt file\ncat <<EOL > requirements.txt\nabsl-py==0.9.0\nadal==1.2.7\naffine==2.3.0\nalembic==1.7.7\nanyio==3.6.2\nappdirs==1.4.4\napplicationinsights==0.11.10\nargcomplete==2.1.2\nargon2-cffi==21.3.0\nargon2-cffi-bindings==21.2.0\nasciitree==0.3.3\nasn1crypto==0.24.0\nastor==0.8.1\nattrs==22.2.0\naudioread==3.0.0\nAutomat==0.6.0\nautoPyTorch==0.0.2\nawscli==1.18.69\nazure-common==1.1.28\nazure-core==1.24.2\nazure-graphrbac==0.61.1\nazure-identity==1.7.0\nazure-mgmt-authorization==2.0.0\nazure-mgmt-containerregistry==10.0.0\nazure-mgmt-core==1.3.2\nazure-mgmt-keyvault==10.0.0\nazure-mgmt-resource==21.1.0\nazure-mgmt-storage==20.0.0\nazureml==0.2.7\nazureml-core==1.47.0\nazureml-dataprep==4.5.7\nazureml-dataprep-native==38.0.0\nazureml-dataprep-rslex==2.11.4\nazureml-dataset-runtime==1.47.0\nazureml-opendatasets==1.47.0\nazureml-telemetry==1.47.0\nBabel==2.11.0\nbackcall==0.1.0\nbackports-datetime-fromisoformat==2.0.0\nbackports.tempfile==1.0\nbackports.weakref==1.0.post1\nbackports.zoneinfo==0.2.1\nbasemap==1.3.8\nbasemap-data==1.3.2\nbcrypt==4.0.1\nbeautifulsoup4==4.6.0\nbleach==3.1.0\nblinker==1.4\nblis==0.7.10\nbokeh==2.3.3\nBoruta==0.3\nbotocore==1.16.19\nBottleneck==1.3.7\nbs4==0.0.1\ncachetools==4.0.0\ncatalogue==1.0.2\ncdo==1.3.5\ncertifi==2018.1.18\ncffi==1.15.1\ncftime==1.6.0\nchardet==3.0.4\ncharset-normalizer==2.0.12\nclick==7.1.2\nclick-plugins==1.1.1\ncligj==0.5.0\n#cloud-init==23.1.2\ncloudpickle==2.2.1\ncolorama==0.3.7\ncolorlover==0.3.0\n#command-not-found==0.3\nconfigobj==5.0.6\nConfigSpace==0.4.19\nconstantly==15.1.0\ncontextlib2==21.6.0\ncontextvars==2.4\ncryptography==40.0.2\ncufflinks==0.17.3\ncurlify==2.2.1\ncycler==0.10.0\ncymem==2.0.8\nCython==0.29.36\ndask==2021.3.0\ndatabricks-cli==0.17.7\n#dataclasses==0.8\ndatefinder==0.7.0\ndateparser==1.1.3\ndecorator==4.3.2\ndefusedxml==0.5.0\ndistributed==2021.3.0\ndistro==1.8.0\n#distro-info===0.18ubuntu0.18.04.1\ndocker==5.0.3\ndocutils==0.14\ndotnetcore2==3.1.23\n#download-espa-order==2.2.5\nearthengine-api==0.1.342\nentrypoints==0.3\nfasteners==0.18\nfilelock==3.4.1\nFiona==1.8.13.post1\nFlask==2.0.3\nfsspec==2022.1.0\nfuncy==2.0\nfusepy==3.0.1\nfuture==0.18.3\ngast==0.2.2\ngcloud==0.18.3\nGDAL>2.2.3\ngensim==3.8.3\ngeojson==2.5.0\ngeopandas==0.7.0\ngitdb==4.0.9\nGitPython==3.1.18\ngoogle-api-core==2.8.2\ngoogle-api-python-client==2.52.0\ngoogle-auth==1.35.0\ngoogle-auth-httplib2==0.1.0\ngoogle-auth-oauthlib==0.4.1\ngoogle-cloud-core==2.3.1\ngoogle-cloud-storage==2.0.0\ngoogle-crc32c==1.3.0\ngoogle-pasta==0.1.8\ngoogle-resumable-media==2.3.3\ngoogleapis-common-protos==1.56.3\ngreenlet==2.0.2\ngrpcio==1.27.2\ngunicorn==21.2.0\nh5py==2.10.0\nharmony-py==0.4.7\nHeapDict==1.0.1\nhpbandster==0.7.4\nhtml5lib==0.999999999\nhtmldate==1.4.2\nhtmlmin==0.1.12\nhttplib2==0.21.0\nhuggingface-hub==0.4.0\nhumanfriendly==10.0\nhyperlink==17.3.1\nidna==2.10\nImageHash==4.3.1\nimageio==2.5.0\nimbalanced-learn==0.7.0\nimblearn==0.0\nimmutables==0.19\nimportlib-metadata==4.8.3\nimportlib-resources==5.4.0\nincremental==16.10.1\ninstall==1.3.5\nipykernel==5.1.0\nipython==7.2.0\nipython-genutils==0.2.0\nipywidgets==7.4.2\nisodate==0.6.1\nitsdangerous==2.0.1\njedi==0.13.2\nJinja2==3.0.3\njmespath==0.9.3\njoblib==1.0.1\njson5==0.9.11\njsonpatch==1.16\njsonpickle==2.2.0\njsonpointer==1.10\njsonschema==3.2.0\njupyter==1.0.0\njupyter-client==7.1.2\njupyter-console==6.0.0\njupyter-core==4.9.2\njupyter-server==1.13.1\njupyterlab==3.2.9\njupyterlab-iframe==0.4.0\njupyterlab-server==2.10.3\nKeras==2.3.1\nKeras-Applications==1.0.8\nKeras-Preprocessing==1.1.0\nkeyring==10.6.0\nkeyrings.alt==3.0\nkiwisolver==1.1.0\nkmodes==0.12.2\nknack==0.10.1\n#language-selector==0.1\nliac-arff==2.5.0\nlibrosa==0.9.2\nlightgbm==3.3.5\nllvmlite==0.36.0\nloader==2017.9.11\nlocket==1.0.0\nlxml==4.9.2\nMako==1.1.6\nMarkdown==3.2.1\nMarkupSafe==2.0.1\nmatplotlib==3.3.4\nminio==7.1.15\nmissingno==0.5.2\nmistune==0.8.4\nmlflow==1.23.1\nmlxtend==0.19.0\nmsal==1.22.0\nmsal-extensions==0.3.1\nmsgpack==1.0.5\nmsrest==0.7.1\nmsrestazure==0.6.4\nmultimethod==1.5\nmunch==2.5.0\nmurmurhash==1.0.10\nnbclassic==0.3.5\nnbconvert==5.4.1\nnbformat==4.4.0\nndg-httpsclient==0.5.1\nnest-asyncio==1.5.6\nnetCDF4==1.6.2\nnetifaces==0.10.4\nnetworkx==2.5.1\nnltk==3.6.7\nnotebook==5.7.4\nnumba==0.53.0\nnumcodecs==0.9.1\nnumexpr==2.6.4\nnumpy==1.19.5\noauth2client==4.1.3\noauthlib==3.1.0\nolefile==0.45.1\nopencv-python==4.7.0.68\nopenml==0.14.0\nopt-einsum==3.2.0\npackaging==21.3\n#PAM==0.4.2\npandas==1.1.5\npandas-profiling==3.1.0\npandocfilters==1.4.2\nparamiko==2.12.0\nparso==0.3.4\npartd==1.2.0\npathlib==1.0.1\npathspec==0.9.0\npatsy==0.5.3\npexpect==4.6.0\nphik==0.12.0\npickleshare==0.7.5\nPillow==8.4.0\nPint==0.17\npkginfo==1.9.6\nplac==1.1.3\nplotly==5.17.0\npockets==0.9.1\nPolygon3==3.0.9.1\npooch==1.6.0\nportalocker==2.7.0\npreshed==3.0.9\nprogressbar2==3.55.0\nprometheus-client==0.5.0\nprometheus-flask-exporter==0.22.4\nprompt-toolkit==2.0.8\nprotobuf==3.19.6\npsutil==5.9.4\nptyprocess==0.6.0\npy4j==0.10.9.5\npyarrow==6.0.1\npyasn1==0.4.2\npyasn1-modules==0.2.1\npycaret==2.3.10\npycparser==2.21\npycrate==0.6.0\npycrypto==2.6.1\npydantic==1.9.2\npyDeprecate==0.3.2\npyEddyTracker==3.2.0\npyEddyTrackerSample==0.1.0\nPygments==2.3.1\n#PyGObject==3.26.1\nPyJWT==2.4.0\npyLDAvis==3.2.2\nPyNaCl==1.5.0\npynisher==0.6.4\npynndescent==0.5.10\npyod==1.1.0\npyOpenSSL==17.5.0\npyparsing==2.4.2\npyproj==2.5.0\nPyro4==4.82\npyrsistent==0.18.0\npyserial==3.4\npyshp==2.1.0\nPySocks==1.7.1\npyspark==3.2.4\npystac==0.5.6\npystac-client==0.1.1\npython-apt==1.6.5+ubuntu0.5\npython-dateutil==2.8.2\npython-debian==0.1.32\npython-dotenv==0.20.0\npython-utils==3.5.2\npytz==2019.2\npytz-deprecation-shim==0.1.0.post0\nPyWavelets==1.0.3\npyxdg==0.25\nPyYAML==5.4.1\npyzmq==17.1.2\nqtconsole==4.4.3\nquerystring-parser==1.2.4\nrasterio==1.1.3\nregex==2023.8.8\nrequests==2.27.1\nrequests-oauthlib==1.3.0\nrequests-unixsocket==0.1.5\nresampy==0.4.2\nroman==2.0.0\nrsa==4.0\ns3transfer==0.3.3\nsacremoses==0.0.53\nscikit-image==0.15.0\nscikit-learn==0.23.2\nscikit-plot==0.3.7\nscipy==1.5.4\nscour==0.36\nscreen-resolution-extra==0.0.0\nseaborn==0.11.2\nSecretStorage==2.3.1\nSend2Trash==1.5.0\nserpent==1.41\nservice-identity==16.0.0\nshap\nShapely==1.7.0\nsix==1.14.0\nsklearn==0.0\nsmart-open==6.4.0\nsmmap==5.0.0\nsniffio==1.2.0\nsnuggs==1.4.7\nsortedcontainers==2.4.0\nsos==4.3\nsoundfile==0.12.1\nspacy==2.3.9\nsphinxcontrib-napoleon==0.7\nSQLAlchemy==1.4.49\nsqlparse==0.4.4\nsrsly==1.0.7\nssh-import-id==5.7\nstatsmodels==0.12.2\nsystemd-python==234\ntables==3.4.2\ntabulate==0.8.10\ntangled-up-in-unicode==0.1.0\ntbb==2021.10.0\ntblib==1.7.0\ntenacity==8.2.2\ntensorboard==2.1.1\ntensorboard-logger==0.1.0\ntensorflow==2.1.0\ntensorflow-estimator==2.1.0\ntensorflow-gpu==2.1.0\ntermcolor==1.1.0\nterminado==0.12.1\ntestpath==0.4.2\ntextblob==0.17.1\nthinc==7.4.6\nthreadpoolctl==3.1.0\ntifftools==1.3.9\ntokenizers==0.12.1\ntoolz==0.8.2\ntorch==1.10.1\ntorchmetrics==0.8.2\ntorchvision==0.11.2\ntornado==6.1\ntornado-proxy-handlers==0.0.5\ntqdm==4.64.1\ntraitlets==4.3.2\ntransformers==4.18.0\nTwisted==17.9.0\ntyping_extensions==4.1.1\ntzdata==2022.7\ntzlocal==4.2\nubuntu-drivers-common==0.0.0\nufw==0.35\numap-learn==0.5.4\nunattended-upgrades==0.1\nuritemplate==4.1.1\nurllib3==1.26.15\nvirtualenv==16.6.1\nvisions==0.7.4\nwasabi==0.10.1\nwcwidth==0.1.7\nwebencodings==0.5.1\nwebsocket-client==1.3.1\nWerkzeug==2.0.3\nwget==3.2\nwidgetsnbextension==3.4.2\nwordcloud==1.9.2\nwrapt==1.12.1\nxarray==0.10.2\nxkit==0.0.0\nxmltodict==0.13.0\nyellowbrick==1.3.post1\nzarr==2.8.3\nzict==2.1.0\nzipp==3.6.0\nzope.interface==4.3.2\nEOL\n\npython -m pip install -r requirements.txt\n# clean up\nrm requirements.txt\n",
  "lang" : "shell",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "6x6myw",
  "name" : "model_evaluation",
  "description" : "python",
  "code" : "# Predict results using the model\n\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn import metrics \nfrom sklearn import tree\nimport joblib\nimport os\nfrom pathlib import Path\nimport json\nimport geopandas as gpd\nimport geojson\nimport os.path\nimport math\nfrom sklearn.model_selection import RandomizedSearchCV\n\nexit(0)  # for now, the workflow is not ready yet\n\n# read the grid geometry file\n\n\n# read the grid geometry file\nhomedir = os.path.expanduser('~')\nprint(homedir)\ngithub_dir = f\"{homedir}/Documents/GitHub/SnowCast\"\nmodis_test_ready_file = f\"{github_dir}/data/ready_for_training/modis_test_ready.csv\"\nmodis_test_ready_pd = pd.read_csv(modis_test_ready_file, header=0, index_col=0)\n\npd_to_clean = modis_test_ready_pd[[\"year\", \"m\", \"doy\", \"ndsi\", \"swe\", \"station_id\", \"cell_id\"]].dropna()\n\nall_features = pd_to_clean[[\"year\", \"m\", \"doy\", \"ndsi\"]].to_numpy()\nall_labels = pd_to_clean[[\"swe\"]].to_numpy().ravel()\n\ndef evaluate(model, test_features, y_test, model_name):\n    y_predicted = model.predict(test_features)\n    mae = metrics.mean_absolute_error(y_test, y_predicted)\n    mse = metrics.mean_squared_error(y_test, y_predicted)\n    r2 = metrics.r2_score(y_test, y_predicted)\n    rmse = math.sqrt(mse)\n\n    print(\"The {} model performance for testing set\".format(model_name))\n    print(\"--------------------------------------\")\n    print('MAE is {}'.format(mae))\n    print('MSE is {}'.format(mse))\n    print('R2 score is {}'.format(r2))\n    print('RMSE is {}'.format(rmse))\n    \n    return y_predicted\n\nbase_model = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest_basic.joblib\")\nbasic_predicted_values = evaluate(base_model, all_features, all_labels, \"Base Model\")\n\nbest_random = joblib.load(f\"{homedir}/Documents/GitHub/snowcast_trained_model/model/wormhole_random_forest.joblib\")\nrandom_predicted_values = evaluate(best_random, all_features, all_labels, \"Optimized\")\n\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "r4knm9",
  "name" : "interpret_model_results",
  "description" : null,
  "code" : "# do real interpretation for the model results and find real reasons for bad predictions\n# prevent aimless and headless attempts that are just wasting time.\n# this is an essential step in the loop\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date, month_to_season\nimport os\nfrom sklearn.inspection import partial_dependence,PartialDependenceDisplay\nimport shap\nimport matplotlib.pyplot as plt\n\ndef load_model(model_path):\n    \"\"\"\n    Load a trained machine learning model from a given path.\n\n    Args:\n        model_path (str): The path to the model file.\n\n    Returns:\n        object: The loaded machine learning model.\n    \"\"\"\n    return joblib.load(model_path)\n\ndef load_data(file_path):\n    \"\"\"\n    Load data from a CSV file and return it as a DataFrame.\n\n    Args:\n        file_path (str): The path to the CSV file.\n\n    Returns:\n        pd.DataFrame: The loaded data as a DataFrame.\n    \"\"\"\n    return pd.read_csv(file_path)\n\ndef preprocess_data(data):\n    \"\"\"\n    Preprocess the input data by converting date columns, handling missing values,\n    renaming columns, and reordering columns.\n\n    Args:\n        data (pd.DataFrame): The input data to be preprocessed.\n\n    Returns:\n        pd.DataFrame: Preprocessed data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    #reference_date = pd.to_datetime('1900-01-01')\n    #data['date'] = (data['date'] - reference_date).dt.days\n    data['date'] = data['date'].dt.month.apply(month_to_season)\n    data.replace('--', pd.NA, inplace=True)\n    \n    data = data.apply(pd.to_numeric, errors='coerce')\n    data.rename(columns={'Latitude': 'lat', \n                         'Longitude': 'lon',\n                         'vpd': 'mean_vapor_pressure_deficit',\n                         'vs': 'wind_speed', \n                         'pr': 'precipitation_amount', \n                         'etr': 'potential_evapotranspiration',\n                         'tmmn': 'air_temperature_tmmn',\n                         'tmmx': 'air_temperature_tmmx',\n                         'rmin': 'relative_humidity_rmin',\n                         'rmax': 'relative_humidity_rmax',\n                         'AMSR_SWE': 'SWE',\n                         'AMSR_Flag': 'Flag',\n                         'Elevation': 'elevation',\n                         'Slope': 'slope',\n                         'Aspect': 'aspect',\n                         'Curvature': 'curvature',\n                         'Northness': 'northness',\n                         'Eastness': 'eastness'\n                        }, inplace=True)\n\n    desired_order = ['date', 'lat', 'lon', 'SWE', 'Flag',\n'air_temperature_tmmn', 'potential_evapotranspiration',\n'mean_vapor_pressure_deficit', 'relative_humidity_rmax',\n'relative_humidity_rmin', 'precipitation_amount',\n'air_temperature_tmmx', 'wind_speed', 'elevation', 'slope', 'curvature',\n'aspect', 'eastness', 'northness']\n    \n    data = data[desired_order]\n    data = data.reindex(columns=desired_order)\n    \n    data.to_csv(f'{work_dir}/testing_all_ready_for_check.csv', index=False)\n    \n    data = data.fillna(-999)\n    print(\"how many rows are left?\", len(data))\n    print('data.shape: ', data.shape)\n    \n    #data = data.drop(['date', 'SWE', 'Flag', 'mean_vapor_pressure_deficit', 'potential_evapotranspiration', 'air_temperature_tmmx', 'relative_humidity_rmax', 'relative_humidity_rmin', ], axis=1)\n    \n    return data\n\ndef predict_swe(model, data):\n    \"\"\"\n    Use a trained model to predict SWE values for the input data.\n\n    Args:\n        model (object): The trained machine learning model.\n        data (pd.DataFrame): The input data for prediction.\n\n    Returns:\n        pd.DataFrame: Input data with predicted SWE values.\n    \"\"\"\n    print(data.head())\n    print(\"how many rows are there?\", len(data))\n    \n    predictions = model.predict(data)\n    data['predicted_swe'] = predictions\n    print(\"predicted swe: \", data['predicted_swe'].describe())\n    \n    return data, model\n\ndef merge_data(original_data, predicted_data):\n    \"\"\"\n    Merge the original data with predicted SWE values.\n\n    Args:\n        original_data (pd.DataFrame): The original data.\n        predicted_data (pd.DataFrame): Data with predicted SWE values.\n\n    Returns:\n        pd.DataFrame: Merged data.\n    \"\"\"\n    #new_data_extracted = predicted_data[[\"date\", \"lat\", \"lon\", \"predicted_swe\"]]\n    new_data_extracted = predicted_data[[ \"lat\", \"lon\", \"predicted_swe\"]]\n    # merged_df = original_data.merge(new_data_extracted, on=[\"date\", 'lat', 'lon'], how='left')\n    merged_df = original_data.merge(new_data_extracted, on=['lat', 'lon'], how='left')\n    print(\"Columns after merge:\", merged_df.columns)\n    \n    return merged_df\n  \ndef plot_feature_importance():\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    model = load_model(model_path)\n    \n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series.csv'\n    print(\"preparing training data from csv: \", training_data_path)\n    data = pd.read_csv(training_data_path)\n    data = data.drop('swe_value', axis=1) \n    data = data.drop('Unnamed: 0', axis=1)\n    \n    \n    # Step 1: Feature Importance\n    analysis_plot_output_folder = f'{work_dir}/testing_output/'\n    feature_importances = model.feature_importances_\n    feature_names = data.columns\n\n    # Create a bar plot of feature importances\n    plt.figure(figsize=(10, 6))\n    print(feature_names.shape)\n    print(feature_importances.shape)\n    plt.barh(feature_names, feature_importances)\n    plt.xlabel('Feature Importance')\n    plt.ylabel('Features')\n    plt.title('Feature Importance Plot')\n    plt.savefig(f'{analysis_plot_output_folder}/importance_summary_plot_latest_model.png')\n  \ndef interpret_prediction():\n    \"\"\"\n    Interpret the model results and find real reasons for bad predictions.\n\n    Returns:\n        None\n    \"\"\"\n    height = 666\n    width = 694\n    model_path = f'/home/chetana/Documents/GitHub/SnowCast/model/wormhole_ETHole_latest.joblib'\n    print(f\"using model : {model_path}\")\n    \n    new_data_path = f'{work_dir}/testing_all_ready.csv'\n    output_path = f'{work_dir}/test_data_predicted.csv'\n    \n    if os.path.exists(output_path):\n        # If the file exists, remove it\n        os.remove(output_path)\n        print(f\"File '{output_path}' has been removed.\")\n\n    model = load_model(model_path)\n    new_data = load_data(new_data_path)\n    print(\"new_data shape: \", new_data.shape)\n\n    preprocessed_data = preprocess_data(new_data)\n    print('data preprocessing completed.')\n    print(f'model used: {model_path}')\n    predicted_data, current_model = predict_swe(model, preprocessed_data)\n    \n    \n\n    # Step 2: Partial Dependence Plots\n\n    # Select features for partial dependence plots (e.g., the first two features)\n    features_to_plot = feature_names\n    \n#     partial_dependence_display = PartialDependenceDisplay.from_estimator(\n#       current_model, \n#       preprocessed_data.drop('predicted_swe', axis=1), \n#       features=features_to_plot, grid_resolution=50\n# )\n#     partial_dependence_display.figure_.suptitle('Partial Dependence Plots')\n#     partial_dependence_display.figure_.subplots_adjust(top=0.9)\n#     partial_dependence_display.figure_.savefig(f'{analysis_plot_output_folder}/partial_dependence_summary_plot_{test_start_date}.png')\n\n    # Step 3: SHAP Values\n#     explainer = shap.Explainer(current_model)\n#     shap_values = explainer.shap_values(\n#       preprocessed_data.drop('predicted_swe', axis=1))\n\n#     # Summary plot of SHAP values\n#     shap.summary_plot(shap_values, preprocessed_data)\n#     plt.title('Summary Plot of SHAP Values')\n#     plt.savefig(f'{analysis_plot_output_folder}/shap_summary_plot_{test_start_date}.png')\n    \n    # Additional code for SHAP interpretation can be added here\n    # Select a single data point for which you want to explain the prediction\n    # Create a SHAP explainer and calculate SHAP values\n    # Visualize the SHAP values as needed\n    \n#   predicted_data = merge_data(preprocessed_data, predicted_data)\n#   print('data prediction completed.')\n    \n#   predicted_data.to_csv(output_path, index=False)\n#   print(\"Prediction successfully done \", output_path)\n\n#   if len(predicted_data) == height * width:\n#     print(f\"The image width, height match with the number of rows in the csv. {len(predicted_data)} rows\")\n#   else:\n#     raise Exception(\"The total number of rows do not match\")\n\n\n#plot_feature_importance()  # no need, this step is already done in the model post processing step. \n#interpret_prediction()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "9c573m",
  "name" : "train_test_pattern_compare",
  "description" : null,
  "code" : "# compare patterns in training and testing\n# plot the comparison of training and testing variables\n\n# This process only analyzes data; we don't touch the model here.\n\nimport joblib\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nimport numpy as np\nfrom snowcast_utils import work_dir, test_start_date\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef clean_train_df(data):\n    \"\"\"\n    Clean and preprocess the training data.\n\n    Args:\n        data (pd.DataFrame): The training data to be cleaned.\n\n    Returns:\n        pd.DataFrame: Cleaned training data.\n    \"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    reference_date = pd.to_datetime('1900-01-01')\n    data['date'] = (data['date'] - reference_date).dt.days\n    data.replace('--', pd.NA, inplace=True)\n    data.fillna(-999, inplace=True)\n    \n    # Remove all the rows that have 'swe_value' as -999\n    data = data[(data['swe_value'] != -999)]\n\n    print(\"Get slope statistics\")\n    print(data[\"slope\"].describe())\n  \n    print(\"Get SWE statistics\")\n    print(data[\"swe_value\"].describe())\n\n    data = data.drop('Unnamed: 0', axis=1)\n    \n\n    return data\n\ndef compare():\n    \"\"\"\n    Compare training and testing data and create variable comparison plots.\n\n    Returns:\n        None\n    \"\"\"\n    new_testing_data_path = f'{work_dir}/testing_all_ready_for_check.csv'\n    training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n    tr_df = pd.read_csv(training_data_path)\n    tr_df = clean_train_df(tr_df)\n    te_df = pd.read_csv(new_testing_data_path)\n    \n    tr_df = tr_df.drop('date', axis=1)\n    te_df = te_df.drop('date', axis=1)\n\n    print(\"Training DataFrame: \", tr_df)\n    print(\"Testing DataFrame: \", te_df)\n\n    print(\"Training columns: \", tr_df.columns)\n    print(\"Testing columns: \", te_df.columns)\n\n    var_comparison_plot_path = f\"{work_dir}/var_comparison/\"\n    if not os.path.exists(var_comparison_plot_path):\n        os.makedirs(var_comparison_plot_path)\n        \n    num_cols = len(tr_df.columns)\n    new_num_cols = int(num_cols**0.5)  # Square grid\n    new_num_rows = int(num_cols / new_num_cols) + 1\n    \n    # Create a figure with multiple subplots\n    fig, axs = plt.subplots(new_num_rows, new_num_cols, figsize=(24, 20))\n    \n    # Flatten the axs array to iterate through subplots\n    axs = axs.flatten()\n    print(\"length: \", len(tr_df.columns))\n    # Iterate over columns and create subplots\n    for i, col in enumerate(tr_df.columns):\n        print(i, \" - \", col)\n        axs[i].hist(tr_df[col], bins=100, alpha=0.5, color='blue', label='Train')\n        if col in te_df.columns:\n            axs[i].hist(te_df[col], bins=100, alpha=0.5, color='red', label='Test')\n\n        axs[i].set_title(f'{col}')\n        axs[i].legend()\n        \n    \n    \n    plt.tight_layout()\n    plt.savefig(f'{var_comparison_plot_path}/{test_start_date}_final_comparison.png')\n    plt.close()\n\ndef calculate_feature_colleration_in_training():\n  training_data_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n  tr_df = pd.read_csv(training_data_path)\n  tr_df = clean_train_df(tr_df)\n  \n    \ncompare()\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "ee5ur4",
  "name" : "correct_slope",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom snowcast_utils import work_dir\n\nready_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned.csv'\ndem_slope_csv_path = f\"{work_dir}/slope_file.tif.csv\"\n\n# Read the cleaned ready CSV and DEM slope CSV\ntrain_ready_df = pd.read_csv(ready_csv_path)\ndem_slope_df = pd.read_csv(dem_slope_csv_path)\n\nprint(train_ready_df.head())\nprint(dem_slope_df.head())\n\nprint(\"all train.csv columns: \", train_ready_df.columns)\nprint(\"all dem slope columns: \", dem_slope_df.columns)\n\ndef replace_slope(row):\n    '''\n    Replace the 'slope' column in the input DataFrame row with the closest slope value from the DEM data.\n\n    Args:\n        row (pandas.Series): A row of data containing 'lat' and 'lon' columns.\n\n    Returns:\n        float: The closest slope value from the DEM data for the given latitude and longitude.\n    '''\n    target_lat = row[\"lat\"]\n    target_lon = row[\"lon\"]\n    # Calculate the squared distance to find the closest DEM point\n    dem_slope_df['Distance'] = (dem_slope_df['Latitude'] - target_lat) ** 2 + (dem_slope_df['Longitude'] - target_lon) ** 2\n    closest_row = dem_slope_df.loc[dem_slope_df['Distance'].idxmin()]\n    return closest_row[\"Slope\"]\n\n# Apply the 'replace_slope' function to calculate and replace slope values in the DataFrame\ntrain_ready_df['slope'] = train_ready_df.apply(lambda row: replace_slope(row), axis=1)\n\nprint(train_ready_df.head())\nprint(train_ready_df.columns)\n\nnew_result_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\n# Save the modified DataFrame to a new CSV file\ntrain_ready_df.to_csv(new_result_csv_path, index=False)\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "f03i7p",
  "name" : "convert_to_time_series",
  "description" : null,
  "code" : "import pandas as pd\nimport os\nfrom snowcast_utils import work_dir\nimport shutil\nimport numpy as np\n\npd.set_option('display.max_columns', None)\n\ncurrent_ready_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3.csv'\n\ntarget_time_series_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1.csv'\n\nbackup_time_series_csv_path = f'{work_dir}/final_merged_data_3yrs_cleaned_v3_time_series_v1_bak.csv'\n\ndef array_describe(arr):\n    stats = {\n        'Mean': np.mean(arr),\n        'Median': np.median(arr),\n        'Standard Deviation': np.std(arr),\n        'Variance': np.var(arr),\n        'Minimum': np.min(arr),\n        'Maximum': np.max(arr),\n        'Sum': np.sum(arr),\n    }\n    \n    return stats\n\ndef convert_to_time_series():\n  columns_to_be_time_series = [\"SWE\", \"Flag\", 'air_temperature_tmmn',\n  'potential_evapotranspiration', 'mean_vapor_pressure_deficit',\n  'relative_humidity_rmax', 'relative_humidity_rmin',\n  'precipitation_amount', 'air_temperature_tmmx', 'wind_speed',]\n  # Read the cleaned ready CSV and DEM slope CSV\n  df = pd.read_csv(current_ready_csv_path)\n  # df['location'] = df['lat'].astype(str) + ',' + df['lon'].astype(str)\n  # unique_location_pairs = df.drop_duplicates(subset='location')[['lat', 'lon']]\n\n  # print(unique_location_pairs)\n  # unique_date = df.drop_duplicates(subset='date')[['date']]\n  # print(unique_date)\n\n  # add a 7 days time series to each row\n  df.sort_values(by=['lat', 'lon', 'date'], inplace=True)\n\n  \n  \n  # fill in the missing values of AMSR and gridMet using polynomial values\n  # Function to perform polynomial interpolation\n  def interpolate_missing_inplace(df, column_name, degree=3):\n    x = df.index\n    y = df[column_name]\n\n    # Create a mask for missing values\n    mask = y > 240\n    # Perform interpolation\n    new_y = np.interp(x, x[~mask], y[~mask])\n    \n    if np.any(new_y > 240):\n      print(\"mask: \", mask)\n      print(\"x[~mask]: \", x[~mask])\n      print(\"y[~mask]: \", y[~mask])\n      print(\"new_y: \", new_y)\n      raise ValueError(\"Single group: shouldn't have values > 240 here\")\n\n    # Replace missing values with interpolated values\n    df[column_name] = new_y\n    #print(df[column_name].describe())\n    return df\n    \n\n  # Group by location and apply interpolation for each column\n  # Group the data by 'lat' and 'lon'\n  grouped = df.groupby(['lat', 'lon'])\n  filled_data = pd.DataFrame()\n  for name, group in grouped:\n    print(f\"Start to filling missing values..{name}\")\n    new_df = interpolate_missing_inplace(group, 'SWE')\n    filled_data = pd.concat([filled_data, group], axis=0)\n\n  filled_data = filled_data.reset_index()\n  \n  filled_data.reset_index(inplace=True)\n  \n  if any(filled_data['SWE'] > 240):\n    raise ValueError(\"Error: shouldn't have SWE>240 at this point\")\n    \n\n  # Create a new DataFrame to store the time series data for each location\n  result = pd.DataFrame()\n\n  # Define the number of days to consider (7 days in this case)\n  num_days = 7\n  \n  grouped = filled_data.groupby(['lat', 'lon'])\n  for name, group in grouped:\n      group = group.set_index('date')\n      for day in range(1, num_days + 1):\n        for target_col in columns_to_be_time_series:\n          new_column_name = f'{target_col}_{day}'\n          group[new_column_name] = group[target_col].shift(day)\n      result = pd.concat([result, group], axis=0)\n\n  # Reset the index of the result DataFrame\n  result = result.reset_index()\n  result.to_csv(target_time_series_csv_path, index=False)\n  print(f\"new data is saved to {target_time_series_csv_path}\")\n  shutil.copy(target_time_series_csv_path, backup_time_series_csv_path)\n  print(f\"file is backed up to {backup_time_series_csv_path}\")\n\n\nconvert_to_time_series()\n\n# df = pd.read_csv(target_time_series_csv_path)\n\n# print(df.columns)\n\n# df.head()\n\n# description = df.describe(include='all')\n# # Print the description\n# print(description)\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
