{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cf1e10-74bd-472c-bd49-9a0ac800f324",
   "metadata": {},
   "source": [
    "## Setting up Your Python Environment\n",
    "per: https://blog.roboflow.com/how-to-use-segment-anything-model-sam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c027e46-6d08-402e-8949-d7047cf45e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-pzam4yxu\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-pzam4yxu\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /srv/conda/envs/notebook/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in /srv/conda/envs/notebook/lib/python3.11/site-packages (0.19.0)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "!pip install -q roboflow supervision\n",
    "!wget -q \\\n",
    "'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e2fc3f7-7904-4827-a377-79e1e9ac6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon\n",
    "import torch\n",
    "from segment_anything import sam_model_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e40e7-3650-47fc-937f-bc42231143ba",
   "metadata": {},
   "source": [
    "## Loading the Segment Anything Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "874e4fe7-9e0b-405b-888a-7a1cdcbae69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.11/site-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # use cpu if no access to gpu\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=\"/home/jovyan/swe_forecasting_prod/notebooks/sam_vit_h_4b8939.pth\")\n",
    "sam.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b171de09-4567-4f7c-aca6-bfbcd8352570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce6499f-c838-4e8f-9847-a4c1db0a79b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-band GeoTIFF has been successfully converted to an RGB JPEG image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2070/3245926074.py:13: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
      "  colormap = cm.get_cmap('viridis')  # Choose a colormap (e.g., 'viridis', 'jet', 'plasma')\n"
     ]
    }
   ],
   "source": [
    "# Function to convert GeoTIFF to JPEG\n",
    "def convert_geotiff_to_jpeg(geotiff_path, jpeg_path):\n",
    "    with rasterio.open(geotiff_path) as src:\n",
    "        # Read the image data\n",
    "        band_data = src.read(1)  # Assuming single band image for SWE\n",
    "        # img_array = np.interp(img_array, (img_array.min(), img_array.max()), (0, 255)).astype(np.uint8)\n",
    "\n",
    "        # Step 2: Normalize the data to 0-1 for colormap application\n",
    "        band_min, band_max = band_data.min(), band_data.max()\n",
    "        normalized_band = (band_data - band_min) / (band_max - band_min)\n",
    "        \n",
    "        # Step 3: Apply a colormap\n",
    "        colormap = cm.get_cmap('viridis')  # Choose a colormap (e.g., 'viridis', 'jet', 'plasma')\n",
    "        colored_image = colormap(normalized_band)  # Apply colormap\n",
    "        colored_image = (colored_image[:, :, :3] * 255).astype(np.uint8)  # Convert to RGB and scale to 0-255\n",
    "        \n",
    "        # Step 4: Save the RGB image as JPEG\n",
    "        jpeg_image = Image.fromarray(colored_image)\n",
    "        jpeg_image.save(jpeg_path, 'JPEG')\n",
    "        \n",
    "        print(\"Single-band GeoTIFF has been successfully converted to an RGB JPEG image.\")\n",
    "\n",
    "\n",
    "        # # Convert to PIL Image\n",
    "        # img = Image.fromarray(img_array)\n",
    "        # img = img.convert('L')  # Convert to grayscale if needed\n",
    "\n",
    "        # # Save as JPEG\n",
    "        # img.save(jpeg_path)\n",
    "        \n",
    "# Paths to input and output files\n",
    "# geotiff_path = '/home/jovyan/shared-public/ml_swe_monitoring_prod/swe_predicted_2023-12-15_median_filtered.tif'\n",
    "jpeg_path = '/home/jovyan/shared-public/ml_swe_monitoring_prod/lansat_glacier_image.jpg'\n",
    "shapefile_path = '/home/jovyan/shared-public/ml_swe_monitoring_prod/lansat_glacier_image.shp'\n",
    "    \n",
    "# # Convert GeoTIFF to JPEG\n",
    "# convert_geotiff_to_jpeg(geotiff_path, jpeg_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8afbd4-bf96-465a-9b4e-9e539805a757",
   "metadata": {},
   "source": [
    "## Automated Mask (Instance Segmentation) Generation with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25219cc5-f51e-4140-bdad-2d060f1e1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "image_bgr = cv2.imread(jpeg_path) # this is the jpeg of our swe prediction \n",
    "\n",
    "# Get image dimensions\n",
    "height, width, _ = image_bgr.shape\n",
    "\n",
    "# Define tile size\n",
    "tile_size = 300\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "            \n",
    "\n",
    "# Loop through the image and extract tiles\n",
    "for y in range(0, height, tile_size):\n",
    "    for x in range(0, width, tile_size):\n",
    "        print(f\"Processing tile {x} {y}\")\n",
    "        # Ensure the tile has the correct size (padding may be needed for edges)\n",
    "        # Optionally pad the tile to ensure it has the correct size\n",
    "        pad_y = tile_size - tile.shape[0]\n",
    "        pad_x = tile_size - tile.shape[1]\n",
    "\n",
    "        # Extract the tile from the image\n",
    "        tile = image_bgr[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "        image_rgb = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n",
    "        result = mask_generator.generate(image_rgb)\n",
    "        \n",
    "        detections = sv.Detections.from_sam(sam_result=result)\n",
    "        \n",
    "        annotated_image = mask_annotator.annotate(scene=tile.copy(), detections=detections)\n",
    "        \n",
    "        sv.plot_images_grid(\n",
    "            images=[tile, annotated_image],\n",
    "            grid_size=(1, 2),\n",
    "            titles=['source image', 'segmented image']\n",
    "        )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad09483-371a-4001-b8e5-7f74ead474b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
