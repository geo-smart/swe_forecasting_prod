{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44cf1e10-74bd-472c-bd49-9a0ac800f324",
   "metadata": {},
   "source": [
    "## Setting up Your Python Environment\n",
    "per: https://blog.roboflow.com/how-to-use-segment-anything-model-sam/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c027e46-6d08-402e-8949-d7047cf45e92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-y9jfwmg2\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-y9jfwmg2\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /srv/conda/envs/notebook/lib/python3.11/site-packages (2.4.0)\n",
      "Requirement already satisfied: torchvision in /srv/conda/envs/notebook/lib/python3.11/site-packages (0.19.0)\n",
      "Requirement already satisfied: filelock in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: triton==3.0.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torch) (3.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
      "Requirement already satisfied: numpy in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \\\n",
    "'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "!pip install -q roboflow supervision\n",
    "!wget -q \\\n",
    "'https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth'\n",
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e2fc3f7-7904-4827-a377-79e1e9ac6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import rasterio\n",
    "from PIL import Image\n",
    "from shapely.geometry import Polygon\n",
    "import torch\n",
    "from segment_anything import sam_model_registry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425e40e7-3650-47fc-937f-bc42231143ba",
   "metadata": {},
   "source": [
    "## Loading the Segment Anything Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "874e4fe7-9e0b-405b-888a-7a1cdcbae69a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu') # use cpu if no access to gpu\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=\"/home/jovyan/swe_forecasting_prod/notebooks/sam_vit_h_4b8939.pth\")\n",
    "sam.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b171de09-4567-4f7c-aca6-bfbcd8352570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ce6499f-c838-4e8f-9847-a4c1db0a79b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single-band GeoTIFF has been successfully converted to an RGB JPEG image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n"
     ]
    }
   ],
   "source": [
    "# Function to convert GeoTIFF to JPEG\n",
    "def convert_geotiff_to_jpeg(geotiff_path, jpeg_path):\n",
    "    with rasterio.open(geotiff_path) as src:\n",
    "        # Read the image data\n",
    "        band_data = src.read(1)  # Assuming single band image for SWE\n",
    "        # img_array = np.interp(img_array, (img_array.min(), img_array.max()), (0, 255)).astype(np.uint8)\n",
    "\n",
    "        # Step 2: Normalize the data to 0-1 for colormap application\n",
    "        band_min, band_max = band_data.min(), band_data.max()\n",
    "        normalized_band = (band_data - band_min) / (band_max - band_min)\n",
    "        \n",
    "        # Step 3: Apply a colormap\n",
    "        colormap = cm.get_cmap('viridis')  # Choose a colormap (e.g., 'viridis', 'jet', 'plasma')\n",
    "        colored_image = colormap(normalized_band)  # Apply colormap\n",
    "        colored_image = (colored_image[:, :, :3] * 255).astype(np.uint8)  # Convert to RGB and scale to 0-255\n",
    "        \n",
    "        # Step 4: Save the RGB image as JPEG\n",
    "        jpeg_image = Image.fromarray(colored_image)\n",
    "        jpeg_image.save(jpeg_path, 'JPEG')\n",
    "        \n",
    "        print(\"Single-band GeoTIFF has been successfully converted to an RGB JPEG image.\")\n",
    "\n",
    "\n",
    "        # # Convert to PIL Image\n",
    "        # img = Image.fromarray(img_array)\n",
    "        # img = img.convert('L')  # Convert to grayscale if needed\n",
    "\n",
    "        # # Save as JPEG\n",
    "        # img.save(jpeg_path)\n",
    "        \n",
    "# Paths to input and output files\n",
    "geotiff_path = '/home/jovyan/shared-public/ml_swe_monitoring_prod/swe_predicted_2023-12-15_median_filtered.tif'\n",
    "jpeg_path = '/home/jovyan/shared-public/ml_swe_monitoring_prod/swe_predicted_2023-12-15_median_filtered.jpg'\n",
    "shapefile_path = '/home/jovyan/shared-public/ml_swe_monitoring_prod/swe_predicted_2023-12-15_median_filtered.shp'\n",
    "    \n",
    "# Convert GeoTIFF to JPEG\n",
    "convert_geotiff_to_jpeg(geotiff_path, jpeg_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8afbd4-bf96-465a-9b4e-9e539805a757",
   "metadata": {},
   "source": [
    "## Automated Mask (Instance Segmentation) Generation with SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "25219cc5-f51e-4140-bdad-2d060f1e1832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "image_bgr = cv2.imread(jpeg_path) # this is the jpeg of our swe prediction \n",
    "\n",
    "tile_size = (666, 666)  # Define the size of the tile\n",
    "top_left_x, top_left_y = 0, 0  # Define the position of the top-left corner of the tile\n",
    "\n",
    "# Calculate the bottom-right corner of the tile\n",
    "bottom_right_x = top_left_x + tile_size[0]\n",
    "bottom_right_y = top_left_y + tile_size[1]\n",
    "\n",
    "# Extract the tile from the image\n",
    "tile = image_bgr[top_left_y:bottom_right_y, top_left_x:bottom_right_x]\n",
    "\n",
    "image_rgb = cv2.cvtColor(tile, cv2.COLOR_BGR2RGB)\n",
    "result = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b98f9fec-5fdd-48a9-91d0-0360bac2fc79",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 1; dimension is 694 but corresponding boolean dimension is 666",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m mask_annotator \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mMaskAnnotator(color_lookup\u001b[38;5;241m=\u001b[39msv\u001b[38;5;241m.\u001b[39mColorLookup\u001b[38;5;241m.\u001b[39mINDEX)\n\u001b[1;32m      5\u001b[0m detections \u001b[38;5;241m=\u001b[39m sv\u001b[38;5;241m.\u001b[39mDetections\u001b[38;5;241m.\u001b[39mfrom_sam(sam_result\u001b[38;5;241m=\u001b[39mresult)\n\u001b[0;32m----> 7\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m \u001b[43mmask_annotator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_bgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m sv\u001b[38;5;241m.\u001b[39mplot_images_grid(\n\u001b[1;32m     10\u001b[0m     images\u001b[38;5;241m=\u001b[39m[image_bgr, annotated_image],\n\u001b[1;32m     11\u001b[0m     grid_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     12\u001b[0m     titles\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource image\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegmented image\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/supervision/utils/conversion.py:23\u001b[0m, in \u001b[0;36mensure_cv2_image_for_annotation.<locals>.wrapper\u001b[0;34m(self, scene, *args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(annotate_func)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, scene: ImageType, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scene, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mannotate_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscene\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scene, Image\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m     26\u001b[0m         scene_np \u001b[38;5;241m=\u001b[39m pillow_to_cv2(scene)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.11/site-packages/supervision/annotators/core.py:360\u001b[0m, in \u001b[0;36mMaskAnnotator.annotate\u001b[0;34m(self, scene, detections, custom_color_lookup)\u001b[0m\n\u001b[1;32m    351\u001b[0m     color \u001b[38;5;241m=\u001b[39m resolve_color(\n\u001b[1;32m    352\u001b[0m         color\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolor,\n\u001b[1;32m    353\u001b[0m         detections\u001b[38;5;241m=\u001b[39mdetections,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m custom_color_lookup,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    359\u001b[0m     mask \u001b[38;5;241m=\u001b[39m detections\u001b[38;5;241m.\u001b[39mmask[detection_idx]\n\u001b[0;32m--> 360\u001b[0m     \u001b[43mcolored_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m color\u001b[38;5;241m.\u001b[39mas_bgr()\n\u001b[1;32m    362\u001b[0m cv2\u001b[38;5;241m.\u001b[39maddWeighted(\n\u001b[1;32m    363\u001b[0m     colored_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopacity, scene, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopacity, \u001b[38;5;241m0\u001b[39m, dst\u001b[38;5;241m=\u001b[39mscene\n\u001b[1;32m    364\u001b[0m )\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scene\n",
      "\u001b[0;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 1; dimension is 694 but corresponding boolean dimension is 666"
     ]
    }
   ],
   "source": [
    "import supervision as sv\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=result)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad09483-371a-4001-b8e5-7f74ead474b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
